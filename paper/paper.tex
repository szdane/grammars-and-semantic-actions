% -*- fill-column: 80; -*-
\documentclass[review,anonymous,screen,acmsmall,nonacm]{acmart}
\usepackage{mathpartir}
\usepackage{tikz-cd}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{xspace}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{stmaryrd}
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows, fit}

\usepackage{pdfpages}

\newcommand{\SPF}{\mathsf{SPF}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\K}{\mathsf{K}}
\newcommand{\map}{\mathsf{map}}
\newcommand{\roll}{\mathsf{roll}}
\newcommand{\fold}{\mathsf{fold}}
\newcommand{\inl}{\mathsf{inl}}
\newcommand{\inr}{\mathsf{inr}}
\newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\lto}{\multimap}
\newcommand{\tol}{\mathrel{\rotatebox[origin=c]{180}{$\lto$}}}
\newcommand{\String}{\textbf{String}}
\newcommand{\Set}{\mathbf{Set}}
\newcommand{\Syn}{\mathbf{Synx}}
\newcommand{\SemAct}{\mathbf{SemAct}}
\newcommand{\Gr}{\mathbf{Gr}}
\newcommand{\Grammar}{\mathbf{Gr}}
\newcommand{\Type}{\mathbf{Type}}
\newcommand{\Prop}{\mathbf{Prop}}
\newcommand{\Bool}{\mathbf{Bool}}
\newcommand{\true}{\mathbf{true}}
\newcommand{\false}{\mathbf{false}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\theoryname}{Dependent Lambek Calculus\xspace}
\newcommand{\theoryabbv}{$\textrm{Lambek}^D$}

\newcommand{\gluedNL}{{\mathcal G}_S}
\newcommand{\gluedNLUniv}{{\mathcal G}_{S,i}}
\newcommand{\gluedL}{{\mathcal G}_L}

\newcommand{\amp}{\mathrel{\&}}
\newcommand{\pair}{\amp}
\DeclareMathOperator*{\bigamp}{\scalerel*{\&}{\bigoplus}}
\DeclareMathOperator*{\bigwith}{\scalerel*{\&}{\bigoplus}}


\newcommand{\bang}{~\textbf{!}~}
% Tentatively using uparrow for linear to non-linear to be in line with
% Pfennings adjoint functional programming
\newcommand{\ltonl}[1]{~\uparrow #1}
\newcommand{\nil}{\mathsf{nil}}
\newcommand{\cons}{\mathsf{cons}}
\newcommand{\epscons}{\epsilon\mathsf{cons}}
\newcommand{\data}{\mathsf{data}}
\newcommand{\where}{\mathsf{where}}
\newcommand{\Trace}{\mathsf{Trace}}
\newcommand{\stringquote}[1]{\texttt{\textquotesingle#1\textquotesingle}}
\newcommand{\internalize}[1]{\lceil#1\rceil}

\newcommand{\oplusinj}[2]{\sigma\,#1\,#2}
\newcommand{\withprj}[2]{\pi\,#1\,#2}

\newcommand{\simulsubst}[2]{#1\{#2\}}
\newcommand{\subst}[3]{\simulsubst {#1} {#2/#3}}
\newcommand{\el}{\mathsf{el}}
\newcommand{\letin}[3]{\mathsf{let}\, #1 = #2 \, \mathsf{in}\, #3}
\newcommand{\lamb}[2]{\lambda #1.\, #2}
\newcommand{\lamblto}[2]{\lambda^{{\lto}} #1.\, #2}
\newcommand{\lambtol}[2]{\lambda^{{\tol}} #1.\, #2}
\newcommand{\dlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\withlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\applto}[2]{#1 \mathop{{}^{\lto}} #2}
\newcommand{\apptol}[2]{#1 \mathop{{}^{\tol}} #2}
\newcommand{\PiTy}[3]{\textstyle\prod (#1 : #2). #3}
\newcommand{\SigTy}[3]{\textstyle\sum (#1 : #2). #3}
\newcommand{\LinPiTy}[3]{\textstyle\bigamp (#1 : #2). #3}
\newcommand{\LinSigTy}[3]{\textstyle\bigoplus (#1 : #2). #3}
%% \newcommand{\DepWith}[2]{{\textstyle\bigamp}\limits_{#1}{#2}
%% \newcommand{\DepPlus}[2]{{\textstyle\bigoplus}\limits_{#1}{#2}
\newcommand{\GrTy}{\mathsf{Gr}}

\newcommand{\equalizer}[3]{\{#1\,|\,\applto {#2}{#1} = \applto{#3}{#1} \}}
\newcommand{\equalizerin}[1]{\langle #1 \rangle}
\newcommand{\equalizerpi}[1]{#1.\pi}

\newcommand{\ctxwff}[1]{#1 \,\, \mathsf{ok}}
\newcommand{\ctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{type}}
\newcommand{\linctxwff}[2]{#1 \vdash #2 \,\, \mathsf{ok}}
\newcommand{\linctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{linear}}

\newif\ifdraft
\drafttrue
\newcommand{\steven}[1]{\ifdraft{\color{orange}[{\bf Steven says}: #1]}\fi}
\renewcommand{\max}[1]{\ifdraft{\color{blue}[{\bf Max says}: #1]}\fi}
\newcommand{\pedro}[1]{\ifdraft{\color{red}[{\bf Pedro says}: #1]}\fi}
\newcommand{\pipe}{\,|\,}

\begin{document}

\pagestyle{plain}

\pagebreak

\title{Intrinsic Verification of Parsers and Formal Grammar Theory in Dependent Lambek Calculus}

\author{Steven Schaefer}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{stschaef@umich.edu}

\author{Nathan Varner}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{nvarner@umich.edu}

\author{Max S. New}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{maxsnew@umich.edu}

\author{Pedro H. Azevedo de Amorim}
\affiliation{
  \department{Department of Computer Science}
  \institution{University of Oxford}
  \country{UK}
}
\email{pedro.azevedo.de.amorim@cs.ox.ac.uk}

\makeatletter
\let\@authorsaddresses\@empty
\makeatother

\begin{abstract}
  We propose \theoryname~(\theoryabbv), a domain-specific dependent type theory
  for the definition of intrinsically verified parsers and verification of weak
  and strong equivalences of formal grammars. The linear types in \theoryabbv
  have a simple interpretation as formal grammars, with values of the types
  interpreted as parse trees and general terms as a form of intrinsically
  verified parse transformers, of which parsers being a special case. We
  demonstrate the expressivity of this system by showing that the indexed
  inductive linear types in this calculus can be used to encode many different
  familiar grammar formalisms, including regular and context-free grammars but
  also traces of various types of automata. We can then construct strong
  equivalences between grammar and automata formalisms and construct verified
  parsers as simple linear functional programs. All of our examples have been
  implemented in a prototype implementation of \theoryabbv as a shallow
  embedding in Agda.

  Our shallow embedding is based on a denotational semantics of \theoryabbv
  where non-linear types denote ordinary sets and linear types denote formal
  grammars. To develop this semantics compositionally, we define a notion of
  general notion of model we call a \emph{Chomsky hyperdoctrine}, a
  generalization of the Kleene algebra approach to formal language theory. In
  addition to our ``standard model'' where linear types denote formal grammars,
  we demonstrate two non-standard models. The first interprets linear types as
  semantic actions over formal grammars, suggesting future extensions of our
  calculus. The second is used to structure a logical relations argument to
  prove a canonicity metatheorem for the type theory, proving that all closed
  terms of context-free linear types normalize to a parse tree.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

%% Outline
%% - Why a framework for parser verification
%% - what intrinsic verification gets us
%% - the secret sauce: indexed linear inductive types
%% - prototype implementation
%% - overview of "abstract" formal grammars?


Parsing data from a serialized format is one of the most common tasks
in all of computing. Accordingly, this is a quite well-studied problem
spanning theoretical computer science and linguistics with such
milestones as Chomsky's hierarchy of grammars
\cite{chomThreeModels1956}, practical algorithms for parsing of
regular and context-free grammars, and variants
\cite{KNUTH1965607,Earley1970}, as well as implementations of
practical tools for the generation of efficient parsers
\cite{Johnsonyacc}.

The theory of formal languages and parsing is one of the oldest and most
thoroughly developed area of theoretical computer science.
The central object of study is a \emph{formal language} $L$ over an alphabet
$\Sigma$, which is classically defined as a
\emph{subset} of strings $L \subseteq \String = \Sigma^{*}$. The expressivity of formal
languages is sufficient for defining a \emph{recognizer} of a language $L$:
a function $r : \String \to \Prop$ such that $r$ maps each string $w$ to the
proposition that $w$ is in the language $w \in L$. This technique of language
recognition can concisely specify lexical analysis, and even lead to verified
lexer implementations \cite{egolfVerbatim,Ouedraogo_2023}.

While there are ample theory and tooling to handle the first stage of a language frontend, the final
frontier for parsing research is the development of
\emph{formally verified} parser implementations. Due to the
ubiquitous nature of parsing, these would useful components of many
formally verified software systems: compilers and servers most
especially. However, language recognition is insufficient for specifying a
parser, and for this task much of the structured foundations from the theory of
formal languages is rendered moot. Rather than returning a proposition-valued result, a parser produces a
structured \emph{parse tree} adherent to a \emph{formal grammar}.
The research on such grammars is often performed atop an ad-hoc choice of grammar
formalism, and results are proven with respect to that formalism. Critically,
the field is lacking a unifying theory across these different formalisms, even
though many of the same
compositional reason principles appear across distinct grammar formalisms.

In this work, we aim to elucidate these similarities and
provide a mathematical theory of formal grammars for constructing verified
parsers. Our approach can be summarized with the
Curry-Howard-style slogan: ``grammars as types''; and the addendum: ``parsers as
terms''. That is, we develop a type theory with a novel
denotational semantics in a category with abstract formal grammars as
objects and \emph{parse transformers} as morphisms. We say these are
\emph{abstract} formal grammars because they are not tied to any
particular syntactic formalism such as Chomskyan grammars, regular
expressions, Lambek calculus etc. The category we choose is a rich
in structure: it is a topos as well as being monoidal closed, and these
universal properties serve as the inspiration for our type theory.

The type theory itself is a form of dependent linear-non-linear type
theory: the linear types can be interpreted as grammars, whereas the
non-linear types can be interpreted as ordinary sets. As in prior work
on dependent linear-non-linear types, the linear types are allowed to
depend on the non-linear types but not vice-versa \cite{krishnaswami_integrating_2015}.

The substructural nature of \theoryabbv is well-aligned with the
requirements intrinsic to parsing and to the theory of formal
languages, where strings constitute a very clear notion of resource
that cannot be duplicated, reordered, or dropped. Moreover, the constructive
aspect of \theoryabbv ensures that verification of parsers written in the
calculus are \emph{correct-by-construction}. The type system is powerful enough
that derivations of a term type checking carry intrinsic proofs of correctness.

\paragraph{Contributions} This paper begins in \cref{sec:synindnotion} by
giving a universal semantic characterization of formal grammar.
From there, in \cref{sec:tt} we provide the syntax of \theoryname, a type theory
that axiomatizes the underlying structures needed to conduct formal grammar
theory. In \cref{sec:automata} we demonstrate the usage of
\theoryname for defining parser terms and internalizing results from formal language theory.
Then in \cref{sec:categorify}, we characterize the abstract categorical
models of our syntax. Afterwards in \cref{sec:othermodels}, we demonstrate several models,
including one to prove a metatheoretic canonicity theorem.
Our contributions are then:
%
\begin{itemize}
  \item A syntax-independent notion of grammar given by the category $\Grammar$
  \item \theoryname: A dependent
    linear-non-linear type theory meant to syntactically capture the latent structures
    in $\Grammar$ that enable parsing
  \item Categorical structures that capture the concepts integral to formal grammar theory, and that are notably modeled by $\Grammar$
  \item An implementation of the language, which is embedded shallowly in Agda
\end{itemize}

\subsection{A Syntax-Independent Notion of Syntax}
\label{sec:synindnotion}

The notion of formal language is central to the theory of parsing.
This definition is
especially useful as it gives a semantics to formal grammars that is completely
independent of any particular syntactic grammar formalism. Any new
notion of grammar can be given a language semantics and it provides a precise
mathematical specification for implementing a \emph{recognizer} of a language.
In this section we propose a \emph{proof-relevant} generalization of formal
languages that can accommodate richer aspects of formal language theory such as
parsing. We begin by proposing a presheaf approach to grammars that encompasses
other classes of grammars in a syntax-independent way.

\paragraph{From Formal Languages to Formal Grammars}

Our first contribution is a novel and syntax-independent characterization of
formal grammar:

\begin{definition}
  \label{def:grammar}
  A \emph{formal grammar} $G$ over a fixed alphabet $\Sigma$ is a function
  $G : \String \to \Set$.
\end{definition}

\steven{TODO need to go through and ensure that $\Sigma^{*}$ and $\String$ are each used appropriately}

We say that a grammar $G$ associates to every string $w$ the set\footnote{We
  make little comment as to which foundations are used to encode $\Set$. Our
  construction is polymorphic in choice of a proper class of small sets, a
  Grothendieck universe, a type theoretic universe, or any similar foundation.} $G w$ of parse
trees showing that $w$ matches $G$. This definition of formal grammar serves as
a mathematical specification for a parser --- taking on the same role as
languages do for recognizers. In this sense, our new characterization of
grammars is just a proof-relevant generalization of the existing notion of
formal language.

By ranging over all input strings, notice that each grammar induces a language.
For each formal grammar $G$, define the language of accepted strings $L_{G}$ as,

\[ L_{G} = \{ w \in \String ~|~ G w~\text{is inhabited} \} \]

As they are simply sets, formal languages are naturally endowed with a partial
order via subset inclusion. Just, formal grammars naturally coalesce into a
\emph{category}.

Define $\Grammar$ to be the category of formal grammars. Whose
objects are grammars, and whose morphisms are functions $f : G \to H$ between two
grammars presented by a family of functions $f^{w} : G w \to H w$ for
$w : \String$. Intuitively, we interpret these morphisms as parse
\emph{transformers} --- as constructive translations of $G$-parses into $H$-parses. Throughout this paper, we will make frequent reference to this categorical
structure on $\Grammar$.

\steven{Above sounds awkward. I want to additionally say something here that signposts that we will give more
  information about that structure without just listing it all here. }

\paragraph{From Grammars to Formal Grammars}
The category-theoretic framework can be further used to describe and compare
different notions of formal grammar. For instance, Chomskyan generative
grammars, semi-Thue systems, Montague grammars, and so on are all distinct syntactic
presentations of the same underlying idea of an abstract specification for
parsing. Each of these is an instance of a general \emph{notion of formal
  grammar} --- a category paired with a functor into
$\Grammar$. That is, they may all be interpreted in our category of grammars. In
this sense, $\Grammar$ provides an
all-encompassing semantic domain for parsing.


Upon further inspection, $\Grammar$ may equivalently
be described as the functor category $\Set^{\String}$ --- where the
set $\String$ is viewed as a discrete\footnote{Objects in $\String$
comprise the set of all strings, and the only morphisms are the
identity morphisms.} category. In fact, in this paper we will take
$\Grammar$ to be defined as the functor category $\Set^{\String}$. Such functor
categories in $\Set$, called presheaf categories, carry a
remarkable amount of structure.  Indeed they are a monoidal topos and,
as such, are a model to a substructural dependently typed theory.

Thus we
present \theoryname (\theoryabbv) as a domain-specific type theory that
abstracts over the structure of $\Grammar$. To begin, let us look at what some
programs in \theoryabbv look like.

\steven{TODO Reword this transition into the examples}

% \max{this section kind of trails off at the end with a very vague aside about a $2$-category of grammar formalisms. Maybe this should go in future work/discussion? What is the message of this section? How does it tie in with the message of the paper? What even is the message of this paper?}

% \pedro{Would building on this proposed structure make the message of the section clearer? In this case
%   we would have to be a bit clearer in terms of how familiar notions of grammar can be seen as categories with functor into $\Grammar$.}


\section{\theoryname by Example}
\label{sec:type-theory-examples}

\steven{To save space, each of these example figures can be
  minified/potentially inlined}

To gain intuition for working in \theoryabbv, we study some examples of
grammars and parses of them internal to \theoryabbv. In each of our examples, we will look at
grammars over the three character alphabet $\{ a , b , c\}$.

Each letter of the
alphabet corresponds to a base type in our calculus. For instance, the grammar
for the single character $a$ has a single parse tree for the string \stringquote{a}.
and no parse trees at any other strings.

\paragraph{Finite Grammars}
First consider finite grammars --- those built from base types via disjunctions $\oplus$ and
concatenations $\otimes$. For instance, $(a \otimes b) \oplus c$.
In \cref{fig:fingram} we find a proof tree that constructs a term of type
$(a \otimes b) \oplus c$ in the context $x : a, y : b$. We think of this term as
a parse tree internal to \theoryabbv.

Parses of a fixed string $w$ into a grammar $A$ are internally represented as
terms of type $A$ in the context $\internalize w$, where $\internalize w$ is a
comma-separated list of the constituent characters of $w$. In particular, to
parse \stringquote{ac}, we build parse trees in the context
$\internalize{\stringquote{ab}} = x : a , y : b$. Thus the term
$\inl(x \otimes y)$ from \cref{fig:fingram} details the
manner by which \stringquote{ac} matches the grammar $(a \otimes b) \oplus c$.

\begin{figure}
\begin{mathpar}
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {~}
      {x : a \vdash x : a}
      \\
      \inferrule
      {~}
      {y : b \vdash y : b}
    }
    {x : a , y : b \vdash x \otimes y : a \otimes b}
  }
  {x : a , y : b \vdash \inl(x \otimes y) : (a \otimes b) \oplus c}
\end{mathpar}
\caption{\stringquote{ac} matches $(a \oplus b) \otimes c$}
\label{fig:fingram}
\end{figure}


Because derivations in a linear context correspond to parse trees in this
manner, we cannot affirm all of the usual structural rules of a type system.
Weakening and contraction each allow for modification of the string in context ---
weakening allows for characters to be dropped at will, while contraction allows
for duplication of characters. Omission of these rules offers a
resource-sensitive view that ensures only the inputted characters are parsed.
Moreover, strings are not simply sets of characters, as they carry an inherent
ordering. Thus, the rule of exchange is also omitted in this calculus so that
the constituent characters of a string are not freely permuted. Without the above structural rules we
arrive at an ordered, linear type system.

\paragraph{Regular Expressions}
Next, we demonstrate how to parse a regular expression in \theoryabbv. Similar
to finite grammars, regular
expressions are built inductively using base types, $\oplus$, and $\otimes$ ;
however, unlike finite grammars, regular expressions additionally have a
restricted form of recursion via Kleene star $(\cdot)^{\ast}$.

\begin{figure}
\begin{align*}
\data &~A^{*} : L~\where\\
      & \nil : \ltonl {A^{*}} \\
      & \cons : \ltonl {(A \lto A^{*} \lto A^{*})}
\end{align*}
\caption{Kleene Star as an inductive type}
\label{fig:kleenestarinductive}
\end{figure}

For a grammar $A$, the recursive nature of $A^{*}$ is captured by defining
$A^{*}$ to be an inductive linear
type, as shown in \cref{fig:kleenestarinductive}. $A^{*}$ has two constructors:
$\nil$, which builds a parse of type $A^{*}$ in the empty context; and $\cons$,
which takes in of $A$ and a parse of $A^{*}$ and linearly combines them to build
a parse of $A^{*}$. The type defined by these constructors belong to the
universe of linear types $L$.

The arrow, $\uparrow$, wrapping these constructors denotes
an interaction between the linear and nonlinear parts of the theory.
In short, a nonlinear term
$M : \ltonl A^{*}$ denotes a linear term in the empty context
$\cdot \vdash M : A^{*}$. Later in
this paper, we will further discuss the behavior of $\uparrow$.
At this moment,
it suffices to read these arrows as allow us to use the reuse the names
``$\nil$'' and ``$\cons$''
as symbols for constructors. That is, the $\nil$ constructor is not linearly
consumed at usage time

Just as in \cref{fig:fingram}, we can show that particular strings match a Kleene
star. In \cref{fig:kleenestarderivation}, we demonstrate that \stringquote{ab}
matches $(a^{*} \otimes b) \oplus c$ through repeated application of the Kleene
star constructors. In this proof tree we expand some syntactic details that are often
left implicit. For instance, every linear context is parameterized by a nonlinear
context, which is syntactically separate by a semicolon. If the nonlinear
context is empty, we may often omit it; however, in this figure we make this
distinction explicit. Further consider the leaves of the tree. First, the
leaves involving the constructors are nonlinear terms, and thus they occur in
the nonlinear empty context. Then we explicit cast from
an nonlinear $uparrow$-wrapped type into a linear one. Such casts may often be
made implicitly, such as in $\cref{fig:kleeneabstractproof}$.


\begin{figure}
\begin{mathpar}
\inferrule
{
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {
        \inferrule
        {
          \inferrule
          {~}
          {\cdot \vdash \cons : \ltonl {(a \lto a^* \lto a^*)}}
        }
        {\cdot ; \cdot \vdash \cons : a \lto a^* \lto a^*}
        \\
        \inferrule
        {~}
        {\cdot ; x : a \vdash x : a}
      }
      {\cdot ; x : a \vdash \cons~x : a^* \lto a^*}
      \\
      \inferrule
      {
        \inferrule
        {~}
        {\cdot \vdash \nil : \ltonl {(a^*)}}
      }
      {\cdot ; \cdot \vdash \nil : a^*}
    }
    {\cdot ; x : a \vdash \cons (x , \nil) : a^{*}}
    \\
    \inferrule
    {~}
    {\cdot ; y : b \vdash y : b}
  }
  {\cdot ; x : a , y : b \vdash \cons(x, \nil) \otimes y : a^{*} \otimes b}
}
{\cdot ; x : a , y : b \vdash \inl (\cons(x) \otimes y) : (a^{*} \otimes b) \oplus c}
\end{mathpar}
\caption{\stringquote{ab} matches $(a^{*} \otimes b) \oplus c$}
\label{fig:kleenestarderivation}
\end{figure}

We may also have derivations where the term in context is not simply a
string of base types. For instance,
in \cref{fig:kleeneabstractproof} we show that every parse of the grammar
$(A \otimes A)^{*}$ induces a parse of $A^{*}$ for an arbitrary grammar $A$.

The context $(A \otimes A)^{*}$ does not correspond directly to a string, so it is
not quite appropriate
to treat linear terms here as parse \textit{trees}. Instead, it is more
appropriate to think of such terms as parse \textit{transformers}. That is, in
\cref{fig:kleeneabstractproof} the
inhabitant $x : (A \otimes A)^{*}$ in the context is not itself data to be parsed; rather,
$x$ is a parse tree and the term from context $x : (A \otimes A)^{*}$ to the
type $A^{*}$ is a transformation of parse trees of the input grammar into parse
trees of the output grammar.

\steven{TODO : describe elimination principle somewhere. Maybe just in prose?}

\begin{figure}
\begin{mathpar}
  \inferrule
  {
    \inferrule
    {~}
    {\cdot \vdash \nil : A^*}
    \\
    \inferrule
    {
      \inferrule
      {
        \inferrule
        {~}
        {x_1 : A, x_2 : A , y : A^* \vdash \cons (x_1 , \cons(x_2 , y)) : A^*}
      }
      {x' : A \otimes A , y : A^* \vdash g := \letin {x_1 \otimes x_2} {x'} {\cons (x_1 , \cons(x_2 , y)) : A^*}}
    }
    {\cdot \vdash f := \lamblto x {\lamblto y {\app {\app g x} {y} }} : (A \otimes A) \lto A^* \lto A^*}
  }
  {x : (A \otimes A)^* \vdash \fold(\nil , f)(x) : A^* }
\end{mathpar}
\caption{An example mapping out of abstract grammars}
\label{fig:kleeneabstractproof}
\end{figure}

\paragraph{Non-deterministic Finite Automata}
Automata play a critical
role in defining parsers, as they often serve as an operationalized and
executable presentation of a formal grammar. Therefore, it is crucial that we
can represent them internal to \theoryabbv. As a final example, consider the
representation of $(a^{*} \otimes b) \oplus c$ as a non-deterministic finite
automaton (NFA), shown in \cref{fig:exampleNFA}.

Our method of representing these automata internally is to provide an \emph{indexed}
inductive type of traces through the machine. An ordered, linear theory is
integral to our design of \theoryabbv ; however, there are some aspects of parsing that
cannot be captured through simple linear types alone. Often one wants to define a
\emph{family} of linear types varying over a set, such as a type of traces through an
NFA parameterized by the first state in the trace. For the example NFA, the data
type $\Trace$ is provided in \cref{fig:nfainductive}.

$\Trace~s$ denotes traces through the NFA beginning in state $s$ and ending in
an accepting state. In this instance there is only a single accepting state, and
thus the traces necessarily end in state 3. Let us dissect the structure
of $\Trace~s$ in more detail.

There are two kinds of constructors for a trace: those those modeling the
termination of a trace at a given state and those modeling transitions in the
NFA.\ The constructor $\mathsf{stop}$ denotes the first kind of constructor and
terminates the trace in state 3. The constructor
$\mathsf{2to3}$ denotes the $b$-labeled transition in the NFA from state 2 to state
3. In this case, we build a trace beginning at state 2
by consuming a parse of the label, $b$, and consuming a trace beginning at state 3.
Finally, the constructor $\mathsf{1to2}$ is built quite similarly as $\mathsf{2to3}$;
however, $\mathsf{1to2}$  may transition states without reading a character. This is because the
transition from state 1 to state 2 is labeled by the empty string $\epsilon$, so
transition consumes no input. In this case, when building a trace out of
state 1, it suffices to build a trace out of state 2 \emph{over the same string}.

Overall, leveraging the \emph{indexed} inductive types provides a concise interface
for defining the traces of the NFA as a family
of mutually recursive types varying coherently over indexing data drawn from
the states of the NFA.

Just as \cref{fig:kleenestarderivation} shows the \stringquote{ab} matches
$(a^{*} \otimes b) \oplus c$, we show in \cref{fig:nfaderivation} that \stringquote{ab} is
accepted by the NFA representation of this regular expression. This is denoted by
showing that \stringquote{ab} matches the trace grammar when the index is the
initial state of the NFA.\ Further, as we
will see in LATER SECTION, we can derive an isomorphism between parses of the
regular expression and accepting traces of their corresponding NFA.

\begin{figure}
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial] (1) {$1$};
    \node[state, below left of=1] (2) {$2$};
    \node[state, right of=2, accepting] (3) {$3$};

    \path[->] (1) edge[above] node{$\epsilon$} (2)
              (1) edge[right] node{$c$} (3)
              (2) edge[loop left] node{$a$} (2)
              (2) edge[below] node{$b$} (3);
  \end{tikzpicture}
  \caption{NFA for $(a^{*} \otimes b) \oplus c$}
  \label{fig:exampleNFA}
\end{figure}

\begin{figure}
\begin{align*}
\data &~\Trace : (s : \mathsf{Fin}~3) \to L~\where\\
      & \mathsf{stop} : \ltonl {(\Trace~3)}  \\
      & \mathsf{1to2} : \ltonl {(\Trace~2 \lto \Trace~1)} \\
      & \mathsf{2to2} : \ltonl {(a \lto \Trace~2 \lto \Trace~2)} \\
      & \mathsf{2to3} : \ltonl {(b \lto \Trace~3 \lto \Trace~2)}  \\
      & \mathsf{1to3} : \ltonl {(c \lto \Trace~3\lto \Trace~1)} \\
\end{align*}
\caption{$N$ as an indexed inductive type}
\label{fig:nfainductive}
\end{figure}

\begin{figure}
\begin{mathpar}
\inferrule
{
  \inferrule
  {
    \inferrule
    {~}
    {x : a \vdash x : a}
    \\
    \inferrule
    {
      \inferrule
      {~}
      {y : b \vdash y : b}
      \\
      \inferrule
      {~}
      {\cdot \vdash \mathsf{stop} : \Trace~3}
    }
    {y : b \vdash \mathsf{2to3}(y , \mathsf{stop}) : \Trace~2}
  }
  {x : a , y : b \vdash \mathsf{2to2}(x , \mathsf{2to3}(y , \mathsf{stop})) : \Trace~2}
}
{x : a , y : b \vdash \mathsf{1to2}(\mathsf{2to2}(x , \mathsf{2to3}(y ,
  \mathsf{stop}))) : \Trace~1}
\end{mathpar}
\caption{\stringquote{ab} is accepted by the NFA for $(a^{*} \otimes b) \oplus c$}
\label{fig:nfaderivation}
\end{figure}

\steven{TODO the example figures need to changed to not use $x$'s for linear variables}

%% 3. Third example: NFA traces to show the indexed inductive types

\section{Syntax and Typing for \theoryname}
\label{sec:tt}

\max{was this paragraph supposed to be deleted?}
Next we present the full details of the syntax and typing of \theoryname.
\theoryname has two kinds of types, linear types $A$ which represent formal
grammars, and non-linear types $X$, which act like ordinary types or sets. Both
linear and non-linear types are checked for well-formedness in a context
$\Gamma$ containing \emph{only} non-linear variables $\Gamma = x:X,\ldots$.

\begin{figure}
  \[
  \begin{array}{rrcl}
    \textrm{non-linear contexts} & \Gamma & ::= & \cdot \pipe \Gamma,x:X \\
    \textrm{non-linear types} & X , Y , Z & ::= & \mathsf{Nat} \pipe
                                                  \mathsf{Int} \pipe
                                                  \mathsf{Empty} \pipe
                                                  \mathsf{Unit} \pipe
                                                  \mathsf{SPF}\,X\pipe U \pipe L
                                                  \pipe \ltonl A \pipe \sum_{x:X} Y
                                                  \pipe \prod_{x:X} Y\pipe M =_A
                                                  N \\
    \textrm{linear contexts} & \Delta & ::= & \cdot \pipe \Delta , a:A\\
    \textrm{linear types} & A , B , C & ::= &
    c \pipe I \pipe A \otimes B \pipe \bigoplus\limits_{x:X} A \pipe \mu A_f M \pipe
    A \lto B \pipe B \tol A \pipe \bigwith\limits_{x:X} A \pipe \equalizer {a}{f}{g}\\
  \end{array}
  \]
  \caption{Overview of Types}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma ; \Delta \vdash e : A}

    \inferrule{~}{\Gamma ; a : A \vdash a : A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \\ \linctxwffjdg \Gamma {A \equiv B}}{\Gamma ; \Delta \vdash e : A}
    %
    \\
    %
    \inferrule{~}{\Gamma ; \cdot \vdash () : I}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : I \\ \Gamma ; \Delta_1',\Delta_2' \vdash e' : C}{\Gamma ; \Delta_1',\Delta,\Delta_2' \vdash \letin {()} e {e'} : C}
    %
    \\
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \\ \Gamma ; \Delta' \vdash e' : B}{\Gamma ; \Delta, \Delta' \vdash e \otimes e' : A \otimes B}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \otimes B \\ \Gamma ; \Delta'_1, a : A, b : B, \Delta'_2 \vdash e'}{\Gamma ;  \Delta_1', \Delta, \Delta'_2 \vdash \letin {a \otimes b} e {e'}}
    \\
    %
    \inferrule{\Gamma ; \Delta , a : A \vdash e : B}{\Gamma ; \Delta \vdash \lamblto a e : A\lto B}
    \and
    \inferrule{\Gamma ; \Delta' \vdash e' : A \\ \Gamma ; \Delta \vdash e : A \lto B}{\Gamma ; \Delta, \Delta' \vdash \applto {e'} {e} : B}
    \\
    %
    \inferrule{\Gamma ; a : A , \Delta \vdash e : B}{\Gamma ; \Delta \vdash \lambtol a e : B\tol A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \tol A \\ \Gamma ; \Delta' \vdash e' : A}{\Gamma ; \Delta', \Delta \vdash \apptol e {e'} : B}
    %
    \\
    %
    \inferrule{\Gamma, x : X ; \Delta  \vdash e : A}
              {\Gamma ; \Delta \vdash \dlamb x e : \LinPiTy x X A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : \LinPiTy x X A \\ \Gamma \vdash e' : X}{\Gamma ; \Delta \vdash e\,.\pi\,M : \subst A {e'} x}
    %
    \\
    %
    \inferrule{\Gamma \vdash M : X \quad \Gamma ; \Delta \vdash e : \subst A e x}{\Gamma ; \Delta \vdash \sigma\,M\,e : \bigoplus\limits_{x:X} A}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta \vdash e : \bigoplus\limits_{x:X} A \quad \Gamma, x : X ; \Delta'_1, a : A, \Delta'_2 \vdash e' : C}{\Gamma; \Delta'_1, \Delta, \Delta'_2 \vdash \letin {\sigma\,x\,a} e {e'}: C}
    %
    \and
    %
    \inferrule
    {\Gamma \vdash M : \ltonl A}
    {\Gamma ; \cdot \vdash M : A}
    %
    \and
    %
    \inferrule
    { \Gamma ; \Delta \vdash e : A \\
      \Gamma ; \Delta \vdash \applto {f}{e} \equiv \applto {g}{e}}
    {\Gamma ; \Delta \vdash \equalizerin{e} : \equalizer {e}{f}{g}}
    %
    \and
    %
    \inferrule
    {\Gamma ; \Delta \vdash e : \equalizer{a}{f}{g}}
    {\Gamma ; \Delta \vdash \equalizerpi {e} : A}
  \end{mathpar}
  \caption{Linear typing}
  \label{fig:linsyntax}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \inferrule{~}{\Gamma \vdash \Set_0 : \Set_1}
 %
    \and
%
    \inferrule{~}{\Gamma \vdash L_i : U_{i+1}}
    {}{}
  \end{mathpar}
  \caption{Universe Types}
\end{figure}

\begin{figure}
  \textrm{TODO: var, konstant, bigoplus, bigamp and tensor}\\
  \textrm{TODO: behavior of map}
  \begin{mathpar}
    \inferrule
    {\Gamma \vdash M : X}
    {\Gamma \vdash \mathsf{Var}\,M : \SPF\,X}

    \inferrule
    {\Gamma \vdash A : L}
    {\Gamma \vdash \mathsf{K}\,A : \SPF\,X}

    \inferrule
    {\Gamma,y:Y \vdash A : \SPF\,X}
    {\Gamma \vdash \bigoplus_{y:Y}\,A : \SPF\,X}

    \inferrule
    {\Gamma,y:Y \vdash A : \SPF\,X}
    {\Gamma \vdash \bigamp_{y:Y}\,A : \SPF\,X}

    \inferrule
    {\Gamma \vdash A : \SPF\,X\and \Gamma \vdash B : \SPF\,X}
    {\Gamma \vdash A \otimes B : \SPF\,X}

    \inferrule
    {A : \SPF\,X}
    {\el(A) : (X \to L) \to L}

    \el(\Var\,M) B = B\,M\and
    \el(\mathsf{K}\,A) B = A\and
    \el(\bigoplus_{y:Y} A) B = \bigoplus_{y:Y}\el(A)B\and
    \el(\bigamp_{y:Y} A) B = \bigamp_{y:Y}\el(A)B\and
    \el(A \otimes A') B = \el(A)B \otimes \el(A')B

  \inferrule
  {A : \textrm{SPF} X\and
  \Gamma,x:X;b: B x \vdash f : C x\and
  \Delta \vdash e : \el(A)(B)
  }
  {\Gamma;\Delta \vdash \mathsf{map}\,A\,(x.b.f)\,e : \textrm{el}(A)\, C}

  \map(\Var\,M)\,(x.b.f)\,e = f[M/x][e/b]\and
  \map(\K\,A)\,(x.b.f)\,e = e\and
  \map(\bigoplus\limits_{y:Y} A)\,(x.b.f)\,e =
  \letin{\oplusinj y a}{e}{\oplusinj y {\map(A)\,(x.b.f)\,e}}\\
  \map(\bigwith\limits_{y:Y} A)\,(x.b.f)\,e =
  \withlamb{y}{\map(A)\,(x.b.f)\,(\withprj a e)}\\
  \map(A \otimes A')\,(x.b.f)\,e =
  \letin{(a,a')}{e}{(\map(A)\,(x.b.f)\,a,\map(A')\,(x.b.f)\,a')}
  \end{mathpar}
  \caption{Strictly Positive Functors}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \inferrule*[right=IndIntro]
    {\Gamma \vdash A : X \to \textrm{SPF}\,X\and
     \Gamma \vdash M : X\and
     \Gamma; \Delta \vdash e : \textrm{el}(A\,M) (\mu\,A)}
    {\Gamma; \Delta \vdash \mathsf{roll}\, e : \mu\,A\, M}

    \inferrule*[right=IndElim]
    {\Gamma;\Delta \vdash e : \mu A M\and
     \Gamma,x:X;b:\el(A\,x)\,B \vdash f : B\,x
    }
    {\Gamma; \Delta \vdash \mathsf{fold}(A)(x.b.f)(M, e) : B\,M}

    \inferrule*[right=Ind$\beta$]
    {}
    {\fold(A)(x.b.f)(M, \roll(e)) =
      f[M/x][\map(A\,M)\,(y.a.\fold(A)(x.b.f)(y,a))\,e/b]}

    \inferrule*[right=Ind$\eta$]
    {\Gamma,x:X; a:\mu\,A\,x \vdash e' : B\,x
      \and \Gamma;\Delta \vdash e : \mu A M\\\\
      \Gamma,x:X; a':\el(A x)(\mu A) \vdash
      e'[x/x][\roll(a')/a] = f[x/x][\map(A\,x)\,(y.a.e'[y/x][a/a])\,a/b] : B\,x
    }
    {\Gamma; \Delta \vdash \fold(A)(x.b.f)(M,e) = e'[M/x][e/a] : B\,M}
  \end{mathpar}
  \caption{Indexed Inductive Linear Types}
\end{figure}

% \steven{TODO the intro to this section is to be condensed based on what is
%   included in the examples section as motivation and intuition building}

% Omission of the structural rules of a deductive system, such as in
% linear logic \cite{GIRARD19871}, offers precise control over how a value is used
% in a derivation. Linear logic omits the weakening and contraction rules
% to ensure that every value in context is used exactly once. This control enables
% \emph{resource-sensitive} reasoning, where we may treat a resource as
% \emph{consumed} after usage. This viewpoint is amenable to parsing applications,
% as we may treat characters of a string as finite resources that are consumed at
% parse-time. That is, when reading a string, the occurrence of any character is read once. Freely duplicating or dropping characters from a string changes the meaning of that string. \max{what about a backtracking parser?}. One may then envision a linear type system where the types comprise
% formal grammars generated over some alphabet $\Sigma$, and the type constructors
% correspond precisely to inductive constructions on grammars --- such as
% conjunction, disjunction, concatenation, etc.

% Programming in a purely linear term language is limiting due to the variable
% usage restrictions. Code in such a language can
% become unnecessarily verbose when ensuring the linearity invariant.
% To alleviate this
% concern, in 1995 Benton et al.\ proposed an alternative categorical
% semantics of linear logic based on adjoint interactions between linear
% and non-linear logics \cite{bentonMixedLinearNonlinear1995} ---
% appropriately referred to as a \emph{linear-non-linear} (LNL)
% system. This work is simply typed, so the boundary between linear and
% non-linear subtheories is entirely characterized via a monoidal
% adjunction between linear terms and non-linear terms.

% \steven{Describe this monoidal adjunction with $\ltonl A$ and the derivable
%   version of $F$}

% Inside of an LNL system, linearity may be thought of as an option that users can
% choose to deliberately invoke at deliberate points in their developments in an
% otherwise intuitionistic system. However, if we are wishing to treat parsers as
% linear terms over input strings, the non-linear fragment of an LNL theory does
% not really assist in the development of parsers. It is instead the case that
% parsers may benefit from a \emph{dependence} on non-linear terms.
% Through the approach described by Krishnaswami et al.\ in
% \cite{krishnaswami_integrating_2015},
% we may define a restricted form of dependent types.

% In particular, dependence
% on linear terms
% is disallowed; however, through dependence of a linear term on a non-linear
% index, we can recover the definition of Aho's indexed grammars \cite{AhoIndexed}
% internal to \theoryabbv. Although, we can define strictly more things, as Aho's
% indexed grammars provide restricted access to the index whereas we can do
% whatever with it.

% \steven{elaborate on indexed grammars in a more articulate way in the last sentence}
% \steven{Put the aho stuff in the subtheory section}
%

In this section we go on a tour of \theoryabbv. We start by presenting its syntax.
In order to avoid a notion of raw-term, we present it in an intrinsically-typed fashion,
where only well-typed terms are meaningful\max{this sentence says the same thing multiple times}. Next, we show how it admits a model in $\Set^{\String}$
where the linear types have natural interpretation in terms of formal grammars --- for instance,
the tensor corresponds to grammar concatenation. We conclude the section by showing how
classes of grammars correspond to certain sub-type-theories inside \theoryabbv.\max{why are you putting the entire paper into this section?}

\subsection{Syntax}
\label{subsec:syntax}

Below we describe \theoryname~(\theoryabbv), an instance of an LNL theory with dependent types
to serve as a deductive
setting for formal grammar theory. \theoryabbv axiomatizes the necessary
structure underlying $\Grammar$ to
specify and construct parsers.

The structural judgments are shown in \cref{fig:structjdg}, the typing
well-formedness in \cref{fig:typewf}, and the intuitionistic typing
rules in \cref{fig:inttyping}. These are mostly just as they appear in
\cite{krishnaswami_integrating_2015}. It has two main differences\max{you absolutely cannot just describe the differences between our calculus and one from prior work. This paper needs to be self-contained}. The
first one it the presence of two-distinct arrow types, one for adding
variables to the beginning of contexts and another one for adding
variables to the end of contexts. This is an adequate\max{adequate?} change in the
context of grammars, which are inherently non-symmetric. The second
change is the introduction of inductive types, which allows for the definition
of recursive grammars. The treatment of the non-linear
types is standard, so below we focus on the linear syntax.

\begin{figure}
  \begin{mathpar}
    \boxed{\ctxwff \Gamma}

    \inferrule{~}{\ctxwff \cdot}
    \and
    \inferrule{\ctxwff \Gamma \\ \ctxwffjdg \Gamma X}{\ctxwff {\Gamma, x : X}}

    \\

    \boxed{\linctxwff \Gamma \Delta}

    \inferrule{~}{\linctxwff \Gamma \cdot}
    \and
    \inferrule{\linctxwff \Gamma \Delta \\ \linctxwffjdg \gamma A}{\linctxwff \Gamma {\Delta, a : A}}

    \\

    \boxed{\ctxwffjdg \Gamma X}

    \inferrule{\Gamma \vdash X : U_i}{\ctxwffjdg \Gamma X}

    \boxed{\linctxwffjdg \Gamma A}

    \inferrule{\Gamma \vdash A : L_i}{\linctxwffjdg \Gamma A}

    \\

    \boxed{\ctxwffjdg \Gamma {X \equiv Y}}

    \inferrule{\Gamma \vdash X \equiv Y : U_i}{\ctxwffjdg \Gamma {X\equiv Y}}

    \boxed{\linctxwffjdg \Gamma {A \equiv B}}

    \inferrule{\Gamma \vdash A \equiv B : L_i}{\linctxwffjdg \Gamma {A \equiv B}}

  \end{mathpar}
  \caption{Structural judgments}
  \label{fig:structjdg}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma \vdash X : U_{i}}

    \boxed{\Gamma \vdash A : L_{i}}

    \inferrule{~}{\Gamma \vdash U_i : U_{i+1}}
 %
    \and
%
    \inferrule{~}{\Gamma \vdash L_i : U_{i+1}}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \PiTy x X Y : U_i }%
%
    \and
%
    \inferrule{\Gamma\vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \SigTy x X Y : U_i}
%
    \and
%
    \inferrule{~}{\Gamma \vdash 1 : U_i}
%
    % \and
%
    % \inferrule{\Gamma \vdash A : L_i}{\Gamma \vdash G A : U_i}

    \and
%
    \inferrule{~}{\Gamma \vdash I : L_i}
 %
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \otimes B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \lto B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash B \tol A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinPiTy x X A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinSigTy x X A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \quad \{\Gamma \vdash e_i : X\}_i}{\Gamma \vdash e_1 =_X e_2 : U_i}
    %
    \and
    %
    \inferrule{~}{\Gamma \vdash \top : L_i}
%
    \and

    \inferrule{~}{\Gamma \vdash \bot : L_i}

    \and
    \inferrule{c \in \Sigma}{\Gamma \vdash c : L_0}
    %
    \and
    %
    \inferrule{\Gamma, x : L_i \vdash A : L_i \and A \textrm{ strictly positive}}{\Gamma \vdash \mu x.\, A : L_i}
    %
    \and
    %
    \inferrule
    {\Gamma \vdash A : L_i}
    {\Gamma \vdash \ltonl { A } : U_i}
    %
    \and
    \inferrule
    {\Gamma ; \Delta \vdash f : \ltonl {(A \lto B)} \\ \Gamma \vdash g : \ltonl { (A \lto B) }}
    {\Gamma ; \Delta \vdash \equalizer {a}{f}{g} : L_i}
  \end{mathpar}
  \caption{Type well-formedness}
  \label{fig:typewf}
\end{figure}

\begin{figure}
  \begin{mathpar}
  \boxed{\Gamma \vdash M : X}

  \inferrule{~}{\Gamma, x : X, \Gamma' \vdash x : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : Y \quad \ctxwffjdg \Gamma {X \equiv Y}}{\Gamma \vdash M : X}
  %
  \and
  %
  \inferrule{~}{\Gamma \vdash () : 1}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : X \\ \Gamma \vdash N : \subst Y M x}{\Gamma \vdash (M, N) : \SigTy x X Y}
  %
  \and
%
  \inferrule{\Gamma \vdash M : \SigTy x X Y}{\Gamma \vdash \pi_1\, M : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : \SigTy x X Y}{\Gamma \vdash \pi_2\, M : \subst Y {\pi_1\, M} x}
  \and
  \inferrule{\Gamma, x : X \vdash M : Y}{\Gamma \vdash \lamb x M : \PiTy x X Y}
  %
  \and
  %
  \inferrule{\Gamma \vdash M : \PiTy x X Y \\ \Gamma \vdash N : X}{\Gamma \vdash \app M {N} : \subst Y {N} x}
  %
  \and
  %
  \inferrule{\Gamma \vdash M \equiv N : X}{\Gamma \vdash \mathsf{refl} : M =_X N}
  \and
  \inferrule{\Gamma ; \cdot \vdash e : A}{\Gamma \vdash e : \ltonl A}

  \textrm{TODO: inductive types (boolean, nat) with their eliminators allowed in }
  \end{mathpar}
  \caption{Intuitionistic typing}
  \label{fig:inttyping}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma \vdash M \equiv N : X}

    \inferrule{\Gamma \vdash p : M =_X N}{\Gamma \vdash M \equiv N : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \app {(\lamb x M)} {N} \equiv \subst x M {N} : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash m \equiv \lamb x {\app M x} : \PiTy x X Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_1\, (M, N) \equiv M : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_2\, (M , N) \equiv N : \subst x {M} Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash M \equiv (\pi_1\, M, \pi_2\, M) : \SigTy x X Y}
%
    \and
%
    % \inferrule{~}{}
%
    \inferrule{~}{\Gamma \vdash M \equiv N : 1}
%
    % \and
%
    % \inferrule{~}{\Gamma \vdash G\, (G^{-1} \, t) \equiv t : G A}

    \\

    \boxed{\Gamma ; \Delta \vdash e \equiv e' : A}

    % \inferrule{~}{\Gamma; \cdot \vdash G^{-1}\, (G \, t ) \equiv t: A}
%
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lamblto a e)} {e'} \equiv \subst e {e'} a : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lamblto a {\app e a} : A \lto B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lambtol a e)} {e'} \equiv \subst e {e'} a : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lambtol a {\app e a} : B \tol A}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\dlamb x e)} {M} \equiv \subst {e} M x : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \dlamb x {\app e x} : \LinPiTy x X A}
%
    \and
%
%     \inferrule{~}{\Gamma; \Delta \vdash e \equiv e' : \top}
% %
%     \and
%
%     \inferrule{~}{\Gamma; \Delta \vdash e_i \equiv \pi_i (e_1, e_2) : A_i}
% %
%     \and
% %
%     \inferrule{~}{\Gamma; \Delta \vdash e \equiv (\pi_1 e, \pi_2 e) : A\& B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} {()} e \equiv e : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} e {\subst {e'} {()} a} \equiv \subst {e'} e a : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {a \otimes b} {e \otimes e'} e'' \equiv e'' \{ e/a , e'/b \} : C}
%
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {a \otimes b} e {\subst {e'} {a \otimes b} c} \equiv \subst {e'} e c : C}
%
    \and
%
    \inferrule{~}{\Gamma;\Delta \vdash \letin {(x, a)} {(M, e)} {e'} \equiv e' \{ M/x , e/a \} : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {(x, a)} e {\subst {e'} {(x, a)} y} \equiv \subst {e'} e y : C}
    \and
    %
    \inferrule
    {~}
    {\Gamma ; \Delta \vdash \equalizerpi {\equalizerin {e}} \equiv e : A}
    %
    \and
    %
    \inferrule
    {~}
    {\Gamma ; \Delta \vdash \equalizerin {\equalizerpi {e}} \equiv e : A}
\end{mathpar}
  \caption{Judgmental equality}
  \label{fig:jdgeq}
\end{figure}
\steven{Do we need inverse for $\ltonl A$ in judgemental equality figure?}
\steven{Do we need to say anything about the equalizer proofs being unique in
  judgemental equality figure?}

\subsection{Subtheories}
Each choice of connective has direct implications on the expressivity of
\theoryabbv. In particular, the set of connectives used will determine where the
grammar semantics is placed in
the Chomsky hierarchy.

\paragraph{Regular Expressions}
We may realize regular expression as a subtheory
of our language by restricting the type constructors to the
linear unit, characters, $\otimes$, $\oplus$, and Kleene star. Classically, the
languages recognized by these are exactly the regular languages --- the lowest
on the Chomsky hierarchy.

\paragraph{$\mu$-Regular Expressions} Similarly, we may restrict the connectives
to be linear unit, characters, $\otimes$, $\oplus$, and arbitrary recursion via
left-fixed point rather than only Kleene star.
Instead of regular expressions these correspond to Lei{\ss}'s $\mu$-regular
expressions, which are known to be equivalent to context-free grammars
\cite{leiss,krishnaswami_typed_2019}.

\paragraph{Beyond Context-Free Grammars} The previous two subtheories induce as semantics
regular grammars and context-free grammars, respectively; however, by including
the LNL dependent types we may actually express the entirety of the Chomsky
hierarchy. Through use of the LNL dependent types, we may encode indexed
grammars, which are known to be properly between context-free and
context-sensitive grammars within the Chomsky hierarchy \cite{AhoIndexed}. We
will further see in \cref{subsubsec:tm} how to use dependence to encode Turing machines internal to our
calculus, which of course induces a semantics of unrestricted grammars.

\paragraph{Indexed Grammars}
The LNL dependent types can be used to encode Aho's indexed grammars
\cite{AhoIndexed} internal to \theoryabbv. Although, we can define strictly more
things with this non-linear index, as we have unrestricted access to the index
whereas Aho's grammars can only manipulate it in a particular way.

\steven{Elaborate on these, and note that they are bw context free and context
  sensitive on the Chomsky hierarchy}

\section{Formal Grammar Theory in \theoryname}
\label{sec:applications}

\steven{this section is mostly word vomit, need to edit}

\subsection{Grammar Properties}
\steven{Mention that $\top$ and $\String$ are iso. Highlight this as a needed axiom}

%% 1. Basic concepts: unambiguity, parseability, weak and strong equivalence
\paragraph{Unambiguity}
In the grammars-as-presheaves model of \theoryabbv, we say that a grammar is unambiguous if there is at most
one parse tree for any input string. There are several ways to capture this
concept inside of the theory of \theoryabbv.

In the presence of distributive coproducts, the following are equivalent:
\begin{itemize}
  \item $A$ is an unambiguous grammar
  \item The morphism $! : A \to \top$ is a monomorphism
  \item Given a grammar $B$, all morphisms $B \to A$ are equal
  \item $A \& A \cong A$
\end{itemize}

\paragraph{Parseability}
We say $A$ is \emph{parseable} if there exists a type $B$ and a map
$\top \to A \oplus B$. Because $\top \cong \String$, this says that any string
can be parsed into either an $A$ or a $B$.

If $\top \cong A \oplus B$, call $A$ \emph{unambiguously parseable}.

\paragraph{Decidability}
In the case that $A$ is parseable with respect to $\neg A$ --- i.e.
there's a map $\top \to A \oplus \neg A$ --- then call $A$ \emph{decidable}.

\paragraph{Equivalence}
Grammars $A$ and $B$ are \emph{weakly equivalent} if there exists maps
$f : A \to B$ and $g : B \to A$.

If $f$ and $g$ form an isomorphism, then $A$ and $B$ are \emph{strongly
  equivalent}.
\steven{TODO cite that these are the same as Chomsky's notions of equivalence}

The induced notion of isomorphism in $\Grammar$ encodes that two grammars are equivalent if there is
a bijective translation of parses. In other words, two grammars are isomorphic
if they capture the same structural descriptions of strings, which is
precisely Chomsky's notion of \emph{strong equivalence} of
grammars \cite{chom1963}. In works like
\cite{yoshinaga2002formal}, this notion of strong equivalence is often treated
like the appropriate choice of ``sameness'' for two grammars. When abstracting over the latent
structure of formal grammars and describing it in the the language of
categories, it is encouraging to see the same definition of equivalence arise
naturally.

\subsection{Regular Expressions and Finite Automata}
%% 2. Regex, NFA, DFA

\newcommand{\states}{\mathsf{states}}
\newcommand{\labelt}{\mathsf{label}}
\newcommand{\transitions}{\mathsf{transitions}}
\newcommand{\epstransitions}{\epsilon\mathsf{transitions}}
\newcommand{\isAcc}{\mathsf{isAcc}}
\newcommand{\init}{\mathsf{init}}
\newcommand{\src}{\mathsf{src}}
\newcommand{\dst}{\mathsf{dst}}
\newcommand{\epssrc}{\epsilon\mathsf{src}}
\newcommand{\epsdst}{\epsilon\mathsf{dst}}

\paragraph{Nondeterministic Finite Automata} In \cref{sec:type-theory-examples}, we saw the demonstration of
one particular NFA corresponding to the regular expression
$(a^{*} \otimes b) \oplus c$. The definition of the linear type of traces
through this
NFA, as
presented in \cref{fig:nfainductive}, inspects the concrete structure of the
NFA.\ When generalizing from this example to an arbitrary NFA, we do not have the
luxury of inspecting a concrete defintion. Therefore, we abstract over the
specification of an arbitrary NFA and define traces with respect to that
specification.

Let the data of an NFA $N$ be given by the following nonlinear data,
\begin{itemize}
\item $N.\states$ --- a finite set of states
\item $N.\init$ --- the initial state
\item $N.\isAcc$ --- a decidable proposition over $N.\states$ denoting which
        states are accepting
\item $N.\transitions$ --- a finite set of labeled transitions
\item $N.\labelt$ --- a mapping from $N.\transitions$ to the character labeling
        the transition
\item $N.\src$ --- a mapping from $N.\transitions$ to source state of a transition
\item $N.\dst$ --- a mapping from $N.\transitions$ to destination state of a transition
\item $N.\epstransitions$ --- a finite set of $\epsilon$-transitions
\item $N.\epssrc$ --- a mapping from $N.\epstransitions$ to source state of a $\epsilon$-transition
\item $N.\epsdst$ --- a mapping from $N.\epstransitions$ to destination state of a $\epsilon$-transition
\end{itemize}

For the above definition of an NFA $N$, in \cref{fig:nfatrace} we define a linear type of traces
through $N$. Traces in $N$ may be built through one of three constructors. We
may terminate a trace at an accepting state with the constructor $\nil$. If we
had a trace beginning at the destination state of a transition, then we may use
the $\cons$ constructor to linearly combine that trace with a parse of the label
of the transition to build a trace beginning at the source of the transition.
Finally, if we had a trace beginning at the destination of an
$\epsilon$-transition then we may use $\epscons$ to pull it back along the
$\epsilon$-transition and construct a trace beginning at the source of the $\epsilon$-transition.

\begin{figure}
\begin{align*}
\data &~\Trace_{N} : (s : N.\states) \to L~\where\\
      & \nil : \ltonl {\left( \LinPiTy {s} {N.\states} {\LinPiTy {\mathsf{acc}} {N.\isAcc(s)} {\Trace_{N}~s}} \right)}  \\
      & \cons : \ltonl {\left( \LinPiTy {t} {N.\transitions} {(N.\labelt~t) \lto \Trace_{N}~(N.\dst(t)) \lto \Trace_{N}(N.\src(t))} \right)} \\
      & \epscons : \ltonl {\left( \LinPiTy {t} {N.\epstransitions} {\Trace_{N}~(N.\epsdst(t)) \lto \Trace_{N}(N.\epssrc(t))} \right)}
\end{align*}
\caption{Traces through the NFA $N$ as an indexed inductive type}
\label{fig:nfatrace}
\end{figure}

\paragraph{Determinstic Finite Automata}
Just as above, we provide a specification for an arbitrary DFA. A DFA $D$ is
given by the following data,

\begin{itemize}
\item $D.\states$ --- a finite set of states
\item $D.\init$ --- the initial state
\item $D.\isAcc$ --- a decidable proposition over $D.\states$ denoting which
      states are accepting
\item $D.\delta$ --- a transition function. Given a character $c$ and a state
        $s$, $D.\delta_{c}(s)$ is the state for which there is a transition $s \overset{c}{\to} D.\delta_{c}(s)$
\end{itemize}

The linear type of traces through $D$ is given in \cref{fig:dfatrace}. These
traces are either terminated in an accepting state with the $\nil$ constructor,
or the $\cons$ constructor builds a trace out of state $s$ by linearly combining
a parse of some character $c$ with a
trace out of the state $D.\delta_{c}(s)$.

\begin{figure}
\begin{align*}
\data &~\Trace_{D} : (s : D.\states) \to L~\where\\
      & \nil : \ltonl {\left( \LinPiTy {s} {D.\states} {\LinPiTy {\mathsf{acc}} {D.\isAcc(s)} {\Trace_{D}~s}} \right)}  \\
      & \cons : \ltonl {\left( \LinPiTy {c} {\Sigma} {\LinPiTy {s} {D.\states} {c \lto \Trace_{D}~(D.\delta_{c}(s)) \lto \Trace_{D}(s)}} \right)}
\end{align*}
\caption{Traces through the DFA $D$ as an indexed inductive type}
\label{fig:dfatrace}
\end{figure}

\newcommand{\Agda}{Formalized in Agda}
\begin{theorem}[Determinization, \Agda]
  Given an NFA $N$, there exists a DFA $D$ such that
  $\Trace_{N}~N.\mathsf{init}$ is weakly equivalent to $\Trace_{D}~D.\mathsf{init}$.
\end{theorem}

\begin{proof}
  We present a variant of the classic powerset construction CITE. First, we build
  the DFA $D$. Define the
  states of $D$ to be the $\mathbb{P}_{\epsilon}(N.\mathsf{states})$ --- the type of $\epsilon$-closed subsets of
  $N.\mathsf{states}$. A subset is accepting in $D$ if it contains an accepting state
  from $N$. Construct the initial state of $D$ as the
  $\epsilon$-closure of $N.\mathsf{init}$. Lastly define the transtion function
  of $D$ to send the subset $X$ under the character $c$ to the
  $\epsilon$-closure of all the states reachable from $X$ via a transition
  labelled with the character $c$.

  We demonstrate the weak equivalence between $\Trace_{N}~(N.\mathsf{init})$ and
  $\Trace_{D}~(D.\mathsf{init})$ by constructing morphisms between the two
  grammars.

  To build a term
  $\Trace_{N}~(N.\mathsf{init}) \vdash \Trace_{D}~(D.\mathsf{init})$, it
  suffices to construct the stronger term

  \[
    f : \ltonl {\left(\Trace_{N}~(s) \lto \bigwith\limits_{X : D.\mathsf{states}} \bigwith\limits_{s \in X} \Trace_{D}~X\right)}
  \]
  where $s$ is arbitary and then instantiate it at $s = N.\mathsf{init}$, $X = D.\mathsf{init}$.

Because
the indexing state $s$ is now arbitrary, we can define the desired term via
induction the trace through the NFA.

If the NFA trace is terminating at
accepting state
$q$ and $q \in X$,
then we return a trace in $D$ that terminates at
$X$, which is accepting because it contains the accepting state $q$.

If the NFA trace is a labeled transition $s \overset{c}{\to} s'$ such that
$s \in X$, then we first decompose the trace to a parse of the character $c$ and a
trace starting from $s'$ --- that is, to a term of type
$c \otimes \Trace_{N}(s')$. We then inductively call $f$ on traces out of $s'$
to get traces in a parse of $c$ concatenated with a family of traces
parametrized by $\epsilon$-closed subsets containing $s'$.
. We instantiate this family at $\delta_{c}(X)$, as
$s'$ is reachable from $s$ by a single transition labelled by $c$, so
$s' \in \overline{\epsilon}\overline{c}{X} = D.\delta_{c}(X)$. Therefore we can
linearly combine the parse of character $c$ and the trace out of $D.\delta_{c}(X)$
to get a trace in $D$ rooted at $X$.


\[\begin{tikzcd}
    {c \otimes \mathsf{Trace}_{N}(s')} &
    {c \otimes \bigwith\limits_{Y : D.\mathsf{states}} \bigwith\limits_{s' \in Y} \Trace_{D}~Y} &
    {c \otimes \mathsf{Trace}_{D}(D.\delta_{c}(X))} & {\mathsf{Trace}_{D}(X)}
	\arrow["{id\otimes f}", from=1-1, to=1-2]
	\arrow["{id\otimes\pi}", from=1-2, to=1-3]
	\arrow["{D.\mathsf{cons}}", from=1-3, to=1-4]
\end{tikzcd}\]

Lastly, if the NFA trace was an $\epsilon$-transition
$s \overset{\epsilon}{\to} s'$ with $s \in X$, then the construction is quite similar. In this
case, we again inductively call $f$ on traces out of $s'$ except we instantiate
the choice of subset containing $s'$ with $X$ itself. We may do this as all
states in $D$ are taken to be $\epsilon$-closed, therefore $s' \in X$ because
$s$ is in $X$ and $\epsilon$-transitions to $s'$.

\[\begin{tikzcd}
    {\mathsf{Trace}_{N}(s')} &
    {\bigwith\limits_{Y : D.\mathsf{states}} \bigwith\limits_{s' \in Y} \Trace_{D}~Y} &
    {\mathsf{Trace}_{D}(X)}
	\arrow["{id\otimes f}", from=1-1, to=1-2]
	\arrow["{id\otimes\pi}", from=1-2, to=1-3]
\end{tikzcd}\]

This concludes the construction of $f$.

To construct a term from DFA traces to NFA traces, we similarly strengthen our
induction hypothesis and build a term
\[
  g : \ltonl {\left( \Trace_{D}(X) \lto \bigoplus\limits_{s : N.\mathsf{states}}\bigoplus\limits_{s \in X} \Trace_{N}(s) \right)}
\]

Let's turn our heads for a moment to the content behind the traces in $D$.
Traces from subset $X$ to subset $Y$ denote the \emph{existence} of some trace
in the $N$ from some $x \in X$ to some $y \in Y$. In general it is not clear how
to extract out a choice of trace in $N$, which is what $g$ is asking of us.
However, we may further take the states and transitions of $N$ to be ordered.
This allows us to
induce an order on traces in $D$ from $X$ to $Y$. From these orderings we can then
define the necessary choice functions on each type by taking the smallest with
respect to the ordering.

We define $g$ by induction on traces through $D$. First if the trace stops at
some accepting $\epsilon$-closed subset $X$, then we may choose some state
$s \in X$ such that $s$ is accepting and terminate the trace in $N$ at $s$.

Next if the trace in $D$ is defined via transition
$X \overset{c}{\to} D.\delta_{c}(X)$ then we may choose some state in
$s \in D.\delta_{c}(X)$. Inductively, $g$ sends the trace rooted at
$D.\delta_{c}(X)$ to one rooted at $s$. By definition of $D.\delta_{c}(X)$,
there exists some $s' \in X$ such that $s'$ tranitions to $s$ by some number of
$\epsilon$-transitions followed by a transition labeled by $c$.
We order all
such traces of this form and choose the smallest one, which is then linearly
combined with the parse of $c$ to build an $N$ trace. While using
$N.\mathsf{cons}$ to build this final trace, we appropriately shift the indexing
data to $s'$, which is adherent to the defintion of $D.\delta_{c}(X)$. This concludes the
definition of $g$.

\[\begin{tikzcd}
    {c \otimes \mathsf{Trace}_{D}(D.\delta_{c}(X))} &
    {c \otimes \bigoplus\limits_{s : N.\mathsf{states}} \bigoplus\limits_{s \in D.\delta_{c}(X)} \Trace_{N}~s} \\
    {\bigoplus\limits_{s' : N.\mathsf{states}} \bigoplus\limits_{s' \in X} \Trace_{N}~s'} &
    {\bigoplus\limits_{s : N.\mathsf{states}} \bigoplus\limits_{s \in D.\delta_{c}(X)} \left( c \otimes \Trace_{N}~s \right)}
	\arrow["{id\otimes g}", from=1-1, to=1-2]
	\arrow["{\mathsf{dist}}", from=1-2, to=2-2]
	\arrow["{}", from=2-2, to=2-1]
\end{tikzcd}\]

Therefore $\Trace_{N}(N.\mathsf{init})$ and $\Trace_{D}(D.\mathsf{init})$ are
weakly equivalent, and thus $N$ and $D$ recognize the same language of strings.
\end{proof}

\begin{theorem}[Thompson's Construction, \Agda]
  Given a regular expression $R$, there exists an NFA $N$ such that $R$ is
  strongly equivalent to $\Trace_{N}(N.\init)$.
\end{theorem}

\begin{proof}
In this proof, we present a variant of Thompson's construction
\cite{thompsonProgrammingTechniquesRegular1968} in which we induct on the
structure of $R$ to construct strongly equivalent NFAs. We will sketch this
proof by walking through some of the inductive cases, the remaining cases follow
similar reasoning and the details may be found in the formalized artifact.

\begin{figure}
  \begin{tikzpicture}[node distance = 17mm ]
    \node[state, initial] (0) {$0$};
    \node[state, right of=0, accepting] (1) {$1$};

    \path[->] (0) edge[above] node{$c$} (1);
  \end{tikzpicture}
  \caption{NFA $N_{c}$ for a single character $c$}
  \label{fig:litnfa}
\end{figure}

First handle the case where $R$ is a literal character $c$. The NFA $N_{c}$ pictured in
\cref{fig:litnfa} is strongly equivalent to the grammar $c$. To demonstrate this
strong equivalence, we build terms between $c$ and $\Trace_{N_{c}}(N_{c}.\init)$
and show they are mutually inverse.

Define $f : \ltonl {(c \lto \Trace_{N_{c}}(N_{c}.\init))}$ as follows,

\[
  a : c \vdash f := \cons(a, \nil) : \Trace_{N_{c}}(N_{c}.\init)
\]

Define $g : \ltonl {(\Trace_{N_{c}}(N_{c}.\init) \lto c)}$ via induction on the
trace. Map traces beginning in state 1, which are necessarily built via $\nil$,
to the linear unit $I$. Then we must handle traces beginning in state 0, which
are necessarily built via $\cons$. We decompose such a trace into a parse of the
character $c$ and a trace beginning in state 1. We inductively transform the
trace beginning in state 1 into a term of type $I$. Finally, we use the right
unitor to turn a term of type $c \otimes I$ into one of type $c$.

\[\begin{tikzcd}
    {c \otimes \mathsf{Trace}_{N_{c}}(1)} &
    {c \otimes I} &
    c
	\arrow["{id\otimes g}", from=1-1, to=1-2]
	\arrow["{}", from=1-2, to=1-3]
\end{tikzcd}\]

Note that the grammar $c$ is unambiguous, so $g \circ f$ is equal to the
identity on $c$. Additionally, we can show that $f$ carries the structure of an
indexed algebra homomorphism for the family of functors defining the trace type
$\Trace_{N_{c}}$. Because $g$ is defined via induction, it carries the structure
of an algebra homomorphism as well. Thus, $f \circ g$ is also an algebra
homomorphism. $f \circ g$ and the identity map are both then algebra
homomorphisms from $\Trace_{N_{c}}$ to itself. Because $\Trace_{N_{c}}$ is the
initial algebra for the relevant family of functors, there is only one
homomorphism from it to itself and thus $f \circ g = \mathsf{id}$. Therefore,
$f$ and $g$ are mutually inverse and the regular expression is strongly
equivalent to the traces through the NFA in this case.

Now consider the case where $R$ is the disjunction of two regular expressions
$R_{1}$ and $R_{2}$. Inductively, we may build NFAs $N_{1}$ and $N_{2}$ that are
strongly equivalent to $R_{1}$ and $R_{2}$, respectively. We define $N_{\oplus}$
as a new NFA built as the disjunction of $N_{1}$ and $N_{2}$.
$N_{\oplus}.\states$ comprises the states from $N_{1}$, the states from
$N_{2}$, and a new marked initial state. The accepting states are precisely
those that were accepting in the subautomata. The initial state has an
$\epsilon$-transition to each of the initial states of the subautomata. Once in
$N_{1}.\init$, the downstream transitions are a copy of those in $N_{1}$,
likewise for $N_{2}.\init$.  That is, $N_{\oplus}$ contains a copy of each
$N_{1}$ and $N_{2}$, and we choose which copy to inhabit by deciding which
$\epsilon$-transition out of the initial state to take. A schematic
representation of this automaton is given in \cref{fig:disjunctionnfa}.

\begin{figure}
  \begin{tikzpicture}[node distance = 17mm ]
    \node[state, initial] (init) {$\init$};
    \node[state, above right of=init] (init1) {$N_{1}.\init$};
    \node[state, below right of=init] (init2) {$N_{2}.\init$};
    \node[right of=init1] (dots1) {$\dots$};
    \node[right of=init2] (dots2) {$\dots$};
    \node[state, accepting, right of=dots1] (acc1) {$s_{1}$};
    \node[state, accepting, right of=dots2] (acc2) {$s_{2}$};

    \path[->] (init) edge[below] node{$\epsilon$} (init1)
              (init) edge[above] node{$\epsilon$} (init2)
              (init1) edge[above] node{} (dots1)
              (init2) edge[above] node{} (dots2)
              (dots1) edge[above] node{} (acc1)
              (dots2) edge[above] node{} (acc2);
  \end{tikzpicture}
  \caption{NFA $N_{\oplus}$ as a disjunction of $N_{1}$ and $N_{2}$}
  \label{fig:disjunctionnfa}
\end{figure}

Because $N_{1}$ and $N_{2}$ are each strongly equivalent to $R_{1}$ and $R_{2}$,
it suffices to show that
$\Trace_{N_{1}}(N_{1}.\init) \oplus \Trace_{N_{2}}(N_{2}.\init)$ is strongly
equivalent to $\Trace_{N_{\oplus}}(N_{\oplus}.\init)$.

Just like the case of a single character, we induct on traces when defining this
strong equivalence. First we map out of $\Trace_{N_{\oplus}}(s)$. If $s$ is a
state from $N_{1}$, then we send the trace from $N_{\oplus}$

\steven{TODO finish this, my brain isn't working rn}


\end{proof}

\subsection{LL(1)}
%% 4. LL(1) parser

\steven{TODO impute LL(1) example, mention that its weak equivalence is mechanized}

\subsection{Turing Machines}
%% 5. Expressiveness: Turing Machines
The expressive power of \theoryabbv spans the entire Chomsky hierachy, as we may
encode the acccepting traces of Turing machines as linear types.

\begin{figure}
\begin{align*}
\data &~\mathsf{Turing}~(t : \mathbb{Z} \to \Gamma): L~\where\\
      & \mathsf{nil} : \ltonl {\left(\LinPiTy {t} {\mathbb{Z} \to \Gamma} {\mathsf{Turing}~t} \right)} \\
      & \mathsf{snoc} : \ltonl {(\mathsf{Turing} \lto A^{*} \lto A^{*})}
\end{align*}
\caption{Kleene Star as an inductive non-linear type}
\label{fig:turinginductive}
\end{figure}

However, unlike
the traces through the automata we have seen until now, the encoding of Turing
machines

We nonlinearly encode the tape of the Turing machine as
function from $\mathbb{Z} \to \Gamma$, where $\Gamma$ is your tape alphabet. The
grammar of traces for a Turing machine linear consumes the input string and
writes it to the tape. From here, we nonlinear reason about machine transitions.
We execute the machine and track its current state as part of the nonlinear
indexing data.


%% Make sure to say which results are mechanized

\subsection{Semantic Actions}
\steven{TODO edit this. synthesize it with the later presentation of semantic
  actions as models, and then remove the model section}
All constructions discussed until now detail how a string may be parsed into a
\emph{concrete} syntax tree; however, this is often not the representation that later
compiler passes would take in. Instead, they often take in an \emph{abstract}
syntax tree that forgets syntactic details that are superfluous to the later
passes. For example, after using parentheses to disambiguate a parse of a string
into a concrete syntax tree, we may remove these parentheses to have a cleaner
structured representation of the underlying data. The procedure that takes in a
concrete syntax tree and returns an abstract one is referred to as a
\emph{semantic action}.

Given a linear type $A$, the inhabitants of $A$ serve as concrete syntax. We may
then chose a non-linear type $X$ that serves as abstract syntax. A semantic
action can then be defined as $\square (A \lto \Delta X)$ where
$\Delta X = \LinSigTy x X \top$.
\steven{Note here that even though $\Delta$ is a map from nonlinear types into
  linear ones, it is very different from $F$/whatever we call that half of the adjunction}

% \steven{Everything in this section below this marker is outdated and needs to be
% folded into the above or cut}

% \steven{Need a different segue because canonicity is no longer before this}
% The canonicity theorem from the previous section gives a tight connection
% between terms in our type theory and the theory of formal languages. In this section
% we further explore these connections by using the fact that, classically
% formal language theory is closely related to the study of automata.
% In the Chomsky hierarchy, each language class is associated to a class of
% automata that serve as recognizers. Internal to \theoryabbv,
% we can characterize these language classes syntactically; moreover, we
% demonstrate the equivalence of these language classes to its associated automata class as a
% proof term within our logic.

% \subsection{Non-deterministic Finite Automata}
% \label{subsec:finiteaut}
% \begin{figure}
%   \begin{tikzpicture}[node distance = 25mm ]
%     \node[state, initial, accepting] (1) {$1$};
%     \node[state, below left of=1] (2) {$2$};
%     \node[state, right of=2] (3) {$3$};

%     \path[->] (1) edge[above] node{$b$} (2)
%               (1) edge[below, bend right, left=0.3] node{$\epsilon$} (3)
%               (2) edge[loop left] node{$a$} (2)
%               (2) edge[below] node{$a, b$} (3)
%               (3) edge[above, bend right, right=0.3] node{$a$} (1);
%   \end{tikzpicture}
%   \caption{An example NFA}
%   \label{fig:NFA}
% \end{figure}

% Classically, a \emph{nondeterministic finite automaton} (NFA) is a finite state machine where
% transitions are labeled with characters from a fixed alphabet $\Sigma$. These
% are often represented formally as a 5-tuple $(Q, \Sigma, \delta, q_{0}, F)$,

% \begin{itemize}
%   \item $Q$ a finite set of states
%   \item $\Sigma$ a fixed, finite alphabet
%   \item $\delta : Q \times (\Sigma \cup \{ \varepsilon\}) \to \mathcal{P}(Q)$ the labeled transition function
%   \item $q_{0} \in Q$ the start state
%   \item $F \subset Q$ a set of accepting states
% \end{itemize}



% \subsection{A DFA Parser}
% \label{subsec:regexparser}

% Just as we encoded traces of NFAs as grammars, we likewise
% encode the traces of a DFA as grammars. The key difference
% between NFAs and DFAs is \emph{determinism} --- meaning,
% that in a state $q$ inside of DFA, given a character $c$ there
% will be exactly one transition that we may take leaving $q$
% with label $c$. For us, this changes the definition of valid
% transitions for a DFA, instead of the definition of
% $\mathsf{Trans}$ provided in \cref{subsec:finiteaut} DFAs
% obey

% \begin{gather*}
%  \mathsf{State} \in \{ g_{q} : q \in Q \} \\
%  \mathsf{Trans}(q) ::= \bigoplus_{c \in \Sigma} (c \otimes \mathsf{State})
% \end{gather*}

% Meaning, each state has a transition for every character. \emph{Note:} the above use
%   of a disjunction over $\Sigma$ is a bit of abuse of notation.
%   As $\Sigma$ is a finite alphabet, we wish to think of this as an iterated
%   binary sum, or perhaps as a finitely indexed sum defined via linear dependent
%   sums in the same manner by which we defined binary sums.

% We now wish to define a parser term for DFA grammars. In
% particular, for a DFA $D$ we want to build a term,

% \[
%   w : \String \vdash \mathsf{parse}_{D} : \mathsf{AccTrace}_{D} \oplus \top
% \]

% where left injection into the output type denotes acceptance
% by the parser, and right injection denotes rejection. To
% build such a parser, it will suffice to construct a term

% \[
%   w : \String \vdash \mathsf{parse}_{D} : \LinSigTy q Q {\mathsf{Trace}_{D}(q_{0} , q)}
% \]
% This is because given a trace of a DFA, we may easily check
% if we should accept or reject by simply testing
% if the final state is accepting.

% Because $w$ is a Kleene star of characters, we may construct
% our desired $\mathsf{parse}_{D}$ term as a $\mathsf{foldl}$
% over $w$. In the empty case, we just have the trace that
% ends at the accepting state. In the recursive case, we
% effectively add to our trace by transitioning one character
% at a time, as we read them moving across $w$.

% Perhaps this derivation is not too surprising. All it says
% is that a DFA may be ran by transitioning a single character
% at a time, and then accepting or rejecting based on the
% final state. This is exactly what DFAs did initially, so
% what did we gain? Well, this has the benefit of our type
% system to ensure its correctness. Moreover, this construction exports to an
% intrinsically verified and executable decision procedure for DFAs in \theoryabbv
% embedded in Agda.

% We should note that the
% construction of a DFA parser requires the addition of an additional but seemingly innocuous
% axiom.

% \[
%   \top \cong \left( \bigoplus_{c : \Sigma} c \right)^{*}
% \]

% That is, the characters in the alphabet $\Sigma$ really are all of the
% characters. The need for this axiom arises when we go to take a transition
% within the DFA,
% as we must ensure that the next character of the string has a corresponding
% transition from the current state
% --- which would be guaranteed by determinism provided there are no surprise characters.

% \subsection{Regular Expressions and DFAs}
% \label{subsec:deriv}

% In order to extend the DFA
% parser from \cref{subsec:regexparser} to the construction of
% a verified parser
% generator for regular expression we need to perform some
% plumbing establishing an equivalence between regular
% expressions and DFAs.

% There are several routes we may hope to take in establishing
% this equivalence. First, we could prove an equivalence
% between NFAs and regular expressions, and separately prove
% an equivalence between NFAs and DFAs.
% In \cref{subsec:eqproofs}, we include a version of
% Thompson's construction --- which established the
% equivalence between regular grammars and DFAs. We may additionally
% hope to internalize a variant of the powerset construction \cite{rabinFiniteAutomataTheir1959}
% --- which takes as input an NFA and constructs a DFA that
% recognizes the same language --- and combine the results of
% Thompson's construction and the powerset constructions to give an equivalence
% between regular expressions and DFAs.  This route is alluring, as it
% internalizes several classic grammar-theoretic constructions. However, it may
% necessitate extensions to the LNL theory, like
% a propositional truncation, and we have not yet investigated
% how this would interact with the existing types in the
% theory. The addition of a propositional truncation may seem
% harmless, but it is not always immediately clear how
% distinct constructions will interact. For instance, when
% exploring LNL models, Benton discovered that the synthesis
% of linear and dependent types require a new presentation of
% the $!$ modality from linear logic
% \cite{bentonMixedLinearNonlinear1995}. That is all to say,
% this is a work in progress and
% it is not immediate that the addition of a propositional
% truncation is adequate for establishing the weak equivalence
% between NFAs and DFAs.

% We may instead hope to internalize an equivalence between
% regular grammars and DFAs by using Brzozowski derivatives to
% directly create a DFA that is weakly equivalent to a given
% regular expression, as described by Owens et al.
% \cite{owensRegularexpressionDerivativesReexamined2009}.
% One characterization of regular grammars is that they are
% precisely those grammars which have finitely many inequivalent Brzozowski
% derivatives
% \cite{brzozowskiDerivativesRegularExpressions1964}.
% The algorithm used by Owens takes in a
% regular grammar and generates a DFA that recognizes the same
% language, and the states in this DFA are the finitely many
% derivative equivalence classes. We initially had a version of
% this theorem very roughly internalized in the LNL theory.
% To our taste, too much of this presentation relied on
% meta-arguments that lived outside of
% our formalism, and thus this particular phrasing of the
% theorem did not translate well into formalization.

% In any case, we believe
% that revisiting these lines of thought will lead to a
% satisfactory internalization of the equivalence between
% regular grammars and DFAs, and thus would bridge the gap
% between our DFA parser and a full regular expression parser.

% \subsection{Equivalence Between Regular Expressions and Finite Automata}
% \label{subsec:eqproofs}
% In this section, we describe a version of Thompson's
% construction \cite{thompsonProgrammingTechniquesRegular1968}
%    where we construct an NFA that recognizes a given regular
% expression. Moreover, we will show that this NFA is strongly equivalent to the
% original grammar. Witnessing this construction in our syntax has two benefits
% \begin{enumerate}
%   \item It reinforces this high-level view that the syntax is a natural and
%         general setting for formal grammar reasoning, as we demonstrate that
%         this formal system subsume results from existing systems, and
%   \item Following the
%         development of Thompson's construction, we then need
%         only establish the equivalence of NFAs and DFAs to
%         complete the full regular expression parser
% \end{enumerate}

% \begin{theorem}[Thompson]
%   \label{thm:thompson}
%   For $g$ a regular grammar $g$, there is an NFA $N$ that recognizes the same
%   language as $g$.
% \end{theorem}
% We make a pretty straightforward adaptation of Thompson's theorem to our setting,

% \begin{theorem}[Typed Thompson]
%   \label{thm:typthompson}
%   For $g$ a regular expression $g$, there is an NFA $N$ such that $g$ is isomorphic
%   to $\mathsf{AccTrace}_{N}$.
% \end{theorem}

% \begin{proof}[Proof Sketch]
%   Recall that regular expressions are inductively defined via
%   disjunction, concatenation, and Kleene star over literals
%   and the empty grammar. By induction over regular expressions,
%   we will construct an NFA that is equivalent to $g$.

%   First, define the recognizing NFA for the empty grammar
%   $I$.

%   \begin{figure}[h!]
%   \begin{tikzpicture}[node distance = 25mm ]
%     \node[state, initial] (1) {$1$};
%     \node[state, right of=1, accepting] (2) {$2$};

%     \path[->] (1) edge[below] node{$\varepsilon$} (2);
%   \end{tikzpicture}
%   \caption{$NFA(I)$}
%   \label{fig:emptyNFA}
%   \end{figure}

%   The type of traces from the initial state of $NFA(I)$ to the single
%   accepting state is given by,

%   \[
%     \mathsf{Trace}_{NFA(I)}(q_{1}, q_{2}) = \mu
%       \begin{pmatrix}
%          g_{q_{1}} := g_{q_{2}} \\
%          g_{q_{2}} := I
%       \end{pmatrix}. g_{q_{1}}
%   \]

%   The accepting traces through $NFA(I)$ are then described
%   as,

%   \[
%     \mathsf{AccTrace}_{NFA(I)} = \LinSigTy q {\{1 , 2\}} {\left( \mathsf{Trace}_{NFA(I)}(q_{1} , q) \pair \mathsf{acc}(q) \right)}
%   \]

%   A quick inspection of \cref{fig:emptyNFA} reveals that the
%   only reasonable choice for $q$ is $q_{2}$ --- because
%   state 2 is accepting while state 1 is not. Therefore,

%   \begin{align*}
%     \mathsf{AccTrace}_{NFA(I)}
%     & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \pair \mathsf{acc}(q_{2}) \\
%     & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \pair \top \\
%     & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})
%   \end{align*}

%   From here, to prove
%   $I \cong \mathsf{AccTrace}_{NFA(I)}$ it suffices to show
%   $I \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})$. Below we
%   give two parse transformers, one from $I$ to
%   $\mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})$ and vice versa.

%   \[
%     \inferrule
%     {p : I \vdash \mathsf{nil} : \mathsf{Trace}(q_{2} , q_{2}) \\
%      \exists \text{~transition~} q_{1} \overset{\varepsilon}{\to} q_{2}
%     }
%     {p : I \vdash \mathsf{\varepsilon cons}(\mathsf{nil}) : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})}
%   \]

%   Let $\gamma$ be the substitution $\{ g_{q_{2}} / g_{q_{1}}, I / g_{q_{2}} \}$,

%   \[
%     \inferrule
%     {
%       p : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \vdash p :
%         \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \\
%       x_{1} : \simulsubst {g_{q_{2}}} {\gamma} = I \vdash x_{1} : I \\
%       x_{2} : \simulsubst {I} {\gamma} = I \vdash x_{2} : I
%     }
%     {
%       p : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \vdash \mathsf{mfold}(x_{1}.x_{1} , x_{2}.x_{2})(p) : I
%     }
%   \]

%   This concludes the proof for the case of the empty
%   grammar. Let's now walk through the construction for
%   literal grammars. Given a character $c$, we construct an
%   NFA that recognizes only the string containing the single
%   character $c$ as,


%   \begin{figure}[h!]
%   \begin{tikzpicture}[node distance = 25mm ]
%     \node[state, initial] (1) {$1$};
%     \node[state, right of=1, accepting] (2) {$2$};

%     \path[->] (1) edge[below] node{$c$} (2);
%   \end{tikzpicture}
%   \caption{$NFA(c)$}
%   \label{fig:literalNFA}
%   \end{figure}

%   The automaton in \cref{fig:literalNFA} induces the
%   following type of traces from $q_{1}$ to $q_{2}$.

%   \[
%     \mathsf{Trace}_{NFA(c)}(q_{1}, q_{2}) = \mu
%       \begin{pmatrix}
%          g_{q_{1}} := c \otimes g_{q_{2}} \\
%          g_{q_{2}} := I
%       \end{pmatrix}. g_{q_{1}}
%   \]

%   Through the same argument as the empty grammar, the only
%   state that is accepting is $q_{2}$ and thus,

%   \[
%     \mathsf{AccTrace}_{NFA(c)} \cong \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2})
%   \]

%   To show the desired isomorphism of $c \cong NFA(c)$ we
%   make a similar argument as we did for the empty grammar
%   $I$, except we leverage the $\mathsf{cons}$ rule instead
%   of $\mathsf{\varepsilon cons}$. That is, the parse
%   transformers in either direction are given as,

%   \[
%     \inferrule
%     {\cdot \vdash \mathsf{nil} : \mathsf{Trace}(q_{2} , q_{2}) \\
%      \exists \text{~transition~} q_{1} \overset{c}{\to} q_{2}
%     }
%     {p : c \vdash \mathsf{cons}(\mathsf{nil}) :
%       \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2})}
%   \]


%   \[
%     \inferrule
%     {
%       p : \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \vdash p :
%         \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \\
%       x_{1} : \simulsubst {(c \otimes g_{q_{2}})} {\gamma} = c \otimes I \vdash \mathsf{unitR}(x_{1}) : c \\
%       x_{2} : \simulsubst {I} {\gamma} = I \vdash x_{2} : I
%     }
%     {
%       p : \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \vdash \mathsf{mfold}(x_{1}.\mathsf{unitR}(x_{1}) , x_{2}.x_{2})(p) : c
%     }
%   \]

%   Where $\gamma$ is the substitution
%   $\{ c \otimes g_{q_{2}} / g_{q_{1}}, I / g_{q_{2}} \}$ and
%   $\mathsf{unitR}$ is a witness to the isomorphism
%   $c \otimes I \cong c$. Again, we may see that these do
%   indeed mutually invert each other in the Agda code.

%   It remains to show that the desired isomorphisms are
%   preserved by $\otimes$, $\oplus$, and Kleene star. Here,
%   we will give the argument for the disjunction case, the
%   others are defined quite similarly.

%   Given two NFAs $N$ and $M$, we may define a new NFA that
%   encodes the disjunction of $N$ and $M$. Denote the
%   internal states of $N$ by $q_{j}$'s and the internal states
%   of $M$ by $r_{k}$'s,

%   \begin{figure}[h!]
%   \begin{tikzpicture}[node distance = 20mm ]
%     \node[state] (2) {$q_{init}$};
%     \node[state, initial, below left of=2] (1) {$init$};
%     \node[state, below right of=1] (3) {$r_{init}$};
%     \node[right of=2] (4) {$\cdots$};
%     \node[right of=3] (5) {$\cdots$};
%     % \node[state, below right of=1] (3) {$3$};
%     % \node[state, below of=2] (4) {$4$};

%     \path[->] (1) edge[below] node{$\varepsilon$} (2);
%     \path[->] (1) edge[below] node{$\varepsilon$} (3);
%     \path[->] (2) edge[below] node{} (4);
%     \path[->] (3) edge[below] node{} (5);

%     \node[label={[name=l] $N$}, draw,line width=2pt,rounded corners=5pt, fit=(2)(4)] {};
%     \node[label={[name=l] $M$}, draw,line width=2pt,rounded corners=5pt, fit=(3)(5)] {};
%   \end{tikzpicture}
%   \caption{$N \oplus_{NFA} M$}
%   \label{fig:disjunctionNFA}
%   \end{figure}

%   \Cref{fig:disjunctionNFA} shows the process of
%   disjunctively combining NFAs. Precisely, we add a single
%   new state and we included copies of the states from each of $N$
%   and $M$. The new state acts as the initial state and has
%   $\varepsilon$-transitions to the initial states of $N$ and
%   $M$. We include all of the internal transitions from $N$
%   and $M$, and the accepting states of $N \oplus_{NFA} M$
%   are exactly the accepting states in each subautomaton.

%   Let $g$ and $g'$ be two regular grammars such that
%   $g \cong NFA(g)$ and $g' \cong NFA(g')$. As a matter of
%   notation\footnote{We shall similarly abuse notation for
%     $\otimes$ and Kleene. That is, for a regular grammar
%     $g$, when we write $NFA(g)$ we mean the NFA inductively
%     built up with the NFA-analogues to the constructors that
%     built up $g$.}, we will write $NFA(g \oplus g')$ for
%   $NFA(g) \oplus_{NFA} NFA(g')$. The traces of
%   $NFA(g \oplus g')$ are then given by,

%   \[
%     \mathsf{Trace}_{NFA(g \oplus g')}(src , dst) = \mu
%       \begin{pmatrix}
%         g_{init} := g_{q_{0}} \oplus g_{r_{0}} \\
%         g_{q_{j}} := \mathsf{Trans}_{NFA(g)}(q_{j}) \oplus \mathsf{isDst}(q_{j}) \\
%         g_{r_{k}} := \mathsf{Trans}_{NFA(g')}(r_{k}) \oplus \mathsf{isDst}(r_{k})
%       \end{pmatrix}.g_{src}
%     \]

%   where $\mathsf{Trans}$ is used to echo the same syntactic
%   definitions that appear in the $NFA(g)$ and $NFA(g')$.
%   Also, $src$ and $dst$ may take on any value in
%   $Q := \{init\} \cup \{q_{j}\} \cup \{r_{k}\}$, and
%   $\mathsf{isDst}(q)$ checks if $q$ is equal to $dst$. Which
%   is all to say, the traces of $NFA(g \oplus g')$ comprise
%   either a trace of $NFA(g)$, or a trace of $NFA(g')$, and
%   the transition coming out of $g_{init}$ determines which
%   subautomaton we step into.

%   The parse transformer from $g \oplus g'$ checks which side
%   of the sum type we are on, then takes the appropriate step
%   from $g_{init}$ in the automaton.

%   \[
%     \inferrule
%     {
%       u : g \vdash \iota (\phi (u)) : \mathsf{AccTrace}_{NFA(g \oplus g')} \\
%       v : g' \vdash \iota' (\psi (v)) : \mathsf{AccTrace}_{NFA(g \oplus g')}
%     }
%     {p : g \oplus g' \vdash \mathsf{case}~p \{ \mathsf{inl}(u) \mapsto s , \mathsf{inr}(v) \mapsto r \} : \mathsf{AccTrace}_{NFA(g \oplus g')}}
%   \]

%   with $\iota$ and $\iota'$ as embeddings from $NFA(g)$ and
%   $NFA(g')$, respectively, into $NFA(g \oplus g')$,
%   $\phi: g \cong \mathsf{AccTrace}_{NFA(g)}$, and $\psi: g' \cong \mathsf{AccTrace}_{NFA(g')}$. On a
%   high level, all this construction does is turn a parse of
%   $g$ into a parse of $NFA(g)$ and then embeds that inside
%   of the larger automaton $NFA(g \oplus g')$. Likewise for $g'$.

%   In the other direction, recall that the data of an
%   accepting trace for $NFA(g \oplus g')$ is a pair of a
%   trace and a proof that
%   the end state $q'$ of that trace is accepting. By
%   multifolding over the first part of that pair, we turn the
%   term of type
%   $\mathsf{Trace}_{NFA(g \oplus g')}(init , q')$ into a
%   trace of either of the subautomata,

%   \[
%     p : \mathsf{Trace}_{NFA(g \oplus g')}(init , q') \vdash \mathsf{mfold}_{NFA(g \oplus g')} : \mathsf{Trace}_{NFA(g)}(q_{0} , q') \oplus \mathsf{Trace}_{NFA(g')}(r_{0} , q')
%   \]

%   Additionally, we leverage the fact that the only accepting
%   states for $NFA(g \oplus g')$ are those from the
%   subautomata to extract that $q'$ must be an accepting
%   state from a subautomaton.

%   \[
%     x : \mathsf{acc}_{NFA(g \oplus g')}(q') \vdash M : \mathsf{acc}_{NFA(g)}(q') \oplus \mathsf{acc}_{NFA(g')}(q')
%   \]

%   We then combine the trace and proof of acceptance into an
%   accepting trace of one of the subautomata,

%   \[
%     p : \mathsf{AccTrace}_{NFA(g \oplus g')} \vdash N : \mathsf{AccTrace}_{NFA(g)} \oplus \mathsf{AccTrace}_{NFA(g')}
%   \]

%   Lastly, we then inductively use the isomorphisms $\phi$
%   and $\psi$ to turn the accepting traces into a parse of
%   $g$ or $g'$,

%   \[
%      N : \mathsf{AccTrace}_{NFA(g)} \oplus \mathsf{AccTrace}_{NFA(g')} \vdash \mathsf{case}~N~\{\mathsf{inl}(n) \mapsto \phi^{-1}(n), \mathsf{inr}(n') \mapsto \psi^{-1}(n')\} : g \oplus g'
%   \]

%   When you wish to show that these actually maps invert each other to form an isomorphism\max{jumbled sentence}, we invoke the
%   universal properties of the $\mathsf{Trace}$ types. That is,
%   $\mathsf{Trace}_{\oplus NFA}$, $\mathsf{Trace}_{N}$, and $\mathsf{Trace}_{M}$
%   are \emph{initial} algebras to the the functors that define them. Thus, the
%   embedding of traces through $N$ into $\oplus NFA$ and then mapped back down to $N$
%   compose to a map from $\mathsf{Trace}_{N}$ to itself. Such a map is unique and
%   thus must be the identity. The same reasoning shows that the desired
%   composite do indeed invert each other to form an isomorphism.

%   As discussed above, the other cases and
%   follow through a similar argument. This
%   concludes the proof of our variant of Thompson's construction.
% \end{proof}

% \subsection{Other Automata}
% \subsubsection{Pushdown Automata}
% \label{subsubsec:pda}
% A (nondeterministic) \emph{pushdown automaton} is an automaton that employs a
% stack. Just like NFAs, they have transitions labeled with characters from a
% fixed string alphabet $\Sigma$. Additionally, they maintain a stack of
% characters drawn from a stack alphabet $\Gamma$. They are often represented
% formally as a 7-tuple $(Q, \Sigma, \Gamma, \delta, q_{0}, Z, F)$,

% \begin{itemize}
%   \item $Q$ a finite set of states
%   \item $\Sigma$ a fixed, finite string alphabet
%   \item $S$ a fixed, finite stack alphabet
%   \item
%         $\delta \subset Q \times (\Sigma \cup \{ \varepsilon \}) \times S \to \mathcal{P}(Q \times S^{*})$
%         the transition function
%   \item $q_{0} \in Q$ the start state
%   \item $Z \in S$ the initial stack symbol
%   \item $F \subset Q$ the accepting states
% \end{itemize}

% We encode the traces of a pushdown automaton very similarly to those of an NFA,
% except the transitions of a PDA are instead encoded via the LNL product type
% $\bigamp$. This is because when simply transitioning via character, a PDA must
% also pop and push characters onto a stack, which is used as the argument to
% these dependent functions. Thus, the appropriate formation of traces through a
% PDA is dependent on a stack.

% Let $S$ be a non-linear type encoding the stack
% alphabet, and build lists over $S$ as the (non-linear) least fixed-point
% $\mathsf{List}(S) := \mu X . 1 + S \times X$. Then, the type of states for a
% PDA $P$ with stack alphabet $S$ are given as a functions that takes in lists $L$,
% and then makes a case distinction between possible transitions based on what was witnessed as $\mathsf{head}(L)$. The choice of transition
% will then determine which character to transition by and what word $w$ should be
% pushed onto the stack. The word that is added to the top of the stack is
% appended to $\mathsf{tail}(L)$ and then we recursively step into another state
% called on argument $w \cdot \mathsf{tail}(L)$.

% \begin{gather*}
%   \mathsf{State} \in {g _{q} : q \in Q} \\
%   \mathsf{Word} \in \String \\
%   \mathsf{Char} \in \Sigma   \\
%   \mathsf{StackChar} \in \Gamma \\
% \end{gather*}
% \begin{align*}
%   \mathsf{Trans}_{P}(q) ::=~&
%                            \LinPiTy {(hd :: tl)} {\mathsf{List}(S)} {\left( \mathsf{Char} \otimes \mathsf{State}(\mathsf{Word} + tl) \right)}~| \\
%   & \LinPiTy {(hd :: tl)} {\mathsf{List}(S)} {\left(  \mathsf{State}(\mathsf{Word} + tl) \right)}~|~\mathsf{Trans}_{P}(q) \oplus \mathsf{Trans}_{P}(q)
% \end{align*}

% That is to say, when transitioning a PDA pops off the head $hd$ of the stack

% \subsubsection{Turing Machines}
% \label{subsubsec:tm}

% Above we gave a grammar presentation of
% traces through a PDA by using a non-linear type $S$
% to encode the stack. We may similarly use pairs $S \times S$
% to encode the tape of a Turing machine. With two stacks we can simulate the behavior of the
% infinite tape of a Turing machine. The intuition behind this correspondence is
% that the left half of the tape is on one stack, the right
% half of the tape the other, and we treat the tops of stacks
% like the head of the tape.

% \pedro{I like the overall structure of this section! Showing how the type theory
% can encode and reason about different kinds of languages is very instructive!}

\section{Semantics and Metatheory}
\label{sec:semantics-and-metatheory}

\subsection{Formal Grammars as Linear Types}
\label{sec:formaltype}
\max{this should be in its own section on semantics}

% The linear typing judgment in our syntax takes on the following schematic form
% $\Gamma ; \Delta \vdash a : A$. First, $A$ represents a \emph{linear type} in
% our syntax. The intended semantics of these linear types are formal grammars.
% That is, the linear typing system is designed to syntactically reflect the
% behavior of formal grammars. For this reason, we may often interchangeably use
% the terms ``linear type'' and ``grammar''.

% The term $a$ is an inhabitant of type $A$, which is thought of as a parse tree of
% the grammar $A$. The core idea of this entire paper follows precisely from this
% single correspondence: grammars are types, and the inhabitants of these types
% are parse trees for the grammars.

% $\Gamma$ represents a non-linear context, while $\Delta$ represents a
% \emph{linear context} dependent on $\Gamma$. These linear contexts behave
% substructurally. As stated earlier, they are linear --- they do not obey
% weakening or contraction --- because a character is exhausted once read by a
% parsing procedure. Moreover, the characters in strings appear in the order in
% which we read them. We do not have the freedom to freely permute characters,
% therefore any type theory that is used to reason about formal grammars ought to
% omit the structural rule of exchange as well. This means that every variable
% within $\Delta$ must be used \emph{exactly once} and \emph{in order of
%   occurrence}. Thus, we can think of the linear contexts as an ordered list of
% limited resources. Once a resource is consumed, we cannot make reference to it
% again. Variables in a linear context then act like building
% blocks for constructing patterns over strings.

% We give the base types and type constructors for linear terms. As the
% interpretation of types as grammars in $\Grammar$ serves as our intended
% semantics, we simultaneously give the interpretation $\sem \cdot$ of the
% semantics as grammars.

% \pedro{The paragraphs above are a bit circular, what about the following alternative?}
% \steven{I am in favor of cutting what's above, but we haven't introduced the
%   syntax in the main body of the paper yet, even if the figures appeared
%   earlier. So what you write needs to take that into account}
% \pedro{In which case we should introduce it in the previous section. I suggest
%   adding this right after ``and the intuitionistic typing rules in fig 3''}
Now that we have specified the syntactic aspect of \theoryabbv, we must justify
its connections to formal grammars in order to show that it is a sound methodology
for reasoning about parsers. We do this by showing that $\mathbf{Gr}$ is a model
to our type theory.

Most of the non-linear semantics is standard \pedro{We should either cite something or add the non-linear semantics in the appendix}, so let us investigate in more
depth the linear semantics.

\begin{enumerate}
  \item A non-linear context $\Gamma$ denotes a set $\sem \Gamma$
  \item A non-linear type $\Gamma \vdash X : U_{i}$ denotes a family of sets
        $\sem X : \sem \Gamma \to \Set_{i}$
  \item A non-linear term $\Gamma \vdash e : X$ denotes a section
        $\sem e : \Pi(\gamma : \sem \Gamma)\sem{X} \gamma$ \pedro{we have not defined what sections are}
  \item Linear contexts $\Gamma \vdash \Delta$ and types
        $\Gamma \vdash A : L_{i}$ both denote families of grammars
        $\sem \Gamma \to \Gr_{i}$
  \item A linear term $\Gamma ; \Delta \vdash M : A$ denotes a family of parse
        transformers
        $\sem M : \Pi(\gamma : \sem \Gamma)\Pi(w : \String) \sem \Delta \gamma w \Rightarrow \sem M \gamma w$
\end{enumerate}

\paragraph{Linear Unit}
First, the linear unit $I$ may be built in the empty linear
context\footnote{When appropriate, statements like ``the empty linear context''
  may be taken polymorphically over all non-linear contexts that they might
  depend on.} $\cdot \dashv I$. $I$ serves as the unit for the operation
$\otimes$ described below.

As a grammar,
%
\[
  \sem {I} \gamma w = \{ () ~|~ w = \varepsilon \}
\]
%
That is, $I$ maps strings to the proposition that they are the empty string. $I$
is clearly only inhabited by the empty string $\varepsilon$, for which the
outputted set contains a single parse tree.


\paragraph{Base Types}
For each character $c$ in the alphabet $\Sigma$, we include a base type at the
lowest universe level $c : L_{0}$.

The grammar interpretation for characters is quite similar to that of $I$.
%
\[
  \sem {c} \gamma w = \{ () ~|~ w = c \}
\]
%
The grammar for the character $c$ likewise maps strings $w$ to the proposition that
they are equal to $c$.

\paragraph{Tensor Product}
The tensor product of two linear types is the first place where the ordering on
contexts really takes effect. That is, the tensor product of two types is
non-commutative.
When context $\Delta$ forms a linear term of
type $A$, and $\Delta'$ forms a linear term of type $B$, then the context
extension $\Delta, \Delta'$ forms a linear term of type $A \otimes B$.

As stated above, the type $I$ serves as the unit for $\otimes$. That is, for all
linear types $A$, the following equalities hold:
%
\[ I \otimes A \equiv A \otimes I \equiv A \]
%
In the grammar semantics,
%
\[
  \sem {A \otimes B} = \Sigma_{w_{1}, w_{2} : \String} (w_{1}w_{2} = w) \land \left(  \sem {A} \gamma w_{1} \times \sem {B} \gamma w_{2} \right)
\]
%
The string $w$ matches $A \otimes B$
precisely when it may be split into two
pieces such that the left one matches $A$
and the right one matches $B$.

\paragraph{Linear Function Types}
The monoidal structure provided by $\otimes$ is both left and right closed and
this is denoted by the left and right linear function types. Further,
for linear types $A$ and $B$ the left linear function type $A \lto B$
enjoys an elimination principle similar to function application or modus ponens.
%
\[
\inferrule{\Delta \vdash A \otimes A \lto B}{\Delta \vdash B}
\]
%
The interpretation of linear function types at a string $w$ is a linear function
that takes in parses of $A$ on a string $w_{a}$ and outputs parses of $B$ on the
string $w_{a} w$.
%
\[
  \sem{A \lto B} \gamma w = \Pi(w_a:\String) \left( A \gamma w_a \Rightarrow B\gamma (w_a w) \right)
\]
%
That is, strings match $A \lto B$ if when prepended with a parse of $A$ they
complete to parses of $B$. In this manner, the linear function types generalize
Brzozowksi's notion of derivative
\cite{brzozowskiDerivativesRegularExpressions1964}.
Brzozowski initially only gave an accounting of this operation for
generalized regular expressions, but later work from Might et al.\ demonstrates that the same
construction can be generalized to context free grammars
\cite{mightParsingDerivativesFunctional2011}. Here, via the linear function
types, the same notion of derivative extends to the grammars of \theoryabbv.

Note, to ensure that the linear function types do indeed generalize Brzozowski
derivatives, we must include the equalities in \cref{fig:brzozowskideriv} as axioms\max{we must? why must we}.
\pedro{We should further explain why these equations are required, or at the very
least give some intuition}

Of course, all the above statements for the left function type also have
corresponding analogues for the
right-handed counterpart.

\begin{figure}
\begin{align*}
  c\lto c &\equiv I\\
  c\lto d &\equiv \bot\\
  c\lto I &\equiv \bot\\
  c\lto 0 &\equiv \bot\\
  c\lto (A \otimes B) & \equiv (c\lto A) \otimes B + (A \amp I) \otimes (c\lto B)\\
  c\lto A^* &\equiv (A \amp I)^{*} \otimes (c \lto A) \otimes A^* \\
  c\lto (A + B) &\equiv (c\lto A) + (c\lto B)
\end{align*}
\caption{Equality for Brzozowski Derivatives}
\label{fig:brzozowskideriv}
\end{figure}

\paragraph{LNL Dependent Types}
\steven{talk about how binary sum and binary with are definable over $\Bool$}
Given a non-linear type $X$, we may form both the dependent product type
$\LinPiTy x X A$ and the dependent pair type $\LinSigTy x X A$ as linear types
where $x$ is free in $A$.

$\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

$\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

The grammar semantics of the linear product type is
indeed a dependent function out of the
semantics of $X$. Likewise, the grammar semantics of the linear dependent pair type is a dependent
pair in $\Set$.\pedro{The connection to grammar semantics needs to be better explained. For instance,
we should explain that disjunction corresponds to ``or'' of grammars}

Note that even though we do not take the binary additive conjunction and
disjunction as primitive, we may define them via these LNL dependent types. In
particular, via a dependence on $\Bool$.
%
\[
  A \amp B := \LinPiTy b \Bool {C(b)}
\]
\[
  A \oplus B := \LinSigTy b \Bool {C(b)},
\]
where $C(\true) = A, C(\false) = B$.

\paragraph{Universal Type}\max{this is absolutely not the right terminology. Universal type means multiple other completely different things}
\steven{Definable as nullary product}
The universal type $\top$ may be formed in any context.
%
\[ \sem{\top} \gamma w = \{ \ast \}\]
%
Its grammar semantics in set outputs the unit type in $\Set$ for all input strings in all contexts.

\paragraph{Empty Type}
\steven{Definable as nullary sum}
The empty type $\bot$ has no inhabitants. The elimination for the empty type
witnesses the principle of explosion --- i.e.\ from a term of type $\bot$ we may
introduce a term $\mathsf{absurd}_{A} : A$ for any type $A$.
%
\[ \sem {\bot} \gamma w = \emptyset \]
%
\paragraph{The $G$ Modality}
\steven{Rename this}
\steven{Compare to persistence modalities from separation logic}
The $G$ modality provides the embedding of linear types in the empty context
into non-linear types. The introduction and elimination forms for $G A$ obey the
same rules as given in \cite{bentonMixedLinearNonlinear1995} and \cite{krishnaswami_integrating_2015}.

The left adjoint $F$ from non-linear types back to linear types may be defined
as,
%
\[
  F X := \LinSigTy a A I
\]
%
\steven{Elaborate on $G$ more and rename $G A$to $\ltonl A$ in this section}

\pedro{Agreed :) maybe mention connections to persistent propositions from the
separation logic literature}

The semantics of $G A$ are exactly the semantics of $A$ at the empty word $\varepsilon$.
%
\[ \sem{G A} \gamma = \sem{A} \gamma \varepsilon \]
%
%
\paragraph{Recursive Types}
\steven{reflavor this section as ``indexed inductive'' instead of ``recursive'',
and entirely rework with the new inductive definition}
\steven{The non-indexed inductive are definable as being trivially indexed over unit}
Recursive linear types may be defined via the least-fixed point operator $\mu$.
% The grammar semantics of which are the fixed-point of sets induced by the
% grammar semantics of $A$.
% %
% \[ \sem{\mu x. A} \gamma = \mu (x:\Gr_i). \sem{A}(\gamma,x) \]
% %
% \pedro{We should justify, even if briefly, the existence of this
%   fixed-point}

% Observe that we need not take the Kleene star as a primitive
% grammar constructor, as it is definable as a fixed point.
% The Kleene star of a grammar $g$ is given as,

% \[
%   g^{*} := \mu X . I \oplus (g \otimes X)
% \]

% \begin{figure}[h!]
% \begin{mathpar}
%   \inferrule
%   {\Gamma ; \Delta \vdash p : I}
%   {\Gamma ; \Delta \vdash \mathsf{nil} : g^{*}}

%   \and

%   \inferrule
%   {\Gamma ; \Delta \vdash p : g \\ \Gamma ; \Delta' \vdash q
%   : g^{*}}
%   {\Gamma ; \Delta \vdash \mathsf{cons}(p , q) : g^{*}}

%   \\

%   \inferrule
%   {
%     \Gamma ; \Delta \vdash p : g^{*} \\
%     \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
%     \Gamma ; x : g , y : h \vdash p_{\ast} : h
%   }
%   {\Gamma ; \Delta \vdash \mathsf{foldr}(p_{\varepsilon} , p_{\ast}) : g^{*}}
% \end{mathpar}
% \caption{Kleene Star Rules}
% \label{fig:star}
% \end{figure}

% Likewise, $g^{*}$ has admissible introduction and
% elimination rules, shown in \cref{fig:star}. Note that this
% definition of $g^{*}$ and these
% rules arbitrarily assigns a handedness to the Kleene star.
% We could have just as well took it to be a fixed point of
% $I \oplus (X \otimes g)$. In fact, the definitions are
% equivalent, as the existence of the $\mathsf{foldl}$ term below
% shows that $g^{*}$ is also a fixed point of
% $I \oplus (X \otimes g)$.
% This is a marked point of difference from Kleene
% algebra with recursion, where the fixed points for the left and right variants
% of Kleene star need not agree \cite{leiss}.

% \begin{equation}
%   \label{eq:foldl}
%   \inferrule
%   {
%     \Gamma ; \Delta \vdash p : g^{*} \\
%     \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
%     \Gamma ; y : h , x : h \vdash p_{\ast} : h
%   }
%   {\Gamma ; \Delta \vdash \mathsf{foldl}(p_{\varepsilon} , p_{\ast}) : g^{*}}
% \end{equation}

% In fact, the $\mathsf{foldl}$ term is defined using
% $\mathsf{foldr}$ --- much in the same way one
% may define a left fold over lists in terms of a right fold
% in a functional programming language.
% The underlying trick is to fold over a list of linear functions
% instead of the original string. We curry each character $c$
% of the string into a function that concatenates $c$, and
% right fold over this list of linear functions. Because function
% application is left-associative, this results in a left
% fold over the original string.

% We only take fixed points of a single variable as a
% primitive operation in the type theory, but we may apply
% Beki\`c's theorem \cite{Bekić1984} to define an admissible
% notion of multivariate fixed point. This is particularly
% useful for defining grammars that encode the states of an
% automaton, as we will see in \cref{sec:automata}. In \cref{fig:multifix} we provide the
% introduction and elimination principles for such a fixed
% point, where $\sigma$ is the substitution that unrolls the
% mutually recursive definitions one level. That is,

% \begin{align*}
%   \sigma = \{ & \mu(X_{1} = A_{1} \dots, X_{n} = A_{n}).X_{1} / X_{1} , \dots, \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{n} / X_{n} \}
% \end{align*}

% \begin{figure}
% \begin{mathpar}
%   \inferrule
%   {\Gamma ; \Delta \vdash e : \simulsubst {A_{k}} {\sigma}}
%   {\Gamma ; \Delta \vdash \mathsf{cons}~e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k}}

%   \and

%   \inferrule
%   {\Gamma ; \Delta \vdash e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k} \\
%              \Gamma ; x_{j} : \simulsubst {A_{j}}{\sigma} \vdash e_{j} : B_{j}\quad \forall j
%   }
%   {\Gamma; \Delta \vdash \mathsf{mfold}(x_{1}.e_{1}, \dots, x_{n}.e_{n})(e) : B_{k}}
% \end{mathpar}
% \caption{Multi-fixed Points}
% \label{fig:multifix}
% \end{figure}

%% \section{Categorical Semantics of \theoryabbv}
%% \label{sec:categorify}

As described in previous sections, \theoryabbv has a ``standard''
interpretation where non-linear types denote sets, linear types denote
formal grammars, non-linear terms denote functions and linear terms
denote parse transformers.
%
Further, as is typical for type theory, every type constructor in the
calculus is interpreted by using a \emph{universal construction} in
either the category of sets or the category of grammars\footnote{the
only exception is that the axioms for Brzozowski derivatives we
considered do not follow solely from universal properties. In this
section we focus on models that do not necessarily satisfy these
axioms.}.
%
This leads to an immediate opportunity for generalization: the type
theory has in addition to the standard grammar-theoretic
interpretation, an interpretation in any categorical structure that
exhibits the same universal constructions.
%
On the one hand this gives us a structured way to formalize the
standard semantics precisely, but additionally it enables us to
consider \emph{non-standard} models in the next section that point to
further applications as well as meta-theoretic results.

In developing this categorical semantics, we will start with the
closest analogue from formal language theory: Kleene algebra.  Kleene
algebras are an important tool in the theory of regular languages
serving as a bridge between algebraic reasoning and equivalence of
regular expressions. More broadly, through various extensions, they
serve as a theoretical substrate to studying different kinds of formal
languages.
%
We can then see that the ``regular fragment'' of our type theory
(i.e., just characters, $\otimes$, $\oplus$, $0,1$ and a Kleene star)
has models in what we call a \emph{Kleene category} a categorification
of Kleene algebra from posets to categories.
%
We then further develop this into our final notion of model, which we
call a \emph{Chomsky category} as it can model not just regular
languages, but the full Chomsky hierarchy.

\subsection{Kleene Algebra and Kleene Categories}
A Kleene algebra is a tuple $(A, +, \cdot, (-)^*, 1, 0)$, where $A$ is
a set, $+$ and $\cdot$ are binary operations over $A$, $(-)^*$ is a
function over $A$, and $1$ and $0$ are constants. These structures
satisfy the axioms depicted in Figure~\ref{fig:axioms}.

\begin{figure}
  \begin{align*}
    x + (y + z) &= (x + y) + z & x + y &= y + x\\
    x + 0 &= x & x + x &= x\\
    x(yz) &= (xy)z & x1 &= 1x = x\\
    x(y + z) &= xy + xz & (x + y)z &= xz + yz\\
    x0 &= 0x = x & & \\
    1 + aa^* &\leq a^* & 1 + a^*a &\leq a^*\\
     b + ax \leq x &\implies a^*b \leq x &  b + xa \leq x &\implies ba^* \leq x
  \end{align*}
  \caption{Kleene algebra axioms}
  \label{fig:axioms}
\end{figure}

The addition operation can be used to define the partial order
structure $a \leq b$ if, and only if, $a + b = b$. In the theory of
formal languages, this order structure can be used to model language
containment. In this section, we categorify the concept of Kleene
algebra and build on top of it in order to define an abstract theory
of parsing. We start by defining \emph{Kleene categories}.

\begin{definition}
  A Kleene category is a distributive monoidal category $\cat{K}$
  such that for every objects $A$ and $B$, the endofunctors $F_{A, B}
  = B + A \otimes X$ and $G_{A, B} = B + X \otimes A$ have initial
  algebras (denoted $\mu X.\, F_{A, B}(X)$) such that $B \otimes (\mu
  X.\, F_{A, 1}) \cong \mu X.\, F_{A, B}(X)$ and the analogous isomorphism
  for $G_{A,B}$ also holds.
\end{definition}

As a sanity check, note that Kleene algebras are indeed examples of
Kleene categories.

\begin{example}
  Every Kleene algebra, seen a posetal category, is a Kleene category.
  The product $\cdot$ is a monoidal product and the addition is a
  least-upper bound, which corresponds to a coproduct. Lastly, the
  axioms of the Kleene star have a direct correspondence to the
  coherence conditions postulated by the initial algebras of Kleene
  categories.
\end{example}

This example provides a neat categorical justification to how
restrictive Kleene algebras are in terms of reasoning about
languages. By only having at most one morphism between objects, there
is not a lot of information they can convey. In this case, the only
information you get is language containment. As demonstrated by
$\mathbf{Gr}$, the extra degrees of freedom granted by having more
morphisms gives you more algebraic structure for reasoning about
languages.

For the next example, we see an unexpected connection with the theory
of substructural logics.

\begin{example}
  The opposite category of every Kleene category is a model of a variant of
  conjunctive ordered logic, where the Kleene star plays the role of the ``of
  course'' modality from substructural logics which allows hypotheses to
  be discarded or duplicated.
\end{example}

As we have seen, the proposed axioms are a direct translation of the
Kleene algebra axioms to a categorical setting. Its most unusual aspect is the
axiomatization of the Kleene star as a family of initial algebras
satisfying certain isomorphisms. If the Kleene category $\cat{K}$ has
more structure, then these isomorphisms hold ``for free''.

\begin{theorem}
  \label{th:kleeneclosed}
  Let $\cat{K}$ be a Kleene category such that it is also monoidal
  closed.  Then, the initial algebras isomorphisms hold automatically.
\end{theorem}
\begin{proof}
  We prove this by the unicity (up-to isomorphism) of initial
  algebras. Let $[hd, tl]: I + (\mu X.\, F_{A, I}(X)) \otimes A \to
  (\mu X.\, F_{A, I}(X))$ be the initial algebra structure of $(\mu
  X.\, F_{A, I}(X))$ and consider the map $[hd, tl] : B + B \otimes
  (\mu X.\, F_{A, I}(X)) \otimes A \to B\otimes (\mu X.\, F_{A,
    I}(X))$.

  Now, let $[f,g] : B + A \otimes Y \to Y$ be an $F_{A,B}$-algebra and
  we want to show that there is a unique algebra morphism $h : \mu X.\, F_{A,I} \to B \lto Y$. We can show existence and
  uniqueness by showing that the diagram on top commutes if, and
  only if, the diagram on the bottom commutes:

% https://q.uiver.app/#q=WzAsOCxbMCwwLCJCICsgQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFsyLDAsIkIgKyBZIFxcb3RpbWVzIEEiXSxbMiwyLCJZIl0sWzAsMiwiQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDMsIjEgKyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDUsIlxcbXUgWC5cXCwgMSArIFggXFxvdGltZXMgQSJdLFsyLDMsIjEgKyAoIEIgXFxsdG8gWSkgXFxvdGltZXMgQSJdLFsyLDUsIkIgXFxsdG8gWSJdLFswLDEsImlkICsgKGlkIFxcb3RpbWVzIGg7IGV2KSBcXG90aW1lcyBpZF9BIl0sWzEsMiwiW2YsZ10iXSxbMywyLCJpZCBcXG90aW1lcyBoOyBldiIsMl0sWzAsMywiW2lkIFxcb3RpbWVzIGgsIGlkIFxcb3RpbWVzIHRsXSIsMl0sWzQsNiwiaWQgKyAoaCBcXG90aW1lcyBYKVxcb3RpbWVzIGlkIl0sWzUsNywiaCIsMl0sWzYsNywiW2YnLCBnJ10iXSxbNCw1LCJbaGQsIHRsXSIsMl1d
\[\begin{tikzcd}
	{B + B \otimes (\mu X.\, I + X \otimes A)} && {B + Y \otimes A} \\
	\\
	{B \otimes (\mu X.\, I + X \otimes A)} && Y \\
	{I + (\mu X.\, I + X \otimes A)} && {I + ( B \lto Y) \otimes A} \\
	\\
	{\mu X.\, I + X \otimes A} && {B \lto Y}
	\arrow["{id + (id \otimes h; ev) \otimes id_A}", from=1-1, to=1-3]
	\arrow["{[f,g]}", from=1-3, to=3-3]
	\arrow["{id \otimes h; ev}"', from=3-1, to=3-3]
	\arrow["{[id \otimes h, id \otimes tl]}"', from=1-1, to=3-1]
	\arrow["{id + (h \otimes X)\otimes id}", from=4-1, to=4-3]
	\arrow["h"', from=6-1, to=6-3]
	\arrow["{[f', g']}", from=4-3, to=6-3]
	\arrow["{[hd, tl]}"', from=4-1, to=6-1]
\end{tikzcd}\]
  This equivalence follows by using the adjunction structure given
  by the monoidal closed structure of $\cat{K}$. A completely analogous
  argument for $G_{A,B}$ also holds. Furthermore, by generalizing the
  construction of \Cref{sec:formaltype}, we can also show that from
  the monoidal closed assumption it follows that $\mu X.\, F_{A, I}(X) \cong \mu X.\, G_{A, I}(X)$
\end{proof}

Something surprising about this lemma is that it provides an alternative
perspective on the observation that if a Kleene algebra has an
residuation operation, also called action algebra \cite{kozen1994action},
then the Kleene star admits a simpler axiomatization.

Since we want Kleene categories to generalize our notion of formal
grammars as presheaves $\String \to \Set$, we prove that they do
indeed form a Kleene category. We start by presenting a well-known
construction from presheaf categories.

\begin{definition}
  Let $\cat{C}$ be a locally small monoidal category and $F$, $G$ be
  two functors $\cat{C} \to \Set$. Their Day convolution tensor
  product is defined as the following coend formula:
  \[
  (F \otimes_{Day} G)(x) = \int^{(y,z) \in \cat{C}\times\cat{C}}\cat{C}(y\otimes z, x) \times F(y) \times G(z)
  \]
  Dually, its internal hom is given by the following end formula:
  \[
  (F \lto_{Day} G)(x) = \int_{y} \Set(F(y), G(x \otimes y))
  \]
\end{definition}

\begin{lemma}[Day \cite{day1970construction}]
  Under the assumptions above, the presheaf category $\Set^{\cat{C}}$ is
  monoidal closed.
\end{lemma}

%% \begin{theorem}
%%   Let $\cat{K}$ be a Kleene category and $A$ a discrete category.
%%   The functor category $[A, \cat{K}]$.
%%   (HOW GENERAL SHOULD THIS THEOREM BE? BY ASSUMING ENOUGH STRUCTURE,
%%   E.G. K = Set, THIS THEOREM BECOMES SIMPLE TO PROVE)
%% \end{theorem}
\begin{theorem}
  If $\cat{C}$ is a locally small monoidal category, then
  $\Set^{\cat{C}}$ is a Kleene category.
\end{theorem}
\begin{proof}

  By the lemma above, $\Set^{\cat{C}}$ is monoidal closed, and since it
  is a presheaf category, it has coproducts. Furthermore, the tensor
  is a left adjoint, i.e. it preserves colimits and, therefore, it is
  a distributive category.

  As for the Kleene star, since presheaf categories admit small colimits,
  the initial algebra of the functors $F_{A,B}$ and $G_{A,B}$ can be
  defined as the filtered colimit of the diagrams:

  From Theorem~\ref{th:kleeneclosed} it follows that these initial
  algebras satisfy the required isomorphisms and this concludes the
  proof.
\end{proof}

\begin{corollary}
  For every alphabet $\Sigma$, the presheaf category $\Set^{\cat{\Sigma^*}}$
  is a Kleene category.
\end{corollary}
\begin{proof}
  Note that string concatenation and the empty string make the
  discrete category $\Sigma^*$ a strict monoidal category.
\end{proof}

Much like in the posetal case, the abstract structure of a Kleene
category is expressive enough to synthetically reason about regular
languages. A significant difference between them is that while Kleene
algebras can reason about language containment, Kleene categories can
reason about \emph{ambiguity}, \emph{strong equivalence} of grammars.

\subsection{Lambek Hyperdoctrines and Chomsky Hyperdoctrines}

Though Kleene categories are expressive enough to reason about
concepts that are outside of reach of Kleene algebras, their
simply-typed nature makes them not so expressive from a type theoretic
point of view. This is limiting because type theories are successful
syntactic frameworks for manipulating complicated categorical
structures while avoiding some issues common in category theory, such
as coherence issues.

With this in mind, we want to design a categorical semantics that
builds on top of Kleene categories with the goal of extending them
with dependent types and making them capable reasoning about languages
and their parsers. This leads us to the abstract notion of model we
are interested in capturing with \theoryabbv: a \emph{Chomsky
category}.

We do this in two stages: first we define a \emph{Lambek hyperdoctrine} to be a
notion of model for the judgmental structure of \theoryabbv: that is we have
notions of linear and non-linear type, contexts, substitution and terms, but do
not assume any particular type constructions exist. Then we will define when a
Lambek hyperdoctrine is a Chomsky hyperdoctrine, meaning it can interpret all
the type formers of \theoryabbv and therefore arbitrary strength grammars.

\begin{definition}[Lambek Hyperdoctrine]
  A \emph{Lambek hyperdoctrine} consists of
  \begin{enumerate}
  \item A category $\mathcal C$ with a terminal object.
  \item A category with families structure over $\mathcal C$.
  \item A contravariant functor $L : \mathcal C^{o} \to \textrm{MultiCat}$ from
    $\mathcal C$ to the category of Multicategories.
  \end{enumerate}
\end{definition}
Here the objects of $\mathcal C$ model the non-linear contexts, and morphisms
model non-linear substitutions. The category with families structure over
$\mathcal C$ models the dependent non-linear types. Finally the
``hyperdoctrine'' of multicategories $L$ models the linear types and terms. The
fact that this is functorial in $\mathcal C$ corresponds to the fact that linear
types are all relative to a non-linear context and that linear variables and
composition commute with non-linear substitution.

We will have three main Lambek hyperdoctrines of interest in this paper: the
``standard model'' of sets and grammars, the ``syntactic model'' given by our
type theory itself and lastly a gluing model introduced in
Section~\ref{sec:canonicity} to prove the canonicity theorem.
\begin{example}
  \begin{enumerate}
  \item Let $\mathcal C$ be the category of sets, equipped with its usual
    category with families structures of families. Define $L : \mathcal C^{o}
    \to \textrm{MultiCat}$ to map $L(X)$ to the representable multicategory
    $(\Set^{\Sigma^*})^X$ of $X$-indexed families of grammars where the monoidal
    structure is given pointwise by the Day convolution monoidal structure.
  \item We can build a ``syntactic'' Lambek hyperdoctrine $\Syn(\Sigma)$ from the syntax
    itself: the category $\mathcal C$ is given by non-linear contexts and
    substitutions, the category with families by the non-linear types and terms
    and the hyperdoctrine of multicategories by the linear types and terms.
  \end{enumerate}
\end{example}

Next each type linear and non-linear type constructor corresponds to extra data
on a Lambek hyperdoctrine.
\begin{definition}
  \label{def:chomsky-data}
  \begin{enumerate}
  \item Dependent type structure: standard (say extensional type theory, give a reference)
  \item Universes
  \item Inductive grammars?
  \item Non-inductive Grammar constructors: standard monoidal category stuff
  \end{enumerate}
\end{definition}
\begin{definition}[Chomsky Hyperdoctrine]
  A Chomsky Hyperdoctrine is a Lambek hyperdoctrine equipped with all of the data in Definition~\ref{def:chomsky-data}.
\end{definition}
\pedro{We should probably define some of the words in this definition}
\begin{definition}
  A Chomsky category is a locally Cartesian category with two hierarchies of
  universes $\{L_i\}_{i\in \nat}$ and $\{U_i\}_{i\in \nat}$ such that
  every $L_i$ and $U_i$ are $U_{i+1}$-small. Furthermore, we require
  $U_i$ to be closed under dependent products and sum,
  $L_i$ to be closed under the Kleene category connectives,
  dependent products, left and right closed structures, with
  a type constructor $G : L_i \to U_i$ and a linear dependent sum
  going the other direction.
\end{definition}

\steven{Max suggests augmenting the definition of a Chomsky category to
  something like two categories $L$ and $U$, and $U$ is Cartesian closed I don't
  fully recall the rest.


  My best guess is that you take $L$ a Kleene category with a hierarchy of universes
  two, and you further require that $L$ is $Psh(U)$-enriched, except he
  suggested a further adjective on these presheaves. Perhaps representability?
}

\begin{theorem}
  The presheaf category $\Grammar = \Set^{\cat{\Sigma^*}}$ is a Chomsky category.
\end{theorem}


Further, the syntactic category of \theoryname is manifestly a Chomsky category.

\pedro{This is likely true, but if we explicitly say so, this warrants a proof. I think that
if we don't say anything about the syntactic category, reviewers won't mind.}

\steven{I agree that we don't want to say anything that opens unnecessary
  questions for proofs we haven't written. However, it seems hard to make the
  case that we have the right categorical model of the syntax if this statement
  isn't true. By restating our definition of Chomsky category, this should be
  obvious or a quick proof}

\pedro{I agree, then we should add the quick proof :) }

\subsection{Concrete Models of \theoryabbv}
\label{sec:othermodels}

\steven{Because we can define a version of semantic actions internally, we
  shouldn't put it as a separate model}

One of the powers of type theories is that it can be profitable to interpret them
in various models. In this section, by using our just-defined Chomsky categories,
we show how other useful concepts from formal language theory can also be organized
as models of \theoryabbv. We illustrate this point by providing two examples that
are closely related to the theory of formal languages: language equivalence and
semantic actions. Furthermore, in order to justify how $\mathbf{Gr}$ relates to
more traditional notions of parsing, we define a glued model that proves a
canonicity property of grammar terms.


\subsection{Language Semantics}
Every grammar induces a language semantics. Also languages can be taken as a
propositionally truncated view of the syntax. Logical equivalence should induce
weak equivalence, and thus even give a syntactic way to reason about language equivalence.
\steven{TODO language semantics}

% \subsection{Semantic Actions}
% \steven{Tentatively planning to cut this subsection for an internal representation
%   of semantic actions}
% Returning to the problem of parsing, the output of a parse usually is not the
% literal parse tree. Rather, the output is the result of some \emph{semantic
%   actions} ran on the parse tree, which usually serve to remove some syntactic
% details that are unnecessary for later processing.

% Given a grammar $G : \String \to \Set$, we define a semantic action to be a set
% $X$ with a function $f$ that produces a semantic element from any parse of $G$.

% \[
%   f : \PiTy w \String {G w \to X}
% \]

% Further, semantic actions can be arranged into a structured category.
% Define $\SemAct$ as the comma
% category $\Grammar / \Delta$, where $\Delta : \Set \to \Grammar$ defines a
% discrete presheaf. That is, for a set $X$, $\Delta (X)(w) = X$ for all
% $w \in \String$. As $\SemAct$ is defined as a comma category, it has a forgetful
% functor into $\Grammar$. That is, $\SemAct$ serves as a notion of formal
% grammar. Moreover, $\SemAct$ is a model of \theoryabbv.

% \steven{It being a notion of formal grammar is distinct from being a model. This
% probably warrants a proof}

% \steven{TODO semantic actions}

% \pedro{This is a very nice opportunity of showing off the supremacy of denotational
%   reasoning ;) We should probably prove the gluing lemma in the previous section
%   and apply it here and in the canonicity section. The actual proof might have to be
%   moved to the appendix, though}

\subsection{Parse Canonicity}
Canonicity is an important metatheoretic theorem in the type theory
literature.  It provides insight on the normal forms of terms and,
therefore, on its computational aspects. Frequently, proving
canonicity for boolean types, i.e. every closed term of type bool
reduces to either true or false, is enough to justify that the type
theory being studied is well-behaved. In our case, however, since we
want to connect \theoryabbv to parsers, we must provide a more
detailed account of canonicity. In particular, we give a nonstandard semantics
of \theoryabbv that carries a proof of canonicity along with it.

If $\cdot \vdash A$ is a closed linear type then there are
two obvious notions of what constitutes a ``parse'' of a string w
according to the grammar $A$:
\begin{enumerate}
\item On the one hand we have the set-theoretic semantics just
  defined, $\llbracket A \rrbracket \cdot w$
\item On the other hand, we can view the string $w = c_1c_2\cdots$ as
  a linear context $\lceil w \rceil = x_1:c_1,x_2:c_2,\ldots$ and
  define a parse to be a $\beta\eta$-equivalence class of linear terms $\cdot;
  \lceil w \rceil \vdash e : G$.
\end{enumerate}
It is not difficult to see that at least for the ``purely positive''
formulae (those featuring only the positive connectives
$0,+,I,\otimes,\mu, \overline\Sigma,c$) that
every element $t \in \llbracket A \rrbracket w$ is a kind of tree and
that the nodes of the tree correspond precisely to the introduction
forms of the type. However it is far less obvious that \emph{every}
linear term $\lceil w \rceil \vdash p : \phi$ is equal to some
sequence of introduction forms since proofs can include elimination
forms as well. To show that this is indeed the case we give a
\emph{canonicity} result for the calculus: that the parses for .

\begin{definition}
  A non-linear type $X$ is purely positive if it is built up using
  only finite sums, finite products and least fixed points.

  A linear type is purely positive if it is built up using only finite
  sums, tensor products, generators $c$, least fixed points and linear
  sigma types over purely positive non-linear types.
\end{definition}

\begin{definition}
  %% Let $X$ be a closed non-linear type. The closed elements $\textrm{Cl}(X)$ of $X$ are the definitional equivalence classes of terms $\cdot \vdash e : X$.

  Let $A$ be a closed linear type. The nerve $N(A)$ is a presheaf on
  strings that takes a string $w$ to the definitional equivalence
  classes of terms $\cdot; \lceil w\rceil \vdash e: N(A)$.
\end{definition}

\begin{theorem}[Canonicity]
  Let $A$ be a closed, purely positive linear type. Then there is an
  isomorphism between $\llbracket A\rrbracket$ and $N(A)$.
\end{theorem}
\begin{proof}
  We outline the proof here, more details are in the appendix. The
  proof proceeds first by a standard logical families construction
  that combines canonicity arguments for dependent type theory
  TODO cite coquand
  % \cite{coquand,etc}
  with logical relations constructions for linear
  types
  TODO cite hylandschalk
  % \cite{hylandschalk}
  . It is easy to see by induction that the
  logical family for $A$, $\hat A$ is isomorphic to $\llbracket A
  \rrbracket$ and the fundamental lemma proves that the projection
  morphism $p : \hat A \to N(A)$ has a section, the canonicalization
  procedure. Then we establish again by induction that
  canonicalization is also a retraction by observing that introduction
  forms are taken to constructors.
\end{proof}


\begin{enumerate}
\item Every term $\lceil w \rceil \vdash p : G + H$ is equal to $\sigma_1q$ or $\sigma_2 r$ (but not both)
\item There are no terms $\lceil w \rceil \vdash p : 0$
\item If there is a term $\lceil w \rceil \vdash p : c$ then $w = c$ and $p = x$.
\item Every term $\lceil w \rceil \vdash p : G \otimes H$ is equal to $(q,r)$ for some $q,r$
\item Every term $\lceil w \rceil \vdash p : \epsilon$ is equal to $()$
\item Every term $\lceil w \rceil \vdash p : c$ is equal to $x:c$
\item Every term $\lceil w \rceil \vdash p : \mu X. G$ is equal to $\textrm{roll}(q)$ where $q : G(\mu X.G/X)$
\item Every term $\lceil w \rceil \vdash p : (x:A) \times G$ is equal
  to $(M,q)$ where $\cdot \vdash M : A$
\end{enumerate}

To prove this result we will use a logical families model. We give a
brief overview of this model concretely:
\begin{enumerate}
\item A context $\Gamma$ denotes a family of sets indexed by closing substitutions $\hat\Gamma : (\cdot \vdash \Gamma) \Rightarrow \Set_i$
\item A type $\Gamma \vdash X : U_i$ denotes a family of sets $\hat X : \Pi(\gamma:\cdot \vdash \Gamma) \hat\Gamma \Rightarrow (\cdot \vdash \simulsubst X \gamma) \Rightarrow \Set_i$
\item A term $\Gamma \vdash e : X$ denotes a section $\hat e : \Pi(\gamma)\Pi(\hat\gamma)\hat X \gamma \hat\gamma (\simulsubst e \gamma)$
\item A linear type $\Gamma \vdash A : L_i$ denotes a family of grammars $\hat A : \Pi(\gamma:\cdot\vdash\Gamma)\,\hat\Gamma \Rightarrow \Pi(w:\Sigma^*) (\cdot;\lceil w\rceil \vdash A[\gamma])\Rightarrow \Set_i$, and the denotation of a linear context $\Delta$ is similar.
\item A linear term $\Gamma;\Delta \vdash e : A$ denotes a function \[\hat e : \Pi(\gamma)\Pi(\hat\gamma)\Pi(w)\Pi(\delta : \lceil w \rceil \vdash \simulsubst \Delta \gamma) \hat\Delta \gamma \hat\gamma \delta \Rightarrow \hat A \gamma \hat\gamma w {(\simulsubst {\simulsubst e \gamma} \delta)}\]
\end{enumerate}
And some of the constructions:
\begin{enumerate}
\item $\widehat {(G A)} \gamma \hat\gamma e = \hat A \gamma \hat\gamma \varepsilon (G^{-1}e)$
\item $\widehat {(A \otimes B)} \gamma \hat\gamma w e = \Sigma(w_Aw_B = w)\Sigma(e_A)\Sigma(e_B) (e_A,e_B) = e \wedge \hat A \gamma \hat \gamma w_A e_A \times \hat B \gamma \hat \gamma w_B e_B$
\item $\widehat {(A \lto B)} \gamma \hat\gamma w e = \Pi(w_A)\Pi(e_A) \hat A \gamma\hat\gamma w_A e_A \Rightarrow \hat B \gamma\hat\gamma (ww_A) (\applto {e_A} e)$
\end{enumerate}

First, the category with families will be
the category of logical families over set contexts/types
$\Delta$/$A$. Then the propositional portion will be defined by
mapping a logical family $\hat \Gamma \to \Gamma$

First, let $L$ be the category of BI formulae and proofs (quotiented
by $\beta\eta$ equality). Define a functor $N : L \to \Set^{\Sigma^*}$ by
\[ N(\phi)(w) = L(w,\phi) \]

Then define the gluing category $\mathcal G$ as the comma category
$\Set^{\Sigma^*}/N$. That is, an object of this category is a pair of
a formula $\phi \in L$ and an object $S \in \mathcal
P(\Sigma^*)/N(\phi)$. We can then use the equivalence $\mathcal
P(\Sigma^*)/N(\phi) \cong \mathcal P(\int N(\phi))$ to get a simple
description of such an $S$: it is simply a family of sets indexed by
proofs $L(w,\phi)$:
\[ \prod_{w\in\Sigma^*} L(w,\phi) \to \Set \]
This category clearly comes with a projection functor $\pi : \mathcal
G \to \mathcal L$ and then our goal is to define a section by using
the universal property of $\mathcal L$.

To this end we define
\begin{enumerate}
\item $(\phi, S) \otimes (\psi, T) = (\phi \otimes \psi, S\otimes T)$ where
  \[ (S \otimes T)(w, p) = (w_1w_2 = w) \times (q_1,q_2 = p) \times S\,w_1\,q_1 \times T\,w_2\,q_2\]
\item $(\phi, S) \multimap (\psi, T) = (\phi \multimap \psi, S \multimap T)$ where
  \[ (S \multimap T)(w,p) = w' \to q \to S\,w'\,q \to T (ww') (p\,q) \]
\item $\mu X. ??$ ??
\end{enumerate}

\pedro{We should conclude this section by explaining the relevance of the canonicity
theorem. Could also be done before stating the theorem.}

\section{Discussion and Future Work}
\label{sec:discussion}
\paragraph{Lambek Calculus and Categorial Grammar}
In 1958 \cite{lambek1958mathematics}, Joachim Lambek introduced his syntactic
calculus as a logical system for linguistic derivations. In fact, one may view
the subtheory of our calculus generated over $\otimes, \lto, \tol$ to be a proof
relevant presentation of Lambek's original system.

\steven{Should discussion of this connection to lambek calculus be spread
  throughout the paper?}

Lambek later explicitly connects his syntactic calculus to the structure of a
non-commutative biclosed monoidal category \cite{lambek1988categorial}. There is
a rich line of subsequent research after these revelations by Lambek, including
the study of pregroup grammars
\cite{coeckeMathematicalFoundationsCompositional2010}, the presentation of
abstract categorial grammars \cite{degrooteAbstractCategorialParsing2015}, and
even Lambek calculus with dependent types \cite{luoLambekCalculusDependent}. As
far as the authors can tell, these works seem to be primarily interested in natural
language semantics in the style of Montague grammars, rather than parsing formal
languages. That is, they seem to try to give a functorial semantic
interpretation of natrual language.

There seem to be some implementations from the abstract categorial grammar
toolkit, but again they seem natural language focused and not up to the task of
verified formal language parsing.

Also mention here that \cite{luoLambekCalculusDependent} seems similar at first
glance, but isn't as similar as it might look because the judgements and
dependent types seem to
take on a different interpretation of ours. Say something here like, ``as we
understand it, it differs in x,y,z ways''

\steven{TODO Edit the above}
\steven{Even though the above sounds bad, are there lambek/categorial things that are missed?}

%% 1. More practical: integrating it into a larger verified development
%%    - verified imperative implementation: Lambek logic ala separation
%%      logic?
%% 2. Semantic actions
%% 3. Type systems as tree grammars

\paragraph{Typed Approaches to Regular Expressions}
The usage of a simple type system to reason about regular expressions was
introduced by Frisch and Cardelli in 2004 \cite{frischCardelli}, and later expanded
by Hengelin and Nielsen in 2011 \cite{henglein_regular_2011} to handle proofs of regular expression
containment. In 1992, while investigating the addition of arbitrary recursion to Kleene
algebra \cite{leiss},
Lei{\ss} used a least fixed-point operator instead of the Kleene star. Although
he did not explicitly make use of a type system, or have access to the view of Frisch and
Cardelli, Lei{\ss} had added the full power of inductive types to a type system
of regular expressions.

Pratt's work on the residuation of action algebras in \cite{prattActionLogicPure1991} closely mirrors
our observation that Brzozowski derivatives mirror linear function types.
Moreover, he shows that his residuals form a Galois connection with sequential
composition. This is precisely a posetal --- or \emph{thin} --- version of the
adjunction between
linear function types and tensor product in \theoryabbv.

Our primary observation in this paper can be summarized as saying that all of
these above works can embed naturally into a rich, unifying theory.

\steven{I'd like to synthesize the discussion of these works with the categorial
grammar works, but I am not sure how. It seems as though there are these
parallel tracks of research that have missed each other for decades}


% In \cite{firsovCertifiedNormalizationContextFree2015}, Firsov and Uustalu
% constructed a verified normalization procedure to turn a
% context-free grammar into an equivalent grammar in Chomsky normal form.
% Combining their work with a CYK parser, they have built a verified context-free
% grammar parser, \emph{up to weak equivalence}. To upgrade from a weak
% equivalence to a strong equivalence, they suggest updating their grammar
% formalism to treat parse trees as first-class proofs of language membership.
% That is, without explicitly stating so, they suggest that they next natural step
% is to reason about grammars in a system where parse trees are terms, as we have
% given in this paper.

% \steven{Make the above paragraph shorter. Also idk if its worth including, but I
% like that they practically ask for a type theory}

\steven{Mention Neel's typed algebraic approach paper here. Say it is similar at
  first glance, as its parsing with types, but that he actually is representing
  something quite different. Or is it so different that its not worth mentioning?}

\steven{TODO add lambek calculus/categorial grammars here}

\paragraph{Kleene Algebra}
Since the early works in the theory of formal languages, Kleene
algebras have played an important role in its development. They
generalize the operations known from regular languages by introducing
operations generalizing language composition, language union and the
Kleene star.  More generally, they are defined as inequational theory
where the inequality is meant to capture language containment. This
theory is extremely successful, having found applications in algebraic
path problems, theory of programming languages, compiler optimizations
and more.

A frequently fruitful research direction is exploring varying
extensions of Kleene algebras, Kleene algebra with tests (KAT) being
one of the most notable ones. Our approach is radically different from
most extensions, which usually aim at modifying or adding new
operations to Kleene algebras, but still keeping it as an inequational
theory. By adopting a category-theoretic treatment and allowing the
``order structure'' to encode more information than merely
inequalities, we were able to extend Kleene algebra to reason about
parsing as well.

\steven{Can we sketch here what an apt categorification of KATs might be? As a
  poor first pass: perhaps a Kleene category with a full involutive
  subcategory that corresponds to the Boolean algebra?.}
\steven{What can we say about internalizing Kleene algebra proofs or extensions?}
\steven{any KA citations needed above?}

\paragraph{Linearity with Dependent Types}
\steven{I don't know if it worth discussing the different influences of
  categorical model here? We take Neel's model and add to it as necessary}

% Vakar might be worth citing to discuss different characterizations of the
% notion of model \cite{vakarSyntaxSemanticsLinear2015}

% \cite{fu2023twolevellineardependenttype} give a compiler for a two-level
% linear dependent type theory. This could serve as a template for a
% compiler implementation of \theoryname that is not just inference rules
% embedded in Agda.

\paragraph{Implementations in Parsing}
\steven{Talk about future implementation-side work}

\cite{fu2023twolevellineardependenttype} give a compiler for a two-level
linear dependent type theory. This could serve as a template for a
compiler implementation of \theoryname that is not just inference rules
embedded in Agda.

%%    - verified imperative implementation: Lambek logic ala separation
%%      logic?
\paragraph{Verified Imperative Implementations}
Lambek logic ala separation logic

\paragraph{Applications to Type Checking}
While parsing typically refers to the generation of semantic
objects from string input, many tasks in programming can be
viewed as parsing of objects with more structure, such as
trees with binding structure or graphs. Fundamental to the
frontend of many
programming language implementations are type systems. In
particular, \emph{type checking}
--- analogous to language recognition --- and \emph{typed
  elaboration} --- analogous to parsing --- arise when
producing a semantic object subject to some analysis. Just
as our string grammars were given as functors from $\String$
to $\Set$, we envision adapting the same philosophy
to functors from a \emph{category of trees} to $\Set$ to craft a syntax
that natively captures typed elaboration. This suggests an
unusual sort of bunched type theory, where context extension
no longer resembles concatenation of strings but instead
takes on the form of tree constructors.

\bibliographystyle{plain}
\bibliography{refs.bib}


\end{document}
