
% -*- fill-column: 80; -*-
\documentclass[review,anonymous,screen,acmsmall,nonacm]{acmart}
\usepackage{mathpartir}
\usepackage{tikz-cd}
\usepackage{lipsum}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{fancyvrb}
\usepackage{xspace}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{stmaryrd}
\usepackage{scalerel}
\usepackage{tikz}
\usetikzlibrary{automata, positioning, arrows, fit}

\usepackage{pdfpages}

\newcommand{\SPF}{\mathsf{SPF}}
\newcommand{\Var}{\mathsf{Var}}
\newcommand{\K}{\mathsf{K}}
\newcommand{\map}{\mathsf{map}}
\newcommand{\roll}{\mathsf{roll}}
\newcommand{\fold}{\mathsf{fold}}
\newcommand{\inl}{\mathsf{inl}}
\newcommand{\inr}{\mathsf{inr}}
\newcommand{\sem}[1]{\llbracket{#1}\rrbracket}
\newcommand{\cat}[1]{\mathbf{#1}}
\newcommand{\lto}{\multimap}
\newcommand{\tol}{\mathrel{\rotatebox[origin=c]{180}{$\lto$}}}
\newcommand{\String}{\textbf{String}}
\newcommand{\Set}{\mathbf{Set}}
\newcommand{\Syn}{\mathbf{Synx}}
\newcommand{\SemAct}{\mathbf{SemAct}}
\newcommand{\Gr}{\mathbf{Gr}}
\newcommand{\Grammar}{\mathbf{Gr}}
\newcommand{\Type}{\mathbf{Type}}
\newcommand{\Prop}{\mathbf{Prop}}
\newcommand{\Bool}{\mathbf{Bool}}
\newcommand{\true}{\mathbf{true}}
\newcommand{\false}{\mathbf{false}}
\newcommand{\nat}{\mathbb{N}}
\newcommand{\theoryname}{Dependent Lambek Calculus\xspace}
\newcommand{\theoryabbv}{$\textrm{Lambek}^D$}

\newcommand{\gluedNL}{{\mathcal G}_S}
\newcommand{\gluedNLUniv}{{\mathcal G}_{S,i}}
\newcommand{\gluedL}{{\mathcal G}_L}

\newcommand{\amp}{\mathrel{\&}}
\newcommand{\pair}{\amp}
\DeclareMathOperator*{\bigamp}{\scalerel*{\&}{\bigoplus}}
\DeclareMathOperator*{\bigwith}{\scalerel*{\&}{\bigoplus}}


\newcommand{\bang}{~\textbf{!}~}
% Tentatively using uparrow for linear to non-linear to be in line with
% Pfennings adjoint functional programming
\newcommand{\ltonl}[1]{~\uparrow #1}
\newcommand{\nil}{\mathsf{nil}}
\newcommand{\cons}{\mathsf{cons}}
\newcommand{\data}{\mathsf{data}}
\newcommand{\where}{\mathsf{where}}
\newcommand{\Trace}{\mathsf{Trace}}
\newcommand{\stringquote}[1]{\texttt{\textquotesingle#1\textquotesingle}}

\newcommand{\oplusinj}[2]{\sigma\,#1\,#2}
\newcommand{\withprj}[2]{\pi\,#1\,#2}

\newcommand{\simulsubst}[2]{#1\{#2\}}
\newcommand{\subst}[3]{\simulsubst {#1} {#2/#3}}
\newcommand{\el}{\mathsf{el}}
\newcommand{\letin}[3]{\mathsf{let}\, #1 = #2 \, \mathsf{in}\, #3}
\newcommand{\lamb}[2]{\lambda #1.\, #2}
\newcommand{\lamblto}[2]{\lambda^{{\lto}} #1.\, #2}
\newcommand{\lambtol}[2]{\lambda^{{\tol}} #1.\, #2}
\newcommand{\dlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\withlamb}[2]{{\lambda}^{{\&}} #1.\, #2}
\newcommand{\app}[2]{#1 \, #2}
\newcommand{\applto}[2]{#1 \mathop{{}^{\lto}} #2}
\newcommand{\apptol}[2]{#1 \mathop{{}^{\tol}} #2}
\newcommand{\PiTy}[3]{\textstyle\prod (#1 : #2). #3}
\newcommand{\SigTy}[3]{\textstyle\sum (#1 : #2). #3}
\newcommand{\LinPiTy}[3]{\textstyle\bigamp (#1 : #2). #3}
\newcommand{\LinSigTy}[3]{\textstyle\bigoplus (#1 : #2). #3}
%% \newcommand{\DepWith}[2]{{\textstyle\bigamp}\limits_{#1}{#2}
%% \newcommand{\DepPlus}[2]{{\textstyle\bigoplus}\limits_{#1}{#2}
\newcommand{\GrTy}{\mathsf{Gr}}

\newcommand{\equalizer}[3]{\{#1\,|\,\apptol {#2}{#1} = \apptol{#3}{#1} \}}
\newcommand{\equalizerpair}[2]{\langle #1 , #2 \rangle}
\newcommand{\equalizerpi}[1]{#1.\pi}

\newcommand{\ctxwff}[1]{#1 \,\, \mathsf{ok}}
\newcommand{\ctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{type}}
\newcommand{\linctxwff}[2]{#1 \vdash #2 \,\, \mathsf{ok}}
\newcommand{\linctxwffjdg}[2]{#1 \vdash #2 \,\, \mathsf{linear}}

\newif\ifdraft
\drafttrue
\newcommand{\steven}[1]{\ifdraft{\color{orange}[{\bf Steven says}: #1]}\fi}
\renewcommand{\max}[1]{\ifdraft{\color{blue}[{\bf Max says}: #1]}\fi}
\newcommand{\pedro}[1]{\ifdraft{\color{red}[{\bf Pedro says}: #1]}\fi}
\newcommand{\pipe}{\,|\,}

\begin{document}

\pagestyle{plain}

\pagebreak

\title{Intrinsic Verification of Parsers and Formal Grammar Theory in Dependent Lambek Calculus}

\author{Steven Schaefer}
\affiliation{\department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{stschaef@umich.edu}

\author{Max S. New}
\affiliation{
  \department{Electrical Engineering and Computer Science}
  \institution{University of Michigan}
  \country{USA}
}
\email{maxsnew@umich.edu}

\author{Pedro H. Azevedo de Amorim}
\affiliation{
  \department{Department of Computer Science}
  \institution{University of Oxford}
  \country{UK}
}
\email{pedro.azevedo.de.amorim@cs.ox.ac.uk}

\makeatletter
\let\@authorsaddresses\@empty
\makeatother

\begin{abstract}
  We propose \theoryname~(\theoryabbv), a domain-specific dependent type theory
  for the definition of intrinsically verified parsers and verification of weak
  and strong equivalences of formal grammars. The linear types in \theoryabbv
  have a simple interpretation as formal grammars, with values of the types
  interpreted as parse trees and general terms as a form of intrinsically
  verified parse transformers, of which parsers being a special case. We
  demonstrate the expressivity of this system by showing that the indexed
  inductive linear types in this calculus can be used to encode many different
  familiar grammar formalisms, including regular and context-free grammars but
  also traces of various types of automata. We can then construct strong
  equivalences between grammar and automata formalisms and construct verified
  parsers as simple linear functional programs. All of our examples have been
  implemented in a prototype implementation of \theoryabbv as a shallow
  embedding in Agda.

  Our shallow embedding is based on a denotational semantics of \theoryabbv
  where non-linear types denote ordinary sets and linear types denote formal
  grammars. To develop this semantics compositionally, we define a notion of
  general notion of model we call a \emph{Chomsky hyperdoctrine}, a
  generalization of the Kleene algebra approach to formal language theory. In
  addition to our ``standard model'' where linear types denote formal grammars,
  we demonstrate two non-standard models. The first interprets linear types as
  semantic actions over formal grammars, suggesting future extensions of our
  calculus. The second is used to structure a logical relations argument to
  prove a canonicity metatheorem for the type theory, proving that all closed
  terms of context-free linear types normalize to a parse tree.
\end{abstract}

\maketitle

\section{Introduction}
\label{sec:intro}

%% Outline
%% - Why a framework for parser verification
%% - what intrinsic verification gets us
%% - the secret sauce: indexed linear inductive types
%% - prototype implementation
%% - overview of "abstract" formal grammars?


Parsing data from a serialized format is one of the most common tasks
in all of computing. Accordingly, this is a quite well-studied problem
spanning theoretical computer science and linguistics with such
milestones as Chomsky's hierarchy of grammars
\cite{chomThreeModels1956}, practical algorithms for parsing of
regular and context-free grammars, and variants
\cite{KNUTH1965607,Earley1970}, as well as implementations of
practical tools for the generation of efficient parsers
\cite{Johnsonyacc}.

The theory of formal languages and parsing is one of the oldest and most
thoroughly developed area of theoretical computer science.
The central object of study is a \emph{formal language} $L$ over an alphabet
$\Sigma$, which is classically defined as a
\emph{subset} of strings $L \subseteq \String = \Sigma^{*}$. The expressivity of formal
languages is sufficient for defining a \emph{recognizer} of a language $L$:
a function $r : \String \to \Prop$ such that $r$ maps each string $w$ to the
proposition that $w$ is in the language $w \in L$. This technique of language
recognition can concisely specify lexical analysis, and even lead to verified
lexer implementations \cite{egolfVerbatim,Ouedraogo_2023}.

While there are ample theory and tooling to handle the first stage of a language frontend, the final
frontier for parsing research is the development of
\emph{formally verified} parser implementations. Due to the
ubiquitous nature of parsing, these would useful components of many
formally verified software systems: compilers and servers most
especially. However, language recognition is insufficient for specifying a
parser, and for this task much of the structured foundations from the theory of
formal languages is rendered moot. Rather than returning a propositon-valued result, a parser produces a
structured \emph{parse tree} adherent to a \emph{formal grammar}.
The research on such grammars is often performed atop an ad-hoc choice of grammar
formalism, and results are proven with respect to that formalism. Critically,
the field is lacking a unifying theory across these different formalisms, even
though many of the same
compositional reason principles appear across distinct grammar formalisms.

In this work, we aim to elucidate these similarities and
provide a mathematical theory of formal grammars for constructing verified
parsers. Our approach can be summarized with the
Curry-Howard-style slogan: ``grammars as types''; and the addendum: ``parsers as
terms''. That is, we develop a type theory with a novel
denotational semantics in a category with abstract formal grammars as
objects and \emph{parse transformers} as morphisms. We say these are
\emph{abstract} formal grammars because they are not tied to any
particular syntactic formalism such as Chomskyan grammars, regular
expressions, Lambek calculus etc. The category we choose is a rich
in structure: it is a topos as well as being monoidal closed, and these
universal properties serve as the inspiration for our type theory.

The type theory itself is a form of dependent linear-non-linear type
theory: the linear types can be interpreted as grammars, whereas the
non-linear types can be interpreted as ordinary sets. As in prior work
on dependent linear-non-linear types, the linear types are allowed to
depend on the non-linear types but not vice-versa \cite{krishnaswami_integrating_2015}.

The substructural nature of \theoryabbv is well-aligned with the
requirements intrinsic to parsing and to the theory of formal
languages, where strings constitute a very clear notion of resource
that cannot be duplicated, reordered, or dropped. Moreover, the constructive
aspect of \theoryabbv ensures that verification of parsers written in the
calculus are \emph{correct-by-construction}. The type system is powerful enough
that derivations of a term type checking carry intrinsic proofs of correctness.

\paragraph{Contributions} This paper begins in \cref{sec:synindnotion} by
giving a universal semantic characterization of formal grammar.
From there, in \cref{sec:tt} we provide the syntax of \theoryname, a type theory
that axiomatizes the underlying structures needed to conduct formal grammar
theory. In \cref{sec:automata} we demonstrate the usage of
\theoryname for defining parser terms and internalizing results from formal language theory.
Then in \cref{sec:categorify}, we characterize the abstract categorical
models of our syntax. Afterwards in \cref{sec:othermodels}, we demonstrate several models,
including one to prove a metatheoretic canonicity theorem.
Our contributions are then:
%
\begin{itemize}
  \item A syntax-independent notion of grammar given by the category $\Grammar$
  \item \theoryname: A dependent
    linear-non-linear type theory meant to syntactically capture the latent structures
    in $\Grammar$ that enable parsing
  \item Categorical structures that capture the concepts integral to formal grammar theory, and that are notably modeled by $\Grammar$
  \item An implementation of the language, which is embedded shallowly in Agda
\end{itemize}

\subsection{A Syntax-Independent Notion of Syntax}
\label{sec:synindnotion}

The notion of formal language is central to the theory of parsing.
This definition is
especially useful as it gives a semantics to formal grammars that is completely
independent of any particular syntactic grammar formalism. Any new
notion of grammar can be given a language semantics and it provides a precise
mathematical specification for implementing a \emph{recognizer} of a language.
In this section we propose a \emph{proof-relevant} generalization of formal
languages that can accommodate richer aspects of formal language theory such as
parsing. We begin by proposing a presheaf approach to grammars that encompasses
other classes of grammars in a syntax-independent way.

\paragraph{From Formal Languages to Formal Grammars}

Our first contribution is a novel and syntax-independent characterization of
formal grammar:

\begin{definition}
  \label{def:grammar}
  A \emph{formal grammar} $G$ over a fixed alphabet $\Sigma$ is a function
  $G : \String \to \Set$.
\end{definition}

We say that a grammar $G$ associates to every string $w$ the set\footnote{We
  make little comment as to which foundations are used to encode $\Set$. Our
  construction is polymorphic in choice of a proper class of small sets, a
  Grothendieck universe, a type theoretic universe, or any similar foundation.} $G w$ of parse
trees showing that $w$ matches $G$. This definition of formal grammar serves as
a mathematical specification for a parser --- taking on the same role as
languages do for recognizers. In this sense, our new characterization of
grammars is just a proof-relevant generalization of the existing notion of
formal language.

By ranging over all input strings, notice that each grammar induces a language.
For each formal grammar $G$, define the language of accepted strings $L_{G}$ as,

\[ L_{G} = \{ w \in \String ~|~ G w~\text{is inhabited} \} \]

As they are simply sets, formal languages are naturally endowed with a partial
order via subset inclusion. Similarly, formal grammars naturally coalesce into a
\emph{category}. Define $\Grammar$ to be the category of formal grammars. Whose
objects are grammars, and whose morphisms are functions $f : G \to H$ between two
grammars presented by a family of functions

\[ f^{w} : G w \to H w \]

for every string $w \in \String$. Intuitively, we read a morphism of grammars as
a \emph{parse transformer} --- i.e. $f$ translates a $G$-parse of $w$ into an $H$-parse
of $w$. The induced notion of isomorphism in $\Grammar$ encodes that two grammars are equivalent if there is
a bijective translation of parses. In other words, two grammars are isomorphic
if they capture the same structural descriptions of strings, which is
precisely Chomsky's notion of \emph{strong equivalence} of
grammars \cite{chom1963}. In works like
\cite{yoshinaga2002formal}, this notion of strong equivalence is often treated
like the appropriate choice of ``sameness'' for two grammars. When abstracting over the latent
structure of formal grammars and describing it in the the language of
categories, it is encouraging to see the same definition of equivalence arise
naturally.

Upon inspection of the above definition, $\Grammar$ may equivalently
be described as the functor category $\Set^{\String}$ --- where the
set $\String$ is viewed as a discrete\footnote{Objects in $\String$
comprise the set of all strings, and the only morphisms are the
identity morphisms.}  category. In fact, in this paper we will take
$\Grammar$ to be defined as the functor category $\Set^{\String}$ ---
also called presheaf category. Such presheaf categories carry a
remarkable amount of structure.  Indeed they are a monoidal topos and,
as such, are a model to a substructural dependently typed theory.

\paragraph{From Grammars to Formal Grammars}
The category-theoretic framework can be further used to describe and compare
different notions of formal grammar. For instance, Chomskyan generative
grammars, semi-Thue systems, Montague grammars, and so on are all distinct syntactic
presentations of the same underlying idea of an abstract specification for
parsing. Each of these is an instance of a general \emph{notion of formal
  grammar} --- a category paired with a functor into
$\Grammar$. That is, they may all be interpreted in our category of grammars. In
this sense, $\Grammar$ provides an
all-encompassing semantic domain for parsing.

\max{this section kind of trails off at the end with a very vague aside about a $2$-category of grammar formalisms. Maybe this should go in future work/discussion? What is the message of this section? How does it tie in with the message of the paper? What even is the message of this paper?}

\pedro{Would building on this proposed structure make the message of the section clearer? In this case
  we would have to be a bit clearer in terms of how familiar notions of grammar can be seen as categories with functor into $\Grammar$.}


\section{\theoryname by Example}
\label{sec:type-theory-examples}

\steven{To save space, each of these example figures can be
  minified/potentially inlined}

To gain intuiton for working in \theoryname, let's walk through some examples of
grammars and parses internal to it. In each of our examples, we will look at
grammars over the three character alphabet $\{ a , b , c\}$.

Each letter of the
alphabet corresponds to a base type in our calculus. For instance, the grammar
for the single character $a$ has a single parse tree for the string \stringquote{a}.
and no parse trees at any other strings.

\paragraph{Finite Grammars}
First consider finite grammars --- those built from base types via dijunctions $\oplus$ and
concatenations $\otimes$. For instance, $(a \oplus b) \otimes c$.
In \cref{fig:fingram} we find a proof tree showing that the input string \stringquote{ac} matches
the grammar $(a \oplus b) \otimes c$.

\begin{figure}
\begin{mathpar}
  \inferrule
  {
    \inferrule
    {~}
    {x : a \vdash \inl(x) : a \oplus b}
    \\
    \inferrule
    {~}
    {y : c \vdash y : c}
  }
  {x : a , y : c \vdash \inl(x) \otimes y : (a \oplus b) \otimes c}
\end{mathpar}
\caption{\stringquote{ac} matches $(a \oplus b) \otimes c$}
\label{fig:fingram}
\end{figure}

To parse the specific string \stringquote{ac}, we populate the context with an
internalized encoding of that string. In contexts of this form, linear terms
describe the structure of a parse tree. That is, $\inl(x) \otimes y$ details the
manner by which \stringquote{ac} matches $(a \oplus b) \otimes c$.

Because derivations in a linear context correspond to parse trees in this
manner, we cannot affirm all of the usual structural rules of a type system.
Weakening and contraction each allow for modification of the string in context ---
weakening allows for characters to be dropped at will, while contraction allows
for duplication of characters. Omission of these rules offers a
resource-sensitive view that ensures only the inputted characters are parsed.
Moreover, strings are not simply sets of characters, as they carry an inherent
ordering. Thus, the rule of exchange is also omitted in this calculus so that
the constituent characters of a string are not freely permuted.

Without the above structural rules we
arrive at an ordered, linear type system, and to summarize this
particular choice of substructural restrictions: when parsing, characters must
be processed
without modification and in the order in which they were written.

\paragraph{Regular Expressions}
Next, we demonstrate how to parse a regular expression in \theoryabbv. Similar
to finite grammars, regular
expresssions are built inductively using base types, $\oplus$, and $\otimes$ ;
however, unlike finite grammars, regular expressions additionally have a
restricted form of recursion via Kleene star $(\cdot)^{\ast}$.

% TODO this should be beter formatted but can't use listings with \otimes ^{\ast}
\begin{figure}
\begin{align*}
\data &~A^{*}: L_{i}~\where\\
      & \nil : I \lto A^{*} \\
      & \cons : A \otimes A^{*} \lto A^{*}
\end{align*}
\caption{Kleene Star as an inductive type}
\label{fig:kleenestarinductive}
\end{figure}

For a grammar $A$, the recursive nature of $A^{*}$ is captured by defining
$A^{*}$ to be an inductive linear
type, as shown in \cref{fig:kleenestarinductive}. Just as in \cref{fig:fingram}, we can show that particular strings match a Kleene
star. In \cref{fig:kleenestarderivation}, we demonstrate that \stringquote{ab}
matches $a^{*} \otimes b$ through repeated application of the Kleene star constructors.

\begin{figure}
\begin{mathpar}
  \inferrule
  {
    \inferrule
    {
      \inferrule
      {~}
      {x : a \vdash x : a}
      \\
      \inferrule
      {
        \inferrule
        {~}
        {I \vdash \nil : a^{*}}
      }
      {\cdot \vdash \nil : a^{*}}
    }
    {x : a \vdash \cons (x , \nil) : a^{*}}
    \\
    \inferrule
    {~}
    {y : b \vdash y : b}
  }
  {x : a , y : b \vdash \cons(x ) \otimes y : a^{*} \otimes b}
\end{mathpar}
\caption{\stringquote{ab} matches $a^{*} \otimes b$}
\label{fig:kleenestarderivation}
\end{figure}

However, we can have derivations where the term in context is not simply a
string of base types. For instance, by using the elimination principle for
Kleene star, \cref{fig:kleeneabstractproof} builds a term of type
$A^{*}$ in context comprising abstract linear types. In these contexts, it is
not quite appropriate
to treat linear terms as parse \textit{trees}, because the information in
context is not a string. Instead, it is more
appropriate to think of such terms as parse \textit{transformers}. That is, in
\cref{fig:kleeneabstractproof} the
inhabitant $x : (A \otimes A)^{*}$ in the context is not itself data to be parsed; rather,
$x$ is a parse tree and the term from context $x : (A \otimes A)^{*}$ to the
type $A^{*}$ is a transformation of parse trees of the input grammar into parse
trees of the output grammar.

\steven{TODO : describe elimination principle somewhere. Maybe just in prose?}
\steven{I probably can't get away with the inline definition of $f$ below}
\begin{figure}
\begin{mathpar}
  \inferrule
  {
    \inferrule
    {~}
    {x_I : I \vdash \nil : A^*}
    \\
    \inferrule
    {
      \inferrule
      {~}
      {x_1 : A , x_2 : A , y : A* \vdash \cons (x_1 , \cons(x_2 , y)) : A^*}
    }
    {x : A \otimes A , y : A^*  \vdash f := \letin {x_1 \otimes x_2} {x} {\cons(x_1 ,
        \cons(x_2 , y))} : A^*}
  }
  {x : (A \otimes A)^* \vdash \fold(\nil , f) : A^* }
\end{mathpar}
\caption{An example mapping out of abstract grammars}
\label{fig:kleeneabstractproof}
\end{figure}

\paragraph{Non-deterministic Finite Automata}
As a final example, consider the non-deterministic finite
automaton (NFA) $N$ from \cref{fig:exampleNFA}. Such automata play a critical
role, as they serve as operationalized and
executable presentation of a formal grammar. Therefore, it is crucial that we
can represent them internal to \theoryabbv.

Our method of representing these automata internally is to provide an \emph{indexed}
inductive type of traces through the machine. For $N$, this indexed inductive
type is given in \cref{fig:nfainductive}.

The type of traces in $N$ is indexed by two pieces of data: a state in $N$ and a
Boolean. For each state $s : N.\mathsf{state}$ and $acc : \Bool$, we define the
type of traces from state $s$, $\Trace_{N}~s~acc$. The Boolean flag $acc$ marks if the trace
ends in either an accepting or a rejecting state. Let us dissect the structure
of this type in more detail for state 1 of $N$.

$\Trace_{N}~1~acc$ has three constructors. The first constructor
$\mathsf{stop}_{1}$ allows a trace to end at state one, and because state
1 is accepting $acc$ is true when terminating at state 1.

The constructor
$\mathsf{1to2}$ denotes the $b$-labeled transition in $N$ from state 1 to state
2. The character $b$ is consumed when taking this transition and then a trace
beginning at state 2 must be produced, which is denoted by the tensor product of
the grammar $b$ and $Trace_{N}~2~acc$. Finally, the constructor $\mathsf{1to2}$
abstracts over the acceptance Boolean because it is not yet determined.
Because the Boolean is set by the acceptance criteria of the \emph{final} state,
it is provided inductively by the trace out of state 2.

The constructor $\mathsf{1to3}$ is built quite similarly as $\mathsf{1to2}$;
however, $\mathsf{1to3}$ does not involve a tensor product. This is because the
transition from state 1 to state 3 is labeled by the empty string $\epsilon$, so
the transition consumes no input. In this case, when building a trace out of
state 1, it suffices to build a trace out of state 3 \emph{over the same string}.

Overall, leveraging \emph{indexed} inductive types provides a concise interface
for defineing a family
of mutually recursive types over the indexing data.

\begin{figure}
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial, accepting] (1) {$1$};
    \node[state, below left of=1] (2) {$2$};
    \node[state, right of=2] (3) {$3$};

    \path[->] (1) edge[above] node{$b$} (2)
              (1) edge[below, bend right, left=0.3] node{$\epsilon$} (3)
              (2) edge[loop left] node{$a$} (2)
              (2) edge[below] node{$b$} (3)
              (3) edge[above, bend right, right=0.3] node{$a$} (1);
  \end{tikzpicture}
  \caption{NFA $N$}
  \label{fig:exampleNFA}
\end{figure}

\begin{figure}
\begin{align*}
\data &~\Trace_{N}~1~(acc : \Bool) : L_{i}~\where\\
      & \mathsf{stop}_{1} : I \lto \Trace_{N}~1~\true \\
      & \mathsf{1to2} : \forall (acc : \Bool) \to b \otimes \Trace_{N}~2~acc \lto \Trace_{N}~1~acc \\
      & \mathsf{1to3} : \forall (acc : \Bool) \to \Trace_{N}~3~acc \lto \Trace_{N}~1~acc \\
\\
\data &~\Trace_{N}~2~(acc : \Bool) : L_{i}~\where\\
      & \mathsf{stop}_{2} : I \lto \Trace_{N}~2~\false \\
      & \mathsf{2to2} : \forall (acc : \Bool) \to a \otimes \Trace_{N}~2~acc \lto \Trace_{N}~2~acc \\
      & \mathsf{2to3} : \forall (acc : \Bool) \to b \otimes \Trace_{N}~3~acc \lto \Trace_{N}~2~acc \\
\\
\data &~\Trace_{N}~3~(acc : \Bool) : L_{i}~\where\\
      & \mathsf{stop}_{3} : I \lto \Trace_{N}~3~\true \\
      & \mathsf{3to1} : \forall (acc : \Bool) \to a \otimes \Trace_{N}~1~acc \lto \Trace_{N}~3~acc \\
\end{align*}
\caption{$N$ as an indexed inductive type}
\label{fig:nfainductive}
\end{figure}

%% 3. Third example: NFA traces to show the indexed inductive types

\section{Syntax and Typing for \theoryname}
\label{sec:tt}

Next we present the full details of the syntax and typing of \theoryname.
\theoryname has two kinds of types, linear types $A$ which represent formal
grammars, and non-linear types $X$, which act like ordinary types or sets. Both
linear and non-linear types are checked for well-formedness in a context
$\Gamma$ containing \emph{only} non-linear variables $\Gamma = x:X,\ldots$.

\begin{figure}
  \[
  \begin{array}{rrcl}
    \textrm{non-linear contexts} & \Gamma & ::= & \cdot \pipe \Gamma,x:X \\
    \textrm{non-linear types} & X , Y , Z & ::= & \mathsf{Nat} \pipe
                                                  \mathsf{Int} \pipe
                                                  \mathsf{Empty} \pipe
                                                  \mathsf{Unit} \pipe
                                                  \mathsf{SPF}\,X\pipe U \pipe L
                                                  \pipe \ltonl A \pipe \sum_{x:X} Y
                                                  \pipe \prod_{x:X} Y\pipe M =_A
                                                  N \\
    \textrm{linear contexts} & \Delta & ::= & \cdot \pipe \Delta , a:A\\
    \textrm{linear types} & A , B , C & ::= &
    c \pipe I \pipe A \otimes B \pipe \bigoplus\limits_{x:X} A \pipe \mu A_f M
    A \lto B \pipe B \tol A \pipe \bigwith\limits_{x:X} A \pipe \equalizer {a}{f}{g}\\
  \end{array}
  \]
  \caption{Overview of Types}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma ; \Delta \vdash a : A}

    \inferrule{~}{\Gamma ; a : A \vdash a : A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \\ \linctxwffjdg \Gamma {A \equiv B}}{\Gamma ; \Delta \vdash e : A}
    %
    \\
    %
    \inferrule{~}{\Gamma ; \cdot \vdash () : I}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : I \\ \Gamma ; \Delta_1',\Delta_2' \vdash e' : C}{\Gamma ; \Delta_1',\Delta,\Delta_2' \vdash \letin {()} e {e'} : C}
    %
    \\
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \\ \Gamma ; \Delta' \vdash e' : B}{\Gamma ; \Delta, \Delta' \vdash e \otimes e' : A \otimes B}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta \vdash e : A \otimes B \\ \Gamma ; \Delta'_1, a : A, b : B, \Delta'_2 \vdash e'}{\Gamma ;  \Delta_1', \Delta, \Delta'_2 \vdash \letin {a \otimes b} e {e'}}
    \\
    %
    \inferrule{\Gamma ; a : A , \Delta \vdash e : B}{\Gamma ; \Delta \vdash \lamblto a e : A\lto B}
    \and
    \inferrule{\Gamma ; \Delta' \vdash e' : A \\ \Gamma ; \Delta \vdash e : A \lto B}{\Gamma ; \Delta', \Delta \vdash \applto {e'} {e} : B}
    \\
    %
    \inferrule{\Gamma ; \Delta , a : A \vdash e : B}{\Gamma ; \Delta \vdash \lambtol a e : B\tol A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : B \tol A \\ \Gamma ; \Delta' \vdash e' : A}{\Gamma ; \Delta, \Delta' \vdash \apptol e {e'} : B}
    %
    \\
    %
    \inferrule{\Gamma, x : X ; \Delta  \vdash e : A}
              {\Gamma ; \Delta \vdash \dlamb x e : \LinPiTy x X A}
    \and
    \inferrule{\Gamma ; \Delta \vdash e : \LinPiTy x X A \\ \Gamma \vdash e' : X}{\Gamma ; \Delta \vdash e\,.\pi\,M : \subst A {e'} x}
    %
    \\
    %
    \inferrule{\Gamma \vdash M : X \quad \Gamma ; \Delta \vdash e : \subst A e x}{\Gamma ; \Delta \vdash \sigma\,M\,e : \bigoplus\limits_{x:X} A}
    %
    \and
    %
    \inferrule{\Gamma ; \Delta \vdash e : \bigoplus\limits_{x:X} A \quad \Gamma, x : X ; \Delta'_1, a : A, \Delta'_2 \vdash e' : C}{\Gamma; \Delta'_1, \Delta, \Delta'_2 \vdash \letin {\sigma\,x\,a} e {e'}: C}
    %
    \and
    %
    \inferrule
    {~}
    {\Gamma , x : \ltonl A ; \cdot \vdash x : A}
    %
    \and
    %
    \inferrule
    {\Gamma \vdash f : \ltonl {(A \lto B)} \\
      \Gamma \vdash g : \ltonl { (A \lto B) } \\
      \Gamma ; \Delta \vdash a : A \\
      \Gamma ; \Delta \vdash p : \apptol {f}{a} = \apptol {g}{a}}
    {\Gamma ; \Delta \vdash \equalizerpair{a}{p} : \equalizer {a}{f}{g}}
    %
    \and
    %
    \inferrule
    {\Gamma \vdash f : \ltonl { (A \lto B) } \\
      \Gamma \vdash g : \ltonl { (A \lto B) } \\
      \Gamma ; \Delta \vdash e : \equalizer{a}{f}{g}}
    {\Gamma ; \Delta \vdash \equalizerpi {e} : A}
    %% \\
    %% %
    %% \inferrule{~}{\Gamma ; \Delta \vdash () : \top}

    %% \and

    %% \inferrule{~}{\Gamma ; x : \bot \vdash \mathsf{absurd}_A : A}
    %% %
    %% \and
    %% %
    %% %
    %% \inferrule{\Gamma \vdash e : \mathsf{G} A}{\Gamma ; \cdot \vdash \mathsf{G}^{-1}\, e : A}
    %% %
  \end{mathpar}
  \caption{Linear typing}
  \label{fig:linsyntax}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \inferrule{~}{\Gamma \vdash \Set_0 : \Set_1}
 %
    \and
%
    \inferrule{~}{\Gamma \vdash L_i : U_{i+1}}
    {}{}
  \end{mathpar}
  \caption{Universe Types}
\end{figure}

\begin{figure}
  \textrm{TODO: var, konstant, bigoplus, bigamp and tensor}\\
  \textrm{TODO: behavior of map}
  \begin{mathpar}
    \inferrule
    {\Gamma \vdash M : X}
    {\Gamma \vdash \mathsf{Var}\,M : \SPF\,X}

    \inferrule
    {\Gamma \vdash A : L}
    {\Gamma \vdash \mathsf{K}\,A : \SPF\,X}

    \inferrule
    {\Gamma,y:Y \vdash A : \SPF\,X}
    {\Gamma \vdash \bigoplus_{y:Y}\,A : \SPF\,X}

    \inferrule
    {\Gamma,y:Y \vdash A : \SPF\,X}
    {\Gamma \vdash \bigamp_{y:Y}\,A : \SPF\,X}

    \inferrule
    {\Gamma \vdash A : \SPF\,X\and \Gamma \vdash B : \SPF\,X}
    {\Gamma \vdash A \otimes B : \SPF\,X}

    \inferrule
    {A : \SPF\,X}
    {\el(A) : (X \to L) \to L}

    \el(\Var\,M) B = B\,M\and
    \el(\mathsf{K}\,A) B = A\and
    \el(\bigoplus_{y:Y} A) B = \bigoplus_{y:Y}\el(A)B\and
    \el(\bigamp_{y:Y} A) B = \bigamp_{y:Y}\el(A)B\and
    \el(A \otimes A') B = \el(A)B \otimes \el(A')B
    
  \inferrule
  {A : \textrm{SPF} X\and
  \Gamma,x:X;b: B x \vdash f : C x\and
  \Delta \vdash e : \el(A)(B)
  }
  {\Gamma;\Delta \vdash \mathsf{map}\,A\,(x.b.f)\,e : \textrm{el}(A)\, C}

  \map(\Var\,M)\,(x.b.f)\,e = f[M/x][e/b]\and
  \map(\K\,A)\,(x.b.f)\,e = e\and
  \map(\bigoplus\limits_{y:Y} A)\,(x.b.f)\,e =
  \letin{\oplusinj y a}{e}{\oplusinj y {\map(A)\,(x.b.f)\,e}}\\
  \map(\bigwith\limits_{y:Y} A)\,(x.b.f)\,e =
  \withlamb{y}{\map(A)\,(x.b.f)\,(\withprj a e)}\\
  \map(A \otimes A')\,(x.b.f)\,e =
  \letin{(a,a')}{e}{(\map(A)\,(x.b.f)\,a,\map(A')\,(x.b.f)\,a')}
  \end{mathpar}
  \caption{Strictly Positive Functors}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \inferrule*[right=IndIntro]
    {\Gamma \vdash A : X \to \textrm{SPF}\,X\and
     \Gamma \vdash M : X\and
     \Gamma; \Delta \vdash e : \textrm{el}(A\,M) (\mu\,A)}
    {\Gamma; \Delta \vdash \mathsf{roll}\, e : \mu\,A\, M}

    \inferrule*[right=IndElim]
    {\Gamma;\Delta \vdash e : \mu A M\and
     \Gamma,x:X;b:\el(A\,x)\,B \vdash f : B\,x
    }
    {\Gamma; \Delta \vdash \mathsf{fold}(A)(x.b.f)(M, e) : B\,M}

    \inferrule*[right=Ind$\beta$]
    {}
    {\fold(A)(x.b.f)(M, \roll(e)) =
      f[M/x][\map(A\,M)\,(y.a.\fold(A)(x.b.f)(y,a))\,e/b]}

    \inferrule*[right=Ind$\eta$]
    {\Gamma,x:X; a:\mu\,A\,x \vdash e' : B\,x
      \and \Gamma;\Delta \vdash e : \mu A M\\\\
      \Gamma,x:X; a':\el(A x)(\mu A) \vdash
      e'[x/x][\roll(a')/a] = f[x/x][\map(A\,x)\,(y.a.e'[y/x][a/a])\,a/b] : B\,x
    }
    {\Gamma; \Delta \vdash \fold(A)(x.b.f)(M,e) = e'[M/x][e/a] : B\,M}
  \end{mathpar}
  \caption{Indexed Inductive Linear Types}
\end{figure}

Omission of the structural rules of a deductive system, such as in
linear logic \cite{GIRARD19871}, offers precise control over how a value is used
in a derivation. Linear logic omits the weakening and contraction rules
to ensure that every value in context is used exactly once. This control enables
\emph{resource-sensitive} reasoning, where we may treat a resource as
\emph{consumed} after usage. This viewpoint is amenable to parsing applications,
as we may treat characters of a string as finite resources that are consumed at
parse-time. That is, when reading a string, the occurrence of any character is read once. Freely duplicating or dropping characters from a string changes the meaning of that string. \max{what about a backtracking parser?}. One may then envision a linear type system where the types comprise
formal grammars generated over some alphabet $\Sigma$, and the type constructors
correspond precisely to inductive constructions on grammars --- such as
conjunction, disjunction, concatenation, etc.

Programming in a purely linear term language is limiting due to the variable
usage restrictions. Code in such a language can
become unnecessarily verbose when ensuring the linearity invariant.
To alleviate this
concern, in 1995 Benton et al.\ proposed an alternative categorical
semantics of linear logic based on adjoint interactions between linear
and non-linear logics \cite{bentonMixedLinearNonlinear1995} ---
appropriately referred to as a \emph{linear-non-linear} (LNL)
system. This work is simply typed, so the boundary between linear and
non-linear subtheories is entirely characterized via a monoidal
adjunction $F \dashv G$ between linear terms and non-linear terms.

Inside of an LNL system, linearity may be thought of as an option that users can
choose to deliberately invoke at deliberate points in their developments in an
otherwise intuitionistic system. However, if we are wishing to treat parsers as
linear terms over input strings, the non-linear fragment of an LNL theory does
not really assist in the development of parsers. It is instead the case that
parsers may benefit from a \emph{dependence} on non-linear terms.
Through the approach described by Krishnaswami et al.\ in
\cite{krishnaswami_integrating_2015},
we may define a restricted form of dependent types. In particular, dependence
on linear terms
is disallowed; however, through dependence of a linear term on a non-linear
index, we recover the definition of Aho's indexed grammars \cite{AhoIndexed}
internal to \theoryabbv.

In this section we go on a tour of \theoryabbv. We start by presenting its syntax.
In order to avoid a notion of raw-term, we present it in an intrinsically-typed fashion,
where only well-typed terms are meaningful. Next, we show how it admits a model in $\Set^{\String}$
where the linear types have natural interpretation in terms of formal languages --- for instance,
the tensor corresponds to grammar concatenation. We conclude the section by showing how
classes of grammars correspond to certain sub-type-theories inside \theoryabbv.

\subsection{Syntax}
\label{subsec:syntax}

Below we describe \theoryname~(\theoryabbv), an instance of an LNL theory with dependent types
to serve as a deductive
setting for formal grammar theory. \theoryabbv axiomatizes the necessary
structure underlying $\Grammar$ to
specify and construct parsers.

The structural judgments are shown in \cref{fig:structjdg}, the typing
well-formedness in \cref{fig:typewf}, and the intuitionistic typing
rules in \cref{fig:inttyping}. These are mostly just as they appear in
\cite{krishnaswami_integrating_2015}. It has two main differences. The
first one it the presence of two-distinct arrow types, one for adding
variables to the beginning of contexts and another one for adding
variables to the end of contexts. This is an adequate change in the
context of grammars, which are inherently non-symmetric. The second
change is the introduction of inductive types, which allows for the definition
of recursive grammars. The treatment of the non-linear
types is standard, so below we focus on the linear syntax.

\begin{figure}
  \begin{mathpar}
    \boxed{\ctxwff \Gamma}

    \inferrule{~}{\ctxwff \cdot}
    \and
    \inferrule{\ctxwff \Gamma \\ \ctxwffjdg \Gamma X}{\ctxwff {\Gamma, x : X}}

    \\

    \boxed{\linctxwff \Gamma \Delta}

    \inferrule{~}{\linctxwff \Gamma \cdot}
    \and
    \inferrule{\linctxwff \Gamma \Delta \\ \linctxwffjdg \gamma A}{\linctxwff \Gamma {\Delta, a : A}}

    \\

    \boxed{\ctxwffjdg \Gamma X}

    \inferrule{\Gamma \vdash X : U_i}{\ctxwffjdg \Gamma X}

    \boxed{\linctxwffjdg \Gamma A}

    \inferrule{\Gamma \vdash A : L_i}{\linctxwffjdg \Gamma A}

    \\

    \boxed{\ctxwffjdg \Gamma {X \equiv Y}}

    \inferrule{\Gamma \vdash X \equiv Y : U_i}{\ctxwffjdg \Gamma {X\equiv Y}}

    \boxed{\linctxwffjdg \Gamma {A \equiv B}}

    \inferrule{\Gamma \vdash A \equiv B : L_i}{\linctxwffjdg \Gamma {A \equiv B}}

  \end{mathpar}
  \caption{Structural judgments}
  \label{fig:structjdg}
\end{figure}

\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma \vdash X : U_{i}}

    \boxed{\Gamma \vdash A : L_{i}}

    \inferrule{~}{\Gamma \vdash U_i : U_{i+1}}
 %
    \and
%
    \inferrule{~}{\Gamma \vdash L_i : U_{i+1}}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \PiTy x X Y : U_i }%
%
    \and
%
    \inferrule{\Gamma\vdash X : U_i \\ \hspace{-0.1cm} \Gamma, x : X \vdash Y : U_i}{\Gamma \vdash \SigTy x X Y : U_i}
%
    \and
%
    \inferrule{~}{\Gamma \vdash 1 : U_i}
%
    % \and
%
    % \inferrule{\Gamma \vdash A : L_i}{\Gamma \vdash G A : U_i}

    \and
%
    \inferrule{~}{\Gamma \vdash I : L_i}
 %
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \otimes B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash A \lto B : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash A : L_i \\ \hspace{-0.1cm}\Gamma \vdash B : L_i}{\Gamma \vdash B \tol A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinPiTy x X A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \\ \Gamma, x : X \vdash A : L_i}{\Gamma \vdash \LinSigTy x X A : L_i}
%
    \and
%
    \inferrule{\Gamma \vdash X : U_i \quad \{\Gamma \vdash e_i : X\}_i}{\Gamma \vdash e_1 =_X e_2 : U_i}
    %
    \and
    %
    \inferrule{~}{\Gamma \vdash \top : L_i}
%
    \and

    \inferrule{~}{\Gamma \vdash \bot : L_i}

    \and
    \inferrule{c \in \Sigma}{\Gamma \vdash c : L_0}
    %
    \and
    %
    \inferrule{\Gamma, x : L_i \vdash A : L_i \and A \textrm{ strictly positive}}{\Gamma \vdash \mu x.\, A : L_i}
    %
    \and
    %
    \inferrule
    {\Gamma \vdash A : L_i}
    {\Gamma \vdash \ltonl { A } : U_i}
    %
    \and
    \inferrule
    {\Gamma ; \Delta \vdash f : A \\ \Gamma \vdash g : \ltonl { (A \lto B) }}
    {\Gamma ; \Delta \vdash \equalizer {a}{f}{g} : L_i}
  \end{mathpar}
  \caption{Type well-formedness}
  \label{fig:typewf}
\end{figure}

\begin{figure}
  \begin{mathpar}
  \boxed{\Gamma \vdash x : X}

  \inferrule{~}{\Gamma, x : X, \Gamma' \vdash x : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : Y \quad \ctxwffjdg \Gamma {X \equiv Y}}{\Gamma \vdash e : X}
  %
  \and
  %
  \inferrule{~}{\Gamma \vdash () : 1}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : X \\ \Gamma \vdash e : \subst Y e x}{\Gamma \vdash (e, e') : \SigTy x X Y}
  %
  \and
%
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_1\, e : X}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \SigTy x X Y}{\Gamma \vdash \pi_2\, e : \subst Y {\pi_1\, e} x}
  \and
  \inferrule{\Gamma, x : X \vdash e : Y}{\Gamma \vdash \lamb x e : \PiTy x X Y}
  %
  \and
  %
  \inferrule{\Gamma \vdash e : \PiTy x X Y \\ \Gamma \vdash e' : X}{\Gamma \vdash \app e {e'} : \subst Y {e'} x}
  %
  \and
  %
  \inferrule{\Gamma \vdash e \equiv e' : X}{\Gamma \vdash \mathsf{refl} : e =_X e'}
  \and
  \inferrule{\Gamma ; \cdot \vdash e : A}{\Gamma \vdash e : \ltonl A}

  \textrm{TODO: inductive types (boolean, nat) with their eliminators allowed in }
  \end{mathpar}
  \caption{Intuitionistic typing}
  \label{fig:inttyping}
\end{figure}


\begin{figure}
  \begin{mathpar}
    \boxed{\Gamma \vdash e \equiv e' : X}

    \inferrule{\Gamma \vdash p : e =_X e'}{\Gamma \vdash e \equiv e' : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \app {(\lamb x e)} {e'} \equiv \subst x e {e'} : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash e \equiv \lamb x {\app e x} : \PiTy x X Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_1\, (e_1, e_2) \equiv e_1 : X}
%
    \and
%
    \inferrule{~}{\Gamma \vdash \pi_2\, (e_1, e_2) \equiv e_2 : \subst x {e_1} Y}
%
    \and
%
    \inferrule{~}{\Gamma \vdash e \equiv (\pi_1\, e, \pi_2\, e) : \SigTy x X Y}
%
    \and
%
    \inferrule{~}{}
%
    \inferrule{~}{\Gamma \vdash t \equiv t' : 1}
%
    % \and
%
    % \inferrule{~}{\Gamma \vdash G\, (G^{-1} \, t) \equiv t : G A}

    \\

    \boxed{\Gamma ; \Delta \vdash a \equiv a' : A}

    % \inferrule{~}{\Gamma; \cdot \vdash G^{-1}\, (G \, t ) \equiv t: A}
%
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lamblto a e)} {e'} \equiv \subst e x {e'} : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lamblto a {\app e a} : A \lto B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\lambtol a e)} {e'} \equiv \subst e x {e'} : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \lambtol a {\app e a} : A \tol B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \app {(\dlamb x e)} {e'} \equiv \subst x a {e'} : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv \dlamb x {\app e x} : \LinPiTy x X A}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv e' : \top}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e_i \equiv \pi_i (e_1, e_2) : A_i}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash e \equiv (\pi_1 e, \pi_2 e) : A\& B}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} {()} e \equiv e : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {()} e {\subst {e'} {()} a} \equiv \subst {e'} a e : C}
%
    \and
%
    \inferrule{~}{\Gamma; \Delta \vdash \letin {e \otimes e'} {a \otimes a'} e'' \equiv \subst {e''} {a, a'} {e, e'} : C}
%
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {a \otimes b} e {\subst {e'} {a \otimes b} c} \equiv \subst {e'} c e : C}
%
    \and
%
    \inferrule{~}{\Gamma;\Delta \vdash \letin {(x, a)} {(e, e')} {e''} \equiv \subst {e''} {x, a} {e, e'} : C}
    %
    \and
    %
    \inferrule{~}{\Gamma; \Delta \vdash \letin {(x, a)} e {\subst {e'} {(x, a)} y} \equiv \subst {e'} e y : C}
    \and
    %
    \inferrule
    {\Gamma \vdash f : \ltonl { (A \lto B) } \\
      \Gamma \vdash g : \ltonl { (A \lto B) } \\
      \Gamma ; \Delta \vdash a : A \\
      \Gamma ; \Delta \vdash p : \apptol {f}{a} \equiv \apptol {g}{a}}
    {\Gamma ; \Delta \vdash \equalizerpi {\equalizerpair {a}{p}} \equiv a : A}
    %
    \and
    %
    \inferrule
    {\Gamma \vdash f : \ltonl { (A \lto B) } \\
      \Gamma \vdash g : \ltonl { (A \lto B) } \\
      \Gamma ; \Delta \vdash p : \apptol {f}{a} = \apptol {g}{a} \\
      \Gamma ; \Delta \vdash e : \equalizer{a}{f}{g}}
    {\Gamma ; \Delta \vdash \equalizerpair {\equalizerpi {e}} {p} \equiv e : A}
\end{mathpar}
  \caption{Judgmental equality}
  \label{fig:jdgeq}
\end{figure}
\steven{Do we need inverse for $\ltonl A$ in judgemental equality figure?}
\steven{Do we need to say anything about the equalizer proofs being unique in
  judgemental equality figure?}

\subsection{Formal Grammars as Linear Types}
\label{sec:formaltype}

The linear typing judgment in our syntax takes on the following schematic form
$\Gamma ; \Delta \vdash a : A$. First, $A$ represents a \emph{linear type} in
our syntax. The intended semantics of these linear types are formal grammars.
That is, the linear typing system is designed to syntactically reflect the
behavior of formal grammars. For this reason, we may often interchangeably use
the terms ``linear type'' and ``grammar''.

The term $a$ is an inhabitant of type $A$, which is thought of as a parse tree of
the grammar $A$. The core idea of this entire paper follows precisely from this
single correspondence: grammars are types, and the inhabitants of these types
are parse trees for the grammars.

$\Gamma$ represents a non-linear context, while $\Delta$ represents a
\emph{linear context} dependent on $\Gamma$. These linear contexts behave
substructurally. As stated earlier, they are linear --- they do not obey
weakening or contraction --- because a character is exhausted once read by a
parsing procedure. Moreover, the characters in strings appear in the order in
which we read them. We do not have the freedom to freely permute characters,
therefore any type theory that is used to reason about formal grammars ought to
omit the structural rule of exchange as well. This means that every variable
within $\Delta$ must be used \emph{exactly once} and \emph{in order of
  occurrence}. Thus, we can think of the linear contexts as an ordered list of
limited resources. Once a resource is consumed, we cannot make reference to it
again. Variables in a linear context then act like building
blocks for constructing patterns over strings.

We give the base types and type constructors for linear terms. As the
interpretation of types as grammars in $\Grammar$ serves as our intended
semantics, we simultaneously give the interpretation $\sem \cdot$ of the
semantics as grammars.

\pedro{The paragraphs above are a bit circular, what about the following alternative?}
\steven{I am in favor of cutting what's above, but we haven't introduced the
  syntax in the main body of the paper yet, even if the figures appeared
  earlier. So what you write needs to take that into account}
\pedro{In which case we should introduce it in the previous section. I suggest
  adding this right after ``and the intuitionistic typing rules in fig 3''}

Now that we have specified the syntactic aspect of \theoryabbv, we must justify
its connections to formal grammars in order to show that it is a sound methodology
for reasoning about parsers. We do this by showing that $\mathbf{Gr}$ is a model
to our type theory.

Most of the non-linear semantics is standard \pedro{We should either cite something or add the non-linear semantics in the appendix}, so let us investigate in more
depth the linear semantics.

\begin{enumerate}
  \item A non-linear context $\Gamma$ denotes a set $\sem \Gamma$
  \item A non-linear type $\Gamma \vdash X : U_{i}$ denotes a family of sets
        $\sem X : \sem \Gamma \to \Set_{i}$
  \item A non-linear term $\Gamma \vdash e : X$ denotes a section
        $\sem e : \Pi(\gamma : \sem \Gamma)\sem{X} \gamma$ \pedro{we have not defined what sections are}
  \item Linear contexts $\Gamma \vdash \Delta$ and types
        $\Gamma \vdash A : L_{i}$ both denote families of grammars
        $\sem \Gamma \to \Gr_{i}$
  \item A linear term $\Gamma ; \Delta \vdash M : A$ denotes a family of parse
        transformers
        $\sem M : \Pi(\gamma : \sem \Gamma)\Pi(w : \String) \sem \Delta \gamma w \Rightarrow \sem M \gamma w$
\end{enumerate}

\paragraph{Linear Unit}
First, the linear unit $I$ may be built in the empty linear
context\footnote{When appropriate, statements like ``the empty linear context''
  may be taken polymorphically over all non-linear contexts that they might
  depend on.} $\cdot \dashv I$. $I$ serves as the unit for the operation
$\otimes$ described below.

As a grammar,
%
\[
  \sem {I} \gamma w = \{ () ~|~ w = \varepsilon \}
\]
%
That is, $I$ maps strings to the proposition that they are the empty string. $I$
is clearly only inhabited by the empty string $\varepsilon$, for which the
outputted set contains a single parse tree.


\paragraph{Base Types}
For each character $c$ in the alphabet $\Sigma$, we include a base type at the
lowest universe level $c : L_{0}$.

The grammar interpretation for characters is quite similar to that of $I$.
%
\[
  \sem {c} \gamma w = \{ () ~|~ w = c \}
\]
%
The grammar for the character $c$ likewise maps strings $w$ to the proposition that
they are equal to $c$.

\paragraph{Tensor Product}
The tensor product of two linear types is the first place where the ordering on
contexts really takes effect. That is, the tensor product of two types is
non-commutative.
When context $\Delta$ forms a linear term of
type $A$, and $\Delta'$ forms a linear term of type $B$, then the context
extension $\Delta, \Delta'$ forms a linear term of type $A \otimes B$.

As stated above, the type $I$ serves as the unit for $\otimes$. That is, for all
linear types $A$, the following equalities hold:
%
\[ I \otimes A \equiv A \otimes I \equiv A \]
%
In the grammar semantics,
%
\[
  \sem {A \otimes B} = \Sigma_{w_{1}, w_{2} : \String} (w_{1}w_{2} = w) \land \left(  \sem {A} \gamma w_{1} \times \sem {B} \gamma w_{2} \right)
\]
%
The string $w$ matches $A \otimes B$
precisely when it may be split into two
pieces such that the left one matches $A$
and the right one matches $B$.

\paragraph{Linear Function Types}
The monoidal structure provided by $\otimes$ is both left and right closed and
this is denoted by the left and right linear function types. Further,
for linear types $A$ and $B$ the left linear function type $A \lto B$
enjoys an elimination principle similar to function application or modus ponens.
%
\[
\inferrule{\Delta \vdash A \otimes A \lto B}{\Delta \vdash B}
\]
%
The interpretation of linear function types at a string $w$ is a linear function
that takes in parses of $A$ on a string $w_{a}$ and outputs parses of $B$ on the
string $w_{a} w$.
%
\[
  \sem{A \lto B} \gamma w = \Pi(w_a:\String) \left( A \gamma w_a \Rightarrow B\gamma (w_a w) \right)
\]
%
That is, strings match $A \lto B$ if when prepended with a parse of $A$ they
complete to parses of $B$. In this manner, the linear function types generalize
Brzozowksi's notion of derivative
\cite{brzozowskiDerivativesRegularExpressions1964}.
Brzozowski initially only gave an accounting of this operation for
generalized regular expressions, but later work from Might et al.\ demonstrates that the same
construction can be generalized to context free grammars
\cite{mightParsingDerivativesFunctional2011}. Here, via the linear function
types, the same notion of derivative extends to the grammars of \theoryabbv.

Note, to ensure that the linear function types do indeed generalize Brzozowski
derivatives, we must include the equalities in \cref{fig:brzozowskideriv} as axioms\max{we must? why must we}.
\pedro{We should further explain why these equations are required, or at the very
least give some intuition}

Of course, all the above statements for the left function type also have
corresponding analogues for the
right-handed counterpart.

\begin{figure}
\begin{align*}
  c\lto c &\equiv I\\
  c\lto d &\equiv \bot\\
  c\lto I &\equiv \bot\\
  c\lto 0 &\equiv \bot\\
  c\lto (A \otimes B) & \equiv (c\lto A) \otimes B + (A \amp I) \otimes (c\lto B)\\
  c\lto A^* &\equiv (A \amp I)^{*} \otimes (c \lto A) \otimes A^* \\
  c\lto (A + B) &\equiv (c\lto A) + (c\lto B)
\end{align*}
\caption{Equality for Brzozowski Derivatives}
\label{fig:brzozowskideriv}
\end{figure}

\paragraph{LNL Dependent Types}
Given a non-linear type $X$, we may form both the dependent product type
$\LinPiTy x X A$ and the dependent pair type $\LinSigTy x X A$ as linear types
where $x$ is free in $A$.

$\sem{\LinPiTy x X A} \gamma w = \Pi(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

$\sem{\LinSigTy x X A} \gamma w = \Sigma(x:\sem{X}\gamma) \sem{A}(\gamma,x) w$

The grammar semantics of the linear product type is
indeed a dependent function out of the
semantics of $X$. Likewise, the grammar semantics of the linear dependent pair type is a dependent
pair in $\Set$.\pedro{The connection to grammar semantics needs to be better explained. For instance,
we should explain that disjunction corresponds to ``or'' of grammars}

Note that even though we do not take the binary additive conjunction and
disjunction as primitive, we may define them via these LNL dependent types. In
particular, via a dependence on $\Bool$.
%
\[
  A \amp B := \LinPiTy b \Bool {C(b)}
\]
\[
  A \oplus B := \LinSigTy b \Bool {C(b)},
\]
where $C(\true) = A, C(\false) = B$.

\paragraph{Universal Type}
The universal type $\top$ may be formed in any context.
%
\[ \sem{\top} \gamma w = \{ \ast \}\]
%
Its grammar semantics in set outputs the unit type in $\Set$ for all input strings in all contexts.

\paragraph{Empty Type}
The empty type $\bot$ has no inhabitants. The elimination for the empty type
witnesses the principle of explosion --- i.e.\ from a term of type $\bot$ we may
introduce a term $\mathsf{absurd}_{A} : A$ for any type $A$.
%
\[ \sem {\bot} \gamma w = \emptyset \]
%
\paragraph{The $G$ Modality}
The $G$ modality provides the embedding of linear types in the empty context
into non-linear types. The introduction and elimination forms for $G A$ obey the
same rules as given in \cite{bentonMixedLinearNonlinear1995} and \cite{krishnaswami_integrating_2015}.

The left adjoint $F$ from non-linear types back to linear types may be defined
as,
%
\[
  F X := \LinSigTy a A I
\]
%
\steven{Elaborate on $G$ more and rename $G A$to $\ltonl A$ in this section}

\pedro{Agreed :) maybe mention connections to persistent propositions from the
separation logic literature}

The semantics of $G A$ are exactly the semantics of $A$ at the empty word $\varepsilon$.
%
\[ \sem{G A} \gamma = \sem{A} \gamma \varepsilon \]
%
%
\paragraph{Recursive Types}
Recursive linear types may be defined via the least-fixed point operator $\mu$.
The grammar semantics of which are the fixed-point of sets induced by the
grammar semantics of $A$.
%
\[ \sem{\mu x. A} \gamma = \mu (x:\Gr_i). \sem{A}(\gamma,x) \]
%
\pedro{We should justify, even if briefly, the existence of this
  fixed-point}

Observe that we need not take the Kleene star as a primitive
grammar constructor, as it is definable as a fixed point.
The Kleene star of a grammar $g$ is given as,

\[
  g^{*} := \mu X . I \oplus (g \otimes X)
\]

\begin{figure}[h!]
\begin{mathpar}
  \inferrule
  {\Gamma ; \Delta \vdash p : I}
  {\Gamma ; \Delta \vdash \mathsf{nil} : g^{*}}

  \and

  \inferrule
  {\Gamma ; \Delta \vdash p : g \\ \Gamma ; \Delta' \vdash q
  : g^{*}}
  {\Gamma ; \Delta \vdash \mathsf{cons}(p , q) : g^{*}}

  \\

  \inferrule
  {
    \Gamma ; \Delta \vdash p : g^{*} \\
    \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
    \Gamma ; x : g , y : h \vdash p_{\ast} : h
  }
  {\Gamma ; \Delta \vdash \mathsf{foldr}(p_{\varepsilon} , p_{\ast}) : g^{*}}
\end{mathpar}
\caption{Kleene Star Rules}
\label{fig:star}
\end{figure}

Likewise, $g^{*}$ has admissible introduction and
elimination rules, shown in \cref{fig:star}. Note that this
definition of $g^{*}$ and these
rules arbitrarily assigns a handedness to the Kleene star.
We could have just as well took it to be a fixed point of
$I \oplus (X \otimes g)$. In fact, the definitions are
equivalent, as the existence of the $\mathsf{foldl}$ term below
shows that $g^{*}$ is also a fixed point of
$I \oplus (X \otimes g)$.
This is a marked point of difference from Kleene
algebra with recursion, where the fixed points for the left and right variants
of Kleene star need not agree \cite{leiss}.

\begin{equation}
  \label{eq:foldl}
  \inferrule
  {
    \Gamma ; \Delta \vdash p : g^{*} \\
    \Gamma ; \cdot \vdash p_{\varepsilon} : h \\
    \Gamma ; y : h , x : h \vdash p_{\ast} : h
  }
  {\Gamma ; \Delta \vdash \mathsf{foldl}(p_{\varepsilon} , p_{\ast}) : g^{*}}
\end{equation}

In fact, the $\mathsf{foldl}$ term is defined using
$\mathsf{foldr}$ --- much in the same way one
may define a left fold over lists in terms of a right fold
in a functional programming language.
The underlying trick is to fold over a list of linear functions
instead of the original string. We curry each character $c$
of the string into a function that concatenates $c$, and
right fold over this list of linear functions. Because function
application is left-associative, this results in a left
fold over the original string.

We only take fixed points of a single variable as a
primitive operation in the type theory, but we may apply
Beki\`c's theorem \cite{Bekić1984} to define an admissible
notion of multivariate fixed point. This is particularly
useful for defining grammars that encode the states of an
automaton, as we will see in \cref{sec:automata}. In \cref{fig:multifix} we provide the
introduction and elimination principles for such a fixed
point, where $\sigma$ is the substitution that unrolls the
mutually recursive definitions one level. That is,

\begin{align*}
  \sigma = \{ & \mu(X_{1} = A_{1} \dots, X_{n} = A_{n}).X_{1} / X_{1} , \dots, \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{n} / X_{n} \}
\end{align*}

\begin{figure}
\begin{mathpar}
  \inferrule
  {\Gamma ; \Delta \vdash e : \simulsubst {A_{k}} {\sigma}}
  {\Gamma ; \Delta \vdash \mathsf{cons}~e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k}}

  \and

  \inferrule
  {\Gamma ; \Delta \vdash e : \mu(X_{1} = A_{1}, \dots, X_{n} = A_{n}).X_{k} \\
             \Gamma ; x_{j} : \simulsubst {A_{j}}{\sigma} \vdash e_{j} : B_{j}\quad \forall j
  }
  {\Gamma; \Delta \vdash \mathsf{mfold}(x_{1}.e_{1}, \dots, x_{n}.e_{n})(e) : B_{k}}
\end{mathpar}
\caption{Multi-fixed Points}
\label{fig:multifix}
\end{figure}

\subsection{Subtheories}
Each choice of connective has direct implications on the expressivity of
\theoryabbv. In particular, the set of connectives used will determine where the
grammar semantics is placed in
the Chomsky hierarchy.

\paragraph{Regular Expressions} We may realize regular expression as a subtheory
of our language by restricting the type constructors to the
linear unit, characters, $\otimes$, $\oplus$, and Kleene star. Classically, the
languages recognized by these are exactly the regular languages --- the lowest
on the Chomsky hierarchy.

\paragraph{$\mu$-Regular Expressions} Similarly, we may restrict the connectives
to be linear unit, characters, $\otimes$, $\oplus$, and arbitrary recursion via
left-fixed point rather than only Kleene star.
Instead of regular expressions these correspond to Lei{\ss}'s $\mu$-regular
expressions, which are known to be equivalent to context-free grammars
\cite{leiss,krishnaswami_typed_2019}.

\paragraph{Beyond Context-Free Grammars} The previous two subtheories induce as semantics
regular grammars and context-free grammars, respectively; however, by including
the LNL dependent types we may actually express the entirety of the Chomsky
hierarchy. Through use of the LNL dependent types, we may encode indexed
grammars, which are known to be properly between context-free and
context-sensitive grammars within the Chomsky hierarchy \cite{AhoIndexed}. We
will further see in \cref{subsubsec:tm} how to use dependence to encode Turing machines internal to our
calculus, which of course induces a semantics of unrestricted grammars.

\section{Formal Grammar Theory in \theoryname}
\label{sec:applications}

\steven{below this is unedited word vomit}
\steven{this entire ``Formal Grammar Theory in Lambek'' section should become much more compact with the examples
  section from the beginning in mind}

\subsection{Grammar Properties}
\steven{Mention that $\top$ and $\String$ are iso. Highlight this as a needed axiom}

%% 1. Basic concepts: unambiguity, parseability, weak and strong equivalence
\paragraph{Unambiguity}
In the grammars model, we say that a grammar is unambiguous if there is at most
one parse tree for any input string. There are several ways to capture this
concept internally.

In the presence of distributive coproducts, the following are equivalent:
\begin{itemize}
  \item $A$ is an unambiguous grammar
  \item The morphism $! : A \to \top$ is a monomorphism
  \item Given a grammar $B$, all morphisms $B \to A$ are equal
  \item $A \& A \cong A$
\end{itemize}

\paragraph{Parseability}
We say $A$ is \emph{weakly parseable} if there exists a type $B$ and a map
$\top \to A \oplus B$. Because $\top \cong \String$, this says that any string
can be parsed into either an $A$ or a $B$.

If $\top \cong A \oplus B$, call $A$ \emph{parseable}. This notion of
parseability implies that $A$ and $B$ are each unambiguous.

\paragraph{Decidability}
In the case that $A$ is parseable with respect to $\neg A$ --- i.e.
$\top \cong A \oplus \neg A$ --- then call $A$ \emph{decidable}.

\paragraph{Equivalence}
Grammars $A$ and $B$ are \emph{weakly equivalent} if there exists maps
$f : A \to B$ and $g : B \to A$.

If $f$ and $g$ form an isomorphsim, then $A$ and $B$ are \emph{strongly
  equivalent}.
\steven{TODO cite that these are the same as Chomsky's notions of equivalence}


\subsection{Regular Expressions and Finite Automata}
%% 2. Regex, NFA, DFA

\subsection{The Dyck Grammar}
%% 3. Dyck language
The Dyck grammar $D$ for balanced parens (summarize what it is + citation)


\steven{this is a VERY loose sketch of the things I want to say about $D$}
\Cref{fig:DyckAutomaton} presents an automaton $M$ that recognizes $D$. We show this
precisely by constructing an isomorphism between $D$ and $\Trace_{M}~0~\true$.
Unlike the finite automata seen thus far, $M$ has infinitely many states --- one
for each $n : \mathbb{N}$ with an additional fail state. This diverges a bit
from classical language theory, where this machine would be encoded as a
pushdown automaton with finitely many
states and stack of memory. Such PDAs are encodable in our theory, although they
are a little cumbersome to use. Moreover, they
seem overfit to classical reasoning as they don't preserve the gramamr up to
strong equivalence, only weak equivalnce in general. \steven{cite lambek
  automata and state that is is a secondary goal to preserve as much structure as possible}
By moving the stack of memory (the nats) into the
state, we obtain infinitely many states, which is fine bc the states need only
be a type for us to interface with it.

\steven{TODO sketch proof of equivalence. Mention that it is mechanized}

\begin{figure}
  \begin{tikzpicture}[node distance = 17mm ]
    \node[state, initial, accepting] (0) {$0$};
    \node[state, right of=0] (1) {$1$};
    \node[state, right of=1] (2) {$2$};
    \node[right of=2] (3) {$\dots$};
    \node[state, below of=0] (fail) {$\mathsf{fail}$};

    \path[->] (0) edge[above, bend left] node{\stringquote{(}} (1)
              (1) edge[below, bend left] node{\stringquote{)}} (0)
              (1) edge[above, bend left] node{\stringquote{(}} (2)
              (2) edge[below, bend left] node{\stringquote{)}} (1)
              (2) edge[above, bend left] node{\stringquote{(}} (3)
              (3) edge[below, bend left] node{\stringquote{)}} (2)
              (fail) edge[loop right] node{\stringquote{(}, \stringquote{)}} (fail)
              (0) edge[left] node{\stringquote{)}} (fail);
  \end{tikzpicture}
  \caption{Automaton $M$ for the Dyck grammar}
  \label{fig:DyckAutomaton}
\end{figure}

\subsection{LL(1)}
%% 4. LL(1) parser

\steven{TODO impute LL(1) example, mention that its weak equivalence is mechanized}

\subsection{UnReStRiCtEd}
%% 5. Expressiveness: Turing Machines

%% Make sure to say which results are mechanized

\subsection{Semantic Actions}
\steven{TODO edit this. synthesize it with the later presentation of sematnic
  actions as models, and then remove the model section}
All constructions discussed until now detail how a string may be parsed into a
\emph{concrete} syntax tree; however, this is often not the representation that later
compiler passes would take in. Instead, they often take in an \emph{abstract}
syntax tree that forgets syntactic details that are superfluous to the later
passes. For example, after using parentheses to disambiguate a parse of a string
into a concrete syntax tree, we may remove these parentheses to have a cleaner
structured representation of the underlying data. The procedure that takes in a
concrete syntax tree and returns an abstract one is referred to as a
\emph{semantic action}.

Given a linear type $A$, the inhaitants of $A$ serve as concrete syntax. We may
then chose a non-linear type $X$ that serves as abstract syntax. A semantic
action can then be defined as $\square (A \lto \Delta X)$ where
$\Delta X = \LinSigTy x X \top$.
\steven{Note here that even though $\Delta$ is a map from nonlinear types into
  linear ones, it is very different from $F$/whatever we call that half of the adjunction}

\steven{Everything in this section below this marker is outdated and needs to be
folded into the above or cut}

\steven{Need a different segue because canonicity is no longer before this}
The canonicity theorem from the previous section gives a tight connection
between terms in our type theory and the theory of formal languages. In this section
we further explore these connections by using the fact that, classically
formal language theory is closely related to the study of automata.
In the Chomsky hierarchy, each language class is associated to a class of
automata that serve as recognizers. Internal to \theoryabbv,
we can characterize these language classes syntactically; moreover, we
demonstrate the equivalence of these language classes to its associated automata class as a
proof term within our logic.

\subsection{Non-deterministic Finite Automata}
\label{subsec:finiteaut}
\begin{figure}
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial, accepting] (1) {$1$};
    \node[state, below left of=1] (2) {$2$};
    \node[state, right of=2] (3) {$3$};

    \path[->] (1) edge[above] node{$b$} (2)
              (1) edge[below, bend right, left=0.3] node{$\epsilon$} (3)
              (2) edge[loop left] node{$a$} (2)
              (2) edge[below] node{$a, b$} (3)
              (3) edge[above, bend right, right=0.3] node{$a$} (1);
  \end{tikzpicture}
  \caption{An example NFA}
  \label{fig:NFA}
\end{figure}

Classically, a \emph{nondeterministic finite automaton} (NFA) is a finite state machine where
transitions are labeled with characters from a fixed alphabet $\Sigma$. These
are often represented formally as a 5-tuple $(Q, \Sigma, \delta, q_{0}, F)$,

\begin{itemize}
  \item $Q$ a finite set of states
  \item $\Sigma$ a fixed, finite alphabet
  \item $\delta : Q \times (\Sigma \cup \{ \varepsilon\}) \to \mathcal{P}(Q)$ the labeled transition function
  \item $q_{0} \in Q$ the start state
  \item $F \subset Q$ a set of accepting states
\end{itemize}



\subsection{A DFA Parser}
\label{subsec:regexparser}

Just as we encoded traces of NFAs as grammars, we likewise
encode the traces of a DFA as grammars. The key difference
between NFAs and DFAs is \emph{determinism} --- meaning,
that in a state $q$ inside of DFA, given a character $c$ there
will be exactly one transition that we may take leaving $q$
with label $c$. For us, this changes the definition of valid
transitions for a DFA, instead of the definition of
$\mathsf{Trans}$ provided in \cref{subsec:finiteaut} DFAs
obey

\begin{gather*}
 \mathsf{State} \in \{ g_{q} : q \in Q \} \\
 \mathsf{Trans}(q) ::= \bigoplus_{c \in \Sigma} (c \otimes \mathsf{State})
\end{gather*}

Meaning, each state has a transition for every character. \emph{Note:} the above use
  of a disjunction over $\Sigma$ is a bit of abuse of notation.
  As $\Sigma$ is a finite alphabet, we wish to think of this as an iterated
  binary sum, or perhaps as a finitely indexed sum defined via linear dependent
  sums in the same manner by which we defined binary sums.

We now wish to define a parser term for DFA grammars. In
particular, for a DFA $D$ we want to build a term,

\[
  w : \String \vdash \mathsf{parse}_{D} : \mathsf{AccTrace}_{D} \oplus \top
\]

where left injection into the output type denotes acceptance
by the parser, and right injection denotes rejection. To
build such a parser, it will suffice to construct a term

\[
  w : \String \vdash \mathsf{parse}_{D} : \LinSigTy q Q {\mathsf{Trace}_{D}(q_{0} , q)}
\]
This is because given a trace of a DFA, we may easily check
if we should accept or reject by simply testing
if the final state is accepting.

Because $w$ is a Kleene star of characters, we may construct
our desired $\mathsf{parse}_{D}$ term as a $\mathsf{foldl}$
over $w$. In the empty case, we just have the trace that
ends at the accepting state. In the recursive case, we
effectively add to our trace by transitioning one character
at a time, as we read them moving across $w$.

Perhaps this derivation is not too surprising. All it says
is that a DFA may be ran by transitioning a single character
at a time, and then accepting or rejecting based on the
final state. This is exactly what DFAs did initially, so
what did we gain? Well, this has the benefit of our type
system to ensure its correctness. Moreover, this construction exports to an
intrinsically verified and executable decision procedure for DFAs in \theoryabbv
embedded in Agda.

We should note that the
construction of a DFA parser requires the addition of an additional but seemingly innocuous
axiom.

\[
  \top \cong \left( \bigoplus_{c : \Sigma} c \right)^{*}
\]

That is, the characters in the alphabet $\Sigma$ really are all of the
characters. The need for this axiom arises when we go to take a transition
within the DFA,
as we must ensure that the next character of the string has a corresponding
transition from the current state
--- which would be guaranteed by determinism provided there are no surprise characters.

\subsection{Regular Expressions and DFAs}
\label{subsec:deriv}

In order to extend the DFA
parser from \cref{subsec:regexparser} to the construction of
a verified parser
generator for regular expression we need to perform some
plumbing establishing an equivalence between regular
expressions and DFAs.

There are several routes we may hope to take in establishing
this equivalence. First, we could prove an equivalence
between NFAs and regular expressions, and separately prove
an equivalence between NFAs and DFAs.
In \cref{subsec:eqproofs}, we include a version of
Thompson's construction --- which established the
equivalence between regular grammars and DFAs. We may additionally
hope to internalize a variant of the powerset construction \cite{rabinFiniteAutomataTheir1959}
--- which takes as input an NFA and constructs a DFA that
recognizes the same language --- and combine the results of
Thompson's construction and the powerset constructions to give an equivalence
between regular expressions and DFAs.  This route is alluring, as it
internalizes several classic grammar-theoretic constructions. However, it may
necessitate extensions to the LNL theory, like
a propositional truncation, and we have not yet investigated
how this would interact with the existing types in the
theory. The addition of a propositional truncation may seem
harmless, but it is not always immediately clear how
distinct constructions will interact. For instance, when
exploring LNL models, Benton discovered that the synthesis
of linear and dependent types require a new presentation of
the $!$ modality from linear logic
\cite{bentonMixedLinearNonlinear1995}. That is all to say,
this is a work in progress and
it is not immediate that the addition of a propositional
truncation is adequate for establishing the weak equivalence
between NFAs and DFAs.

We may instead hope to internalize an equivalence between
regular grammars and DFAs by using Brzozowski derivatives to
directly create a DFA that is weakly equivalent to a given
regular expression, as described by Owens et al.
\cite{owensRegularexpressionDerivativesReexamined2009}.
One characterization of regular grammars is that they are
precisely those grammars which have finitely many inequivalent Brzozowski
derivatives
\cite{brzozowskiDerivativesRegularExpressions1964}.
The algorithm used by Owens takes in a
regular grammar and generates a DFA that recognizes the same
language, and the states in this DFA are the finitely many
derivative equivalence classes. We initially had a version of
this theorem very roughly internalized in the LNL theory.
To our taste, too much of this presentation relied on
meta-arguments that lived outside of
our formalism, and thus this particular phrasing of the
theorem did not translate well into formalization.

In any case, we believe
that revisiting these lines of thought will lead to a
satisfactory internalization of the equivalence between
regular grammars and DFAs, and thus would bridge the gap
between our DFA parser and a full regular expression parser.

\subsection{Equivalence Between Regular Expressions and Finite Automata}
\label{subsec:eqproofs}
In this section, we describe a version of Thompson's
construction \cite{thompsonProgrammingTechniquesRegular1968}
   where we construct an NFA that recognizes a given regular
expression. Moreover, we will show that this NFA is strongly equivalent to the
original grammar. Witnessing this construction in our syntax has two benefits
\begin{enumerate}
  \item It reinforces this high-level view that the syntax is a natural and
        general setting for formal grammar reasoning, as we demonstrate that
        this formal system subsume results from existing systems, and
  \item Following the
        development of Thompson's construction, we then need
        only establish the equivalence of NFAs and DFAs to
        complete the full regular expression parser
\end{enumerate}

\begin{theorem}[Thompson]
  \label{thm:thompson}
  For $g$ a regular grammar $g$, there is an NFA $N$ that recognizes the same
  language as $g$.
\end{theorem}
We make a pretty straightforward adaptation of Thompson's theorem to our setting,

\begin{theorem}[Typed Thompson]
  \label{thm:typthompson}
  For $g$ a regular expression $g$, there is an NFA $N$ such that $g$ is isomorphic
  to $\mathsf{AccTrace}_{N}$.
\end{theorem}

\begin{proof}[Proof Sketch]
  Recall that regular expressions are inductively defined via
  disjunction, concatenation, and Kleene star over literals
  and the empty grammar. By induction over regular expressions,
  we will construct an NFA that is equivalent to $g$.

  First, define the recognizing NFA for the empty grammar
  $I$.

  \begin{figure}[h!]
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial] (1) {$1$};
    \node[state, right of=1, accepting] (2) {$2$};

    \path[->] (1) edge[below] node{$\varepsilon$} (2);
  \end{tikzpicture}
  \caption{$NFA(I)$}
  \label{fig:emptyNFA}
  \end{figure}

  The type of traces from the initial state of $NFA(I)$ to the single
  accepting state is given by,

  \[
    \mathsf{Trace}_{NFA(I)}(q_{1}, q_{2}) = \mu
      \begin{pmatrix}
         g_{q_{1}} := g_{q_{2}} \\
         g_{q_{2}} := I
      \end{pmatrix}. g_{q_{1}}
  \]

  The accepting traces through $NFA(I)$ are then described
  as,

  \[
    \mathsf{AccTrace}_{NFA(I)} = \LinSigTy q {\{1 , 2\}} {\left( \mathsf{Trace}_{NFA(I)}(q_{1} , q) \pair \mathsf{acc}(q) \right)}
  \]

  A quick inspection of \cref{fig:emptyNFA} reveals that the
  only reasonable choice for $q$ is $q_{2}$ --- because
  state 2 is accepting while state 1 is not. Therefore,

  \begin{align*}
    \mathsf{AccTrace}_{NFA(I)}
    & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \pair \mathsf{acc}(q_{2}) \\
    & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \pair \top \\
    & \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})
  \end{align*}

  From here, to prove
  $I \cong \mathsf{AccTrace}_{NFA(I)}$ it suffices to show
  $I \cong \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})$. Below we
  give two parse transformers, one from $I$ to
  $\mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})$ and vice versa.

  \[
    \inferrule
    {p : I \vdash \mathsf{nil} : \mathsf{Trace}(q_{2} , q_{2}) \\
     \exists \text{~transition~} q_{1} \overset{\varepsilon}{\to} q_{2}
    }
    {p : I \vdash \mathsf{\varepsilon cons}(\mathsf{nil}) : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2})}
  \]

  Let $\gamma$ be the substitution $\{ g_{q_{2}} / g_{q_{1}}, I / g_{q_{2}} \}$,

  \[
    \inferrule
    {
      p : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \vdash p :
        \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \\
      x_{1} : \simulsubst {g_{q_{2}}} {\gamma} = I \vdash x_{1} : I \\
      x_{2} : \simulsubst {I} {\gamma} = I \vdash x_{2} : I
    }
    {
      p : \mathsf{Trace}_{NFA(I)}(q_{1} , q_{2}) \vdash \mathsf{mfold}(x_{1}.x_{1} , x_{2}.x_{2})(p) : I
    }
  \]

  This concludes the proof for the case of the empty
  grammar. Let's now walk through the construction for
  literal grammars. Given a character $c$, we construct an
  NFA that recognizes only the string containing the single
  character $c$ as,


  \begin{figure}[h!]
  \begin{tikzpicture}[node distance = 25mm ]
    \node[state, initial] (1) {$1$};
    \node[state, right of=1, accepting] (2) {$2$};

    \path[->] (1) edge[below] node{$c$} (2);
  \end{tikzpicture}
  \caption{$NFA(c)$}
  \label{fig:literalNFA}
  \end{figure}

  The automaton in \cref{fig:literalNFA} induces the
  following type of traces from $q_{1}$ to $q_{2}$.

  \[
    \mathsf{Trace}_{NFA(c)}(q_{1}, q_{2}) = \mu
      \begin{pmatrix}
         g_{q_{1}} := c \otimes g_{q_{2}} \\
         g_{q_{2}} := I
      \end{pmatrix}. g_{q_{1}}
  \]

  Through the same argument as the empty grammar, the only
  state that is accepting is $q_{2}$ and thus,

  \[
    \mathsf{AccTrace}_{NFA(c)} \cong \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2})
  \]

  To show the desired isomorphism of $c \cong NFA(c)$ we
  make a similar argument as we did for the empty grammar
  $I$, except we leverage the $\mathsf{cons}$ rule instead
  of $\mathsf{\varepsilon cons}$. That is, the parse
  transformers in either direction are given as,

  \[
    \inferrule
    {\cdot \vdash \mathsf{nil} : \mathsf{Trace}(q_{2} , q_{2}) \\
     \exists \text{~transition~} q_{1} \overset{c}{\to} q_{2}
    }
    {p : c \vdash \mathsf{cons}(\mathsf{nil}) :
      \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2})}
  \]


  \[
    \inferrule
    {
      p : \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \vdash p :
        \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \\
      x_{1} : \simulsubst {(c \otimes g_{q_{2}})} {\gamma} = c \otimes I \vdash \mathsf{unitR}(x_{1}) : c \\
      x_{2} : \simulsubst {I} {\gamma} = I \vdash x_{2} : I
    }
    {
      p : \mathsf{Trace}_{NFA(c)}(q_{1} , q_{2}) \vdash \mathsf{mfold}(x_{1}.\mathsf{unitR}(x_{1}) , x_{2}.x_{2})(p) : c
    }
  \]

  Where $\gamma$ is the substitution
  $\{ c \otimes g_{q_{2}} / g_{q_{1}}, I / g_{q_{2}} \}$ and
  $\mathsf{unitR}$ is a witness to the isomorphism
  $c \otimes I \cong c$. Again, we may see that these do
  indeed mutually invert each other in the Agda code.

  It remains to show that the desired isomorphisms are
  preserved by $\otimes$, $\oplus$, and Kleene star. Here,
  we will give the argument for the disjunction case, the
  others are defined quite similarly.

  Given two NFAs $N$ and $M$, we may define a new NFA that
  encodes the disjunction of $N$ and $M$. Denote the
  internal states of $N$ by $q_{j}$'s and the internal states
  of $M$ by $r_{k}$'s,

  \begin{figure}[h!]
  \begin{tikzpicture}[node distance = 20mm ]
    \node[state] (2) {$q_{init}$};
    \node[state, initial, below left of=2] (1) {$init$};
    \node[state, below right of=1] (3) {$r_{init}$};
    \node[right of=2] (4) {$\cdots$};
    \node[right of=3] (5) {$\cdots$};
    % \node[state, below right of=1] (3) {$3$};
    % \node[state, below of=2] (4) {$4$};

    \path[->] (1) edge[below] node{$\varepsilon$} (2);
    \path[->] (1) edge[below] node{$\varepsilon$} (3);
    \path[->] (2) edge[below] node{} (4);
    \path[->] (3) edge[below] node{} (5);

    \node[label={[name=l] $N$}, draw,line width=2pt,rounded corners=5pt, fit=(2)(4)] {};
    \node[label={[name=l] $M$}, draw,line width=2pt,rounded corners=5pt, fit=(3)(5)] {};
  \end{tikzpicture}
  \caption{$N \oplus_{NFA} M$}
  \label{fig:disjunctionNFA}
  \end{figure}

  \Cref{fig:disjunctionNFA} shows the process of
  disjunctively combining NFAs. Precisely, we add a single
  new state and we included copies of the states from each of $N$
  and $M$. The new state acts as the initial state and has
  $\varepsilon$-transitions to the initial states of $N$ and
  $M$. We include all of the internal transitions from $N$
  and $M$, and the accepting states of $N \oplus_{NFA} M$
  are exactly the accepting states in each subautomaton.

  Let $g$ and $g'$ be two regular grammars such that
  $g \cong NFA(g)$ and $g' \cong NFA(g')$. As a matter of
  notation\footnote{We shall similarly abuse notation for
    $\otimes$ and Kleene. That is, for a regular grammar
    $g$, when we write $NFA(g)$ we mean the NFA inductively
    built up with the NFA-analogues to the constructors that
    built up $g$.}, we will write $NFA(g \oplus g')$ for
  $NFA(g) \oplus_{NFA} NFA(g')$. The traces of
  $NFA(g \oplus g')$ are then given by,

  \[
    \mathsf{Trace}_{NFA(g \oplus g')}(src , dst) = \mu
      \begin{pmatrix}
        g_{init} := g_{q_{0}} \oplus g_{r_{0}} \\
        g_{q_{j}} := \mathsf{Trans}_{NFA(g)}(q_{j}) \oplus \mathsf{isDst}(q_{j}) \\
        g_{r_{k}} := \mathsf{Trans}_{NFA(g')}(r_{k}) \oplus \mathsf{isDst}(r_{k})
      \end{pmatrix}.g_{src}
    \]

  where $\mathsf{Trans}$ is used to echo the same syntactic
  definitions that appear in the $NFA(g)$ and $NFA(g')$.
  Also, $src$ and $dst$ may take on any value in
  $Q := \{init\} \cup \{q_{j}\} \cup \{r_{k}\}$, and
  $\mathsf{isDst}(q)$ checks if $q$ is equal to $dst$. Which
  is all to say, the traces of $NFA(g \oplus g')$ comprise
  either a trace of $NFA(g)$, or a trace of $NFA(g')$, and
  the transition coming out of $g_{init}$ determines which
  subautomaton we step into.

  The parse transformer from $g \oplus g'$ checks which side
  of the sum type we are on, then takes the appropriate step
  from $g_{init}$ in the automaton.

  \[
    \inferrule
    {
      u : g \vdash \iota (\phi (u)) : \mathsf{AccTrace}_{NFA(g \oplus g')} \\
      v : g' \vdash \iota' (\psi (v)) : \mathsf{AccTrace}_{NFA(g \oplus g')}
    }
    {p : g \oplus g' \vdash \mathsf{case}~p \{ \mathsf{inl}(u) \mapsto s , \mathsf{inr}(v) \mapsto r \} : \mathsf{AccTrace}_{NFA(g \oplus g')}}
  \]

  with $\iota$ and $\iota'$ as embeddings from $NFA(g)$ and
  $NFA(g')$, respectively, into $NFA(g \oplus g')$,
  $\phi: g \cong \mathsf{AccTrace}_{NFA(g)}$, and $\psi: g' \cong \mathsf{AccTrace}_{NFA(g')}$. On a
  high level, all this construction does is turn a parse of
  $g$ into a parse of $NFA(g)$ and then embeds that inside
  of the larger automaton $NFA(g \oplus g')$. Likewise for $g'$.

  In the other direction, recall that the data of an
  accepting trace for $NFA(g \oplus g')$ is a pair of a
  trace and a proof that
  the end state $q'$ of that trace is accepting. By
  multifolding over the first part of that pair, we turn the
  term of type
  $\mathsf{Trace}_{NFA(g \oplus g')}(init , q')$ into a
  trace of either of the subautomata,

  \[
    p : \mathsf{Trace}_{NFA(g \oplus g')}(init , q') \vdash \mathsf{mfold}_{NFA(g \oplus g')} : \mathsf{Trace}_{NFA(g)}(q_{0} , q') \oplus \mathsf{Trace}_{NFA(g')}(r_{0} , q')
  \]

  Additionally, we leverage the fact that the only accepting
  states for $NFA(g \oplus g')$ are those from the
  subautomata to extract that $q'$ must be an accepting
  state from a subautomaton.

  \[
    x : \mathsf{acc}_{NFA(g \oplus g')}(q') \vdash M : \mathsf{acc}_{NFA(g)}(q') \oplus \mathsf{acc}_{NFA(g')}(q')
  \]

  We then combine the trace and proof of acceptance into an
  accepting trace of one of the subautomata,

  \[
    p : \mathsf{AccTrace}_{NFA(g \oplus g')} \vdash N : \mathsf{AccTrace}_{NFA(g)} \oplus \mathsf{AccTrace}_{NFA(g')}
  \]

  Lastly, we then inductively use the isomorphisms $\phi$
  and $\psi$ to turn the accepting traces into a parse of
  $g$ or $g'$,

  \[
     N : \mathsf{AccTrace}_{NFA(g)} \oplus \mathsf{AccTrace}_{NFA(g')} \vdash \mathsf{case}~N~\{\mathsf{inl}(n) \mapsto \phi^{-1}(n), \mathsf{inr}(n') \mapsto \psi^{-1}(n')\} : g \oplus g'
  \]

  When you wish to show that these actually maps invert each other to form an isomorphism\max{jumbled sentence}, we invoke the
  universal properties of the $\mathsf{Trace}$ types. That is,
  $\mathsf{Trace}_{\oplus NFA}$, $\mathsf{Trace}_{N}$, and $\mathsf{Trace}_{M}$
  are \emph{initial} algebras to the the functors that define them. Thus, the
  embedding of traces through $N$ into $\oplus NFA$ and then mapped back down to $N$
  compose to a map from $\mathsf{Trace}_{N}$ to itself. Such a map is unique and
  thus must be the identity. The same reasoning shows that the desired
  composite do indeed invert each other to form an isomorphism.

  As discussed above, the other cases and
  follow through a similar argument. This
  concludes the proof of our variant of Thompson's construction.
\end{proof}

\subsection{Other Automata}
\subsubsection{Pushdown Automata}
\label{subsubsec:pda}
A (nondeterministic) \emph{pushdown automaton} is an automaton that employs a
stack. Just like NFAs, they have transitions labeled with characters from a
fixed string alphabet $\Sigma$. Additionally, they maintain a stack of
characters drawn from a stack alphabet $\Gamma$. They are often represented
formally as a 7-tuple $(Q, \Sigma, \Gamma, \delta, q_{0}, Z, F)$,

\begin{itemize}
  \item $Q$ a finite set of states
  \item $\Sigma$ a fixed, finite string alphabet
  \item $S$ a fixed, finite stack alphabet
  \item
        $\delta \subset Q \times (\Sigma \cup \{ \varepsilon \}) \times S \to \mathcal{P}(Q \times S^{*})$
        the transition function
  \item $q_{0} \in Q$ the start state
  \item $Z \in S$ the initial stack symbol
  \item $F \subset Q$ the accepting states
\end{itemize}

We encode the traces of a pushdown automaton very similarly to those of an NFA,
except the transitions of a PDA are instead encoded via the LNL product type
$\bigamp$. This is because when simply transitioning via character, a PDA must
also pop and push characters onto a stack, which is used as the argument to
these dependent functions. Thus, the appropriate formation of traces through a
PDA is dependent on a stack.

Let $S$ be a non-linear type encoding the stack
alphabet, and build lists over $S$ as the (non-linear) least fixed-point
$\mathsf{List}(S) := \mu X . 1 + S \times X$. Then, the type of states for a
PDA $P$ with stack alphabet $S$ are given as a functions that takes in lists $L$,
and then makes a case distinction between possible transitions based on what was witnessed as $\mathsf{head}(L)$. The choice of transition
will then determine which character to transition by and what word $w$ should be
pushed onto the stack. The word that is added to the top of the stack is
appended to $\mathsf{tail}(L)$ and then we recursively step into another state
called on argument $w \cdot \mathsf{tail}(L)$.

% \begin{gather*}
%   \mathsf{State} \in {g _{q} : q \in Q} \\
%   \mathsf{Word} \in \String \\
%   \mathsf{Char} \in \Sigma   \\
%   \mathsf{StackChar} \in \Gamma \\
% \end{gather*}
% \begin{align*}
%   \mathsf{Trans}_{P}(q) ::=~&
%                            \LinPiTy {(hd :: tl)} {\mathsf{List}(S)} {\left( \mathsf{Char} \otimes \mathsf{State}(\mathsf{Word} + tl) \right)}~| \\
%   & \LinPiTy {(hd :: tl)} {\mathsf{List}(S)} {\left(  \mathsf{State}(\mathsf{Word} + tl) \right)}~|~\mathsf{Trans}_{P}(q) \oplus \mathsf{Trans}_{P}(q)
% \end{align*}

% That is to say, when transitioning a PDA pops off the head $hd$ of the stack

\subsubsection{Turing Machines}
\label{subsubsec:tm}

Above we gave a grammar presentation of
traces through a PDA by using a non-linear type $S$
to encode the stack. We may similarly use pairs $S \times S$
to encode the tape of a Turing machine. With two stacks we can simulate the behavior of the
infinite tape of a Turing machine. The intuition behind this correspondence is
that the left half of the tape is on one stack, the right
half of the tape the other, and we treat the tops of stacks
like the head of the tape.

\pedro{I like the overall structure of this section! Showing how the type theory
can encode and reason about different kinds of languages is very instructive!}

\section{Semantics and Metatheory}
\label{sec:semantics-and-metatheory}
%% \section{Categorical Semantics of \theoryabbv}
%% \label{sec:categorify}

As described in previous sections, \theoryabbv has a ``standard''
interpretation where non-linear types denote sets, linear types denote
formal grammars, non-linear terms denote functions and linear terms
denote parse transformers.
%
Further, as is typical for type theory, every type constructor in the
calculus is interpreted by using a \emph{universal construction} in
either the category of sets or the category of grammars\footnote{the
only exception is that the axioms for Brzozowski derivatives we
considered do not follow solely from universal properties. In this
section we focus on models that do not necessarily satisfy these
axioms.}.
%
This leads to an immediate opportunity for generalization: the type
theory has in addition to the standard grammar-theoretic
interpretation, an interpretation in any categorical structure that
exhibits the same universal constructions.
%
On the one hand this gives us a structured way to formalize the
standard semantics precisely, but additionally it enables us to
consider \emph{non-standard} models in the next section that point to
further applications as well as meta-theoretic results.

In developing this categorical semantics, we will start with the
closest analogue from formal language theory: Kleene algebra.  Kleene
algebras are an important tool in the theory of regular languages
serving as a bridge between algebraic reasoning and equivalence of
regular expressions. More broadly, through various extensions, they
serve as a theoretical substrate to studying different kinds of formal
languages.
%
We can then see that the ``regular fragment'' of our type theory
(i.e., just characters, $\otimes$, $\oplus$, $0,1$ and a Kleene star)
has models in what we call a \emph{Kleene category} a categorification
of Kleene algebra from posets to categories.
%
We then further develop this into our final notion of model, which we
call a \emph{Chomsky category} as it can model not just regular
languages, but the full Chomsky hierarchy.

\subsection{Kleene Algebra and Kleene Categories}
A Kleene algebra is a tuple $(A, +, \cdot, (-)^*, 1, 0)$, where $A$ is
a set, $+$ and $\cdot$ are binary operations over $A$, $(-)^*$ is a
function over $A$, and $1$ and $0$ are constants. These structures
satisfy the axioms depicted in Figure~\ref{fig:axioms}.

\begin{figure}
  \begin{align*}
    x + (y + z) &= (x + y) + z & x + y &= y + x\\
    x + 0 &= x & x + x &= x\\
    x(yz) &= (xy)z & x1 &= 1x = x\\
    x(y + z) &= xy + xz & (x + y)z &= xz + yz\\
    x0 &= 0x = x & & \\
    1 + aa^* &\leq a^* & 1 + a^*a &\leq a^*\\
     b + ax \leq x &\implies a^*b \leq x &  b + xa \leq x &\implies ba^* \leq x
  \end{align*}
  \caption{Kleene algebra axioms}
  \label{fig:axioms}
\end{figure}

The addition operation can be used to define the partial order
structure $a \leq b$ if, and only if, $a + b = b$. In the theory of
formal languages, this order structure can be used to model language
containment. In this section, we categorify the concept of Kleene
algebra and build on top of it in order to define an abstract theory
of parsing. We start by defining \emph{Kleene categories}.

\begin{definition}
  A Kleene category is a distributive monoidal category $\cat{K}$
  such that for every objects $A$ and $B$, the endofunctors $F_{A, B}
  = B + A \otimes X$ and $G_{A, B} = B + X \otimes A$ have initial
  algebras (denoted $\mu X.\, F_{A, B}(X)$) such that $B \otimes (\mu
  X.\, F_{A, 1}) \cong \mu X.\, F_{A, B}(X)$ and the analogous isomorphism
  for $G_{A,B}$ also holds.
\end{definition}

As a sanity check, note that Kleene algebras are indeed examples of
Kleene categories.

\begin{example}
  Every Kleene algebra, seen a posetal category, is a Kleene category.
  The product $\cdot$ is a monoidal product and the addition is a
  least-upper bound, which corresponds to a coproduct. Lastly, the
  axioms of the Kleene star have a direct correspondence to the
  coherence conditions postulated by the initial algebras of Kleene
  categories.
\end{example}

This example provides a neat categorical justification to how
restrictive Kleene algebras are in terms of reasoning about
languages. By only having at most one morphism between objects, there
is not a lot of information they can convey. In this case, the only
information you get is language containment. As demonstrated by
$\mathbf{Gr}$, the extra degrees of freedom granted by having more
morphisms gives you more algebraic structure for reasoning about
languages.

For the next example, we see an unexpected connection with the theory
of substructural logics.

\begin{example}
  The opposite category of every Kleene category is a model of a variant of
  conjunctive ordered logic, where the Kleene star plays the role of the ``of
  course'' modality from substructural logics which allows hypotheses to
  be discarded or duplicated.
\end{example}

As we have seen, the proposed axioms are a direct translation of the
Kleene algebra axioms to a categorical setting. Its most unusual aspect is the
axiomatization of the Kleene star as a family of initial algebras
satisfying certain isomorphisms. If the Kleene category $\cat{K}$ has
more structure, then these isomorphisms hold ``for free''.

\begin{theorem}
  \label{th:kleeneclosed}
  Let $\cat{K}$ be a Kleene category such that it is also monoidal
  closed.  Then, the initial algebras isomorphisms hold automatically.
\end{theorem}
\begin{proof}
  We prove this by the unicity (up-to isomorphism) of initial
  algebras. Let $[hd, tl]: I + (\mu X.\, F_{A, I}(X)) \otimes A \to
  (\mu X.\, F_{A, I}(X))$ be the initial algebra structure of $(\mu
  X.\, F_{A, I}(X))$ and consider the map $[hd, tl] : B + B \otimes
  (\mu X.\, F_{A, I}(X)) \otimes A \to B\otimes (\mu X.\, F_{A,
    I}(X))$.

  Now, let $[f,g] : B + A \otimes Y \to Y$ be an $F_{A,B}$-algebra and
  we want to show that there is a unique algebra morphism $h : \mu X.\, F_{A,I} \to B \lto Y$. We can show existence and
  uniqueness by showing that the diagram on top commutes if, and
  only if, the diagram on the bottom commutes:

% https://q.uiver.app/#q=WzAsOCxbMCwwLCJCICsgQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFsyLDAsIkIgKyBZIFxcb3RpbWVzIEEiXSxbMiwyLCJZIl0sWzAsMiwiQiBcXG90aW1lcyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDMsIjEgKyAoXFxtdSBYLlxcLCAxICsgWCBcXG90aW1lcyBBKSJdLFswLDUsIlxcbXUgWC5cXCwgMSArIFggXFxvdGltZXMgQSJdLFsyLDMsIjEgKyAoIEIgXFxsdG8gWSkgXFxvdGltZXMgQSJdLFsyLDUsIkIgXFxsdG8gWSJdLFswLDEsImlkICsgKGlkIFxcb3RpbWVzIGg7IGV2KSBcXG90aW1lcyBpZF9BIl0sWzEsMiwiW2YsZ10iXSxbMywyLCJpZCBcXG90aW1lcyBoOyBldiIsMl0sWzAsMywiW2lkIFxcb3RpbWVzIGgsIGlkIFxcb3RpbWVzIHRsXSIsMl0sWzQsNiwiaWQgKyAoaCBcXG90aW1lcyBYKVxcb3RpbWVzIGlkIl0sWzUsNywiaCIsMl0sWzYsNywiW2YnLCBnJ10iXSxbNCw1LCJbaGQsIHRsXSIsMl1d
\[\begin{tikzcd}
	{B + B \otimes (\mu X.\, I + X \otimes A)} && {B + Y \otimes A} \\
	\\
	{B \otimes (\mu X.\, I + X \otimes A)} && Y \\
	{I + (\mu X.\, I + X \otimes A)} && {I + ( B \lto Y) \otimes A} \\
	\\
	{\mu X.\, I + X \otimes A} && {B \lto Y}
	\arrow["{id + (id \otimes h; ev) \otimes id_A}", from=1-1, to=1-3]
	\arrow["{[f,g]}", from=1-3, to=3-3]
	\arrow["{id \otimes h; ev}"', from=3-1, to=3-3]
	\arrow["{[id \otimes h, id \otimes tl]}"', from=1-1, to=3-1]
	\arrow["{id + (h \otimes X)\otimes id}", from=4-1, to=4-3]
	\arrow["h"', from=6-1, to=6-3]
	\arrow["{[f', g']}", from=4-3, to=6-3]
	\arrow["{[hd, tl]}"', from=4-1, to=6-1]
\end{tikzcd}\]
  This equivalence follows by using the adjunction structure given
  by the monoidal closed structure of $\cat{K}$. A completely analogous
  argument for $G_{A,B}$ also holds. Furthermore, by generalizing the
  construction of \Cref{sec:formaltype}, we can also show that from
  the monoidal closed assumption it follows that $\mu X.\, F_{A, I}(X) \cong \mu X.\, G_{A, I}(X)$
\end{proof}

Something surprising about this lemma is that it provides an alternative
perspective on the observation that if a Kleene algebra has an
residuation operation, also called action algebra \cite{kozen1994action},
then the Kleene star admits a simpler axiomatization.

Since we want Kleene categories to generalize our notion of formal
grammars as presheaves $\String \to \Set$, we prove that they do
indeed form a Kleene category. We start by presenting a well-known
construction from presheaf categories.

\begin{definition}
  Let $\cat{C}$ be a locally small monoidal category and $F$, $G$ be
  two functors $\cat{C} \to \Set$. Their Day convolution tensor
  product is defined as the following coend formula:
  \[
  (F \otimes_{Day} G)(x) = \int^{(y,z) \in \cat{C}\times\cat{C}}\cat{C}(y\otimes z, x) \times F(y) \times G(z)
  \]
  Dually, its internal hom is given by the following end formula:
  \[
  (F \lto_{Day} G)(x) = \int_{y} \Set(F(y), G(x \otimes y))
  \]
\end{definition}

\begin{lemma}[Day \cite{day1970construction}]
  Under the assumptions above, the presheaf category $\Set^{\cat{C}}$ is
  monoidal closed.
\end{lemma}

%% \begin{theorem}
%%   Let $\cat{K}$ be a Kleene category and $A$ a discrete category.
%%   The functor category $[A, \cat{K}]$.
%%   (HOW GENERAL SHOULD THIS THEOREM BE? BY ASSUMING ENOUGH STRUCTURE,
%%   E.G. K = Set, THIS THEOREM BECOMES SIMPLE TO PROVE)
%% \end{theorem}
\begin{theorem}
  If $\cat{C}$ is a locally small monoidal category, then
  $\Set^{\cat{C}}$ is a Kleene category.
\end{theorem}
\begin{proof}

  By the lemma above, $\Set^{\cat{C}}$ is monoidal closed, and since it
  is a presheaf category, it has coproducts. Furthermore, the tensor
  is a left adjoint, i.e. it preserves colimits and, therefore, it is
  a distributive category.

  As for the Kleene star, since presheaf categories admit small colimits,
  the initial algebra of the functors $F_{A,B}$ and $G_{A,B}$ can be
  defined as the filtered colimit of the diagrams:

  From Theorem~\ref{th:kleeneclosed} it follows that these initial
  algebras satisfy the required isomorphisms and this concludes the
  proof.
\end{proof}

\begin{corollary}
  For every alphabet $\Sigma$, the presheaf category $\Set^{\cat{\Sigma^*}}$
  is a Kleene category.
\end{corollary}
\begin{proof}
  Note that string concatenation and the empty string make the
  discrete category $\Sigma^*$ a strict monoidal category.
\end{proof}

Much like in the posetal case, the abstract structure of a Kleene
category is expressive enough to synthetically reason about regular
languages. A significant difference between them is that while Kleene
algebras can reason about language containment, Kleene categories can
reason about \emph{ambiguity}, \emph{strong equivalence} of grammars.

\subsection{Lambek Hyperdoctines and Chomsky Hyperdoctrines}

Though Kleene categories are expressive enough to reason about
concepts that are outside of reach of Kleene algebras, their
simply-typed nature makes them not so expressive from a type theoretic
point of view. This is limiting because type theories are successful
syntactic frameworks for manipulating complicated categorical
structures while avoiding some issues common in category theory, such
as coherence issues.

With this in mind, we want to design a categorical semantics that
builds on top of Kleene categories with the goal of extending them
with dependent types and making them capable reasoning about languages
and their parsers. This leads us to the abstract notion of model we
are interested in capturing with \theoryabbv: a \emph{Chomsky
category}.

We do this in two stages: first we define a \emph{Lambek hyperdoctrine} to be a
notion of model for the judgmental structure of \theoryabbv: that is we have
notions of linear and non-linear type, contexts, substitution and terms, but do
not assume any particular type constructions exist. Then we will define when a
Lambek hyperdoctrine is a Chomsky hyperdoctrine, meaning it can interpret all
the type formers of \theoryabbv and therefore arbitrary strength grammars.

\begin{definition}[Lambek Hyperdoctrine]
  A \emph{Lambek hyperdoctrine} consists of
  \begin{enumerate}
  \item A category $\mathcal C$ with a terminal object.
  \item A category with families structure over $\mathcal C$.
  \item A contravariant functor $L : \mathcal C^{o} \to \textrm{MultiCat}$ from
    $\mathcal C$ to the category of Multicategories.
  \end{enumerate}
\end{definition}
Here the objects of $\mathcal C$ model the non-linear contexts, and morphisms
model non-linear substitutions. The category with families structure over
$\mathcal C$ models the dependent non-linear types. Finally the
``hyperdoctrine'' of multicategories $L$ models the linear types and terms. The
fact that this is functorial in $\mathcal C$ corresponds to the fact that linear
types are all relative to a non-linear context and that linear variables and
composition commute with non-linear substitution.

We will have three main Lambek hyperdoctrines of interest in this paper: the
``standard model'' of sets and grammars, the ``syntactic model'' given by our
type theory itself and lastly a gluing model introduced in
Section~\ref{sec:canonicity} to prove the canonicity theorem.
\begin{example}
  \begin{enumerate}
  \item Let $\mathcal C$ be the category of sets, equipped with its usual
    category with families structures of families. Define $L : \mathcal C^{o}
    \to \textrm{MultiCat}$ to map $L(X)$ to the representable multicategory
    $(\Set^{\Sigma^*})^X$ of $X$-indexed families of grammars where the monoidal
    structure is given pointwise by the Day convolution monoidal structure.
  \item We can build a ``syntactic'' Lambek hyperdoctrine $\Syn(\Sigma)$ from the syntax
    itself: the category $\mathcal C$ is given by non-linear contexts and
    substitutions, the category with families by the non-linear types and terms
    and the hyperdoctrine of multicategories by the linear types and terms.
  \end{enumerate}
\end{example}

Next each type linear and non-linear type constructor corresponds to extra data
on a Lambek hyperdoctrine.
\begin{definition}
  \label{def:chomsky-data}
  \begin{enumerate}
  \item Dependent type structure: standard (say extensional type theory, give a reference)
  \item Universes
  \item Inductive grammars?
  \item Non-inductive Grammar constructors: standard monoidal category stuff
  \end{enumerate}
\end{definition}
\begin{definition}[Chomsky Hyperdoctrine]
  A Chomsky Hyperdoctrine is a Lambek hyperdoctrine equipped with all of the data in Definition~\ref{def:chomsky-data}.
\end{definition}
\pedro{We should probably define some of the words in this definition}
\begin{definition}
  A Chomsky category is a locally Cartesian category with two hierarchies of
  universes $\{L_i\}_{i\in \nat}$ and $\{U_i\}_{i\in \nat}$ such that
  every $L_i$ and $U_i$ are $U_{i+1}$-small. Furthermore, we require
  $U_i$ to be closed under dependent products and sum,
  $L_i$ to be closed under the Kleene category connectives,
  dependent products, left and right closed structures, with
  a type constructor $G : L_i \to U_i$ and a linear dependent sum
  going the other direction.
\end{definition}

\steven{Max suggests augmenting the definition of a Chomsky category to
  something like two categories $L$ and $U$, and $U$ is Cartesian closed I don't
  fully recall the rest.


  My best guess is that you take $L$ a Kleene category with a hierarchy of universes
  two, and you further require that $L$ is $Psh(U)$-enriched, except he
  suggested a further adjective on these presheaves. Perhaps representability?
}

\begin{theorem}
  The presheaf category $\Grammar = \Set^{\cat{\Sigma^*}}$ is a Chomsky category.
\end{theorem}


Further, the syntactic category of \theoryname is manifestly a Chomsky category.

\pedro{This is likely true, but if we explicitly say so, this warrants a proof. I think that
if we don't say anything about the syntactic category, reviewers won't mind.}

\steven{I agree that we don't want to say anything that opens unnecessary
  questions for proofs we haven't written. However, it seems hard to make the
  case that we have the right categorical model of the syntax if this statement
  isn't true. By restating our definition of Chomsky category, this should be
  obvious or a quick proof}

\pedro{I agree, then we should add the quick proof :) }

\subsection{Concrete Models of \theoryabbv}
\label{sec:othermodels}

One of the powers of type theories is that it can be profitable to interpret them
in various models. In this section, by using our just-defined Chomsky categories,
we show how other useful concepts from formal language theory can also be organized
as models of \theoryabbv. We illustrate this point by providing two examples that
are closely related to the theory of formal languages: language equivalence and
semantic actions. Furthermore, in order to justify how $\mathbf{Gr}$ relates to
more traditional notions of parsing, we define a glued model that proves a
canonicity property of grammar terms.


\subsection{Language Semantics}
Every grammar induces a language semantics. Also languages can be taken as a
propositionally truncated view of the syntax. Logical equivalence should induce
weak equivalence, and thus even give a syntactic way to reason about language equivalence.
\steven{TODO language semantics}

\subsection{Semantic Actions}
\steven{Tentatively planning to cut this subsection for an internal representation
  of semantic actions}
Returning to the problem of parsing, the output of a parse usually is not the
literal parse tree. Rather, the output is the result of some \emph{semantic
  actions} ran on the parse tree, which usually serve to remove some syntactic
details that are unnecessary for later processing.

Given a grammar $G : \String \to \Set$, we define a semantic action to be a set
$X$ with a function $f$ that produces a semantic element from any parse of $G$.

\[
  f : \PiTy w \String {G w \to X}
\]

Further, semantic actions can be arranged into a structured category.
Define $\SemAct$ as the comma
category $\Grammar / \Delta$, where $\Delta : \Set \to \Grammar$ defines a
discrete presheaf. That is, for a set $X$, $\Delta (X)(w) = X$ for all
$w \in \String$. As $\SemAct$ is defined as a comma category, it has a forgetful
functor into $\Grammar$. That is, $\SemAct$ serves as a notion of formal
grammar. Moreover, $\SemAct$ is a model of \theoryabbv.

\steven{It being a notion of formal grammar is distinct from being a model. This
probably warrants a proof}

\steven{TODO semantic actions}

\pedro{This is a very nice opportunity of showing off the supremacy of denotational
  reasoning ;) We should probably prove the gluing lemma in the previous section
  and apply it here and in the canonicity section. The actual proof might have to be
  moved to the appendix, though}

\subsection{Parse Canonicity}
Canonicity is an important metatheoretic theorem in the type theory
literature.  It provides insight on the normal forms of terms and,
therefore, on its computational aspects. Frequently, proving
canonicity for boolean types, i.e. every closed term of type bool
reduces to either true or false, is enough to justify that the type
theory being studied is well-behaved. In our case, however, since we
want to connect \theoryabbv to parsers, we must provide a more
detailed account of canonicity. In particular, we give a nonstandard semantics
of \theoryabbv that carries a proof of canonicity along with it.

If $\cdot \vdash A$ is a closed linear type then there are
two obvious notions of what constitutes a ``parse'' of a string w
according to the grammar $A$:
\begin{enumerate}
\item On the one hand we have the set-theoretic semantics just
  defined, $\llbracket A \rrbracket \cdot w$
\item On the other hand, we can view the string $w = c_1c_2\cdots$ as
  a linear context $\lceil w \rceil = x_1:c_1,x_2:c_2,\ldots$ and
  define a parse to be a $\beta\eta$-equivalence class of linear terms $\cdot;
  \lceil w \rceil \vdash e : G$.
\end{enumerate}
It is not difficult to see that at least for the ``purely positive''
formulae (those featuring only the positive connectives
$0,+,I,\otimes,\mu, \overline\Sigma,c$) that
every element $t \in \llbracket A \rrbracket w$ is a kind of tree and
that the nodes of the tree correspond precisely to the introduction
forms of the type. However it is far less obvious that \emph{every}
linear term $\lceil w \rceil \vdash p : \phi$ is equal to some
sequence of introduction forms since proofs can include elimination
forms as well. To show that this is indeed the case we give a
\emph{canonicity} result for the calculus: that the parses for .

\begin{definition}
  A non-linear type $X$ is purely positive if it is built up using
  only finite sums, finite products and least fixed points.

  A linear type is purely positive if it is built up using only finite
  sums, tensor products, generators $c$, least fixed points and linear
  sigma types over purely positive non-linear types.
\end{definition}

\begin{definition}
  %% Let $X$ be a closed non-linear type. The closed elements $\textrm{Cl}(X)$ of $X$ are the definitional equivalence classes of terms $\cdot \vdash e : X$.

  Let $A$ be a closed linear type. The nerve $N(A)$ is a presheaf on
  strings that takes a string $w$ to the definitional equivalence
  classes of terms $\cdot; \lceil w\rceil \vdash e: N(A)$.
\end{definition}

\begin{theorem}[Canonicity]
  Let $A$ be a closed, purely positive linear type. Then there is an
  isomorphism between $\llbracket A\rrbracket$ and $N(A)$.
\end{theorem}
\begin{proof}
  We outline the proof here, more details are in the appendix. The
  proof proceeds first by a standard logical families construction
  that combines canonicity arguments for dependent type theory
  TODO cite coquand
  % \cite{coquand,etc}
  with logical relations constructions for linear
  types
  TODO cite hylandschalk
  % \cite{hylandschalk}
  . It is easy to see by induction that the
  logical family for $A$, $\hat A$ is isomorphic to $\llbracket A
  \rrbracket$ and the fundamental lemma proves that the projection
  morphism $p : \hat A \to N(A)$ has a section, the canonicalization
  procedure. Then we establish again by induction that
  canonicalization is also a retraction by observing that introduction
  forms are taken to constructors.
\end{proof}


\begin{enumerate}
\item Every term $\lceil w \rceil \vdash p : G + H$ is equal to $\sigma_1q$ or $\sigma_2 r$ (but not both)
\item There are no terms $\lceil w \rceil \vdash p : 0$
\item If there is a term $\lceil w \rceil \vdash p : c$ then $w = c$ and $p = x$.
\item Every term $\lceil w \rceil \vdash p : G \otimes H$ is equal to $(q,r)$ for some $q,r$
\item Every term $\lceil w \rceil \vdash p : \epsilon$ is equal to $()$
\item Every term $\lceil w \rceil \vdash p : c$ is equal to $x:c$
\item Every term $\lceil w \rceil \vdash p : \mu X. G$ is equal to $\textrm{roll}(q)$ where $q : G(\mu X.G/X)$
\item Every term $\lceil w \rceil \vdash p : (x:A) \times G$ is equal
  to $(M,q)$ where $\cdot \vdash M : A$
\end{enumerate}

To prove this result we will use a logical families model. We give a
brief overview of this model concretely:
\begin{enumerate}
\item A context $\Gamma$ denotes a family of sets indexed by closing substitutions $\hat\Gamma : (\cdot \vdash \Gamma) \Rightarrow \Set_i$
\item A type $\Gamma \vdash X : U_i$ denotes a family of sets $\hat X : \Pi(\gamma:\cdot \vdash \Gamma) \hat\Gamma \Rightarrow (\cdot \vdash \simulsubst X \gamma) \Rightarrow \Set_i$
\item A term $\Gamma \vdash e : X$ denotes a section $\hat e : \Pi(\gamma)\Pi(\hat\gamma)\hat X \gamma \hat\gamma (\simulsubst e \gamma)$
\item A linear type $\Gamma \vdash A : L_i$ denotes a family of grammars $\hat A : \Pi(\gamma:\cdot\vdash\Gamma)\,\hat\Gamma \Rightarrow \Pi(w:\Sigma^*) (\cdot;\lceil w\rceil \vdash A[\gamma])\Rightarrow \Set_i$, and the denotation of a linear context $\Delta$ is similar.
\item A linear term $\Gamma;\Delta \vdash e : A$ denotes a function \[\hat e : \Pi(\gamma)\Pi(\hat\gamma)\Pi(w)\Pi(\delta : \lceil w \rceil \vdash \simulsubst \Delta \gamma) \hat\Delta \gamma \hat\gamma \delta \Rightarrow \hat A \gamma \hat\gamma w {(\simulsubst {\simulsubst e \gamma} \delta)}\]
\end{enumerate}
And some of the constructions:
\begin{enumerate}
\item $\widehat {(G A)} \gamma \hat\gamma e = \hat A \gamma \hat\gamma \varepsilon (G^{-1}e)$
\item $\widehat {(A \otimes B)} \gamma \hat\gamma w e = \Sigma(w_Aw_B = w)\Sigma(e_A)\Sigma(e_B) (e_A,e_B) = e \wedge \hat A \gamma \hat \gamma w_A e_A \times \hat B \gamma \hat \gamma w_B e_B$
\item $\widehat {(A \lto B)} \gamma \hat\gamma w e = \Pi(w_A)\Pi(e_A) \hat A \gamma\hat\gamma w_A e_A \Rightarrow \hat B \gamma\hat\gamma (ww_A) (\applto {e_A} e)$
\end{enumerate}

First, the category with families will be
the category of logical families over set contexts/types
$\Delta$/$A$. Then the propositional portion will be defined by
mapping a logical family $\hat \Gamma \to \Gamma$

First, let $L$ be the category of BI formulae and proofs (quotiented
by $\beta\eta$ equality). Define a functor $N : L \to \Set^{\Sigma^*}$ by
\[ N(\phi)(w) = L(w,\phi) \]

Then define the gluing category $\mathcal G$ as the comma category
$\Set^{\Sigma^*}/N$. That is, an object of this category is a pair of
a formula $\phi \in L$ and an object $S \in \mathcal
P(\Sigma^*)/N(\phi)$. We can then use the equivalence $\mathcal
P(\Sigma^*)/N(\phi) \cong \mathcal P(\int N(\phi))$ to get a simple
description of such an $S$: it is simply a family of sets indexed by
proofs $L(w,\phi)$:
\[ \prod_{w\in\Sigma^*} L(w,\phi) \to \Set \]
This category clearly comes with a projection functor $\pi : \mathcal
G \to \mathcal L$ and then our goal is to define a section by using
the universal property of $\mathcal L$.

To this end we define
\begin{enumerate}
\item $(\phi, S) \otimes (\psi, T) = (\phi \otimes \psi, S\otimes T)$ where
  \[ (S \otimes T)(w, p) = (w_1w_2 = w) \times (q_1,q_2 = p) \times S\,w_1\,q_1 \times T\,w_2\,q_2\]
\item $(\phi, S) \multimap (\psi, T) = (\phi \multimap \psi, S \multimap T)$ where
  \[ (S \multimap T)(w,p) = w' \to q \to S\,w'\,q \to T (ww') (p\,q) \]
\item $\mu X. ??$ ??
\end{enumerate}

\pedro{We should conclude this section by explaining the relevance of the canonicity
theorem. Could also be done before stating the theorem.}

\section{Discussion and Future Work}
\label{sec:discussion}

%% 1. More practical: integrating it into a larger verified development
%%    - verified imperative implementation: Lambek logic ala separation
%%      logic?
%% 2. Semantic actions
%% 3. Type systems as tree grammars


The core objects of study in the theory of formal grammars have stood stalwart
for decades. Study of interesting extensions or restrictions of particular
grammar formalisms has remained consistent line of research over the years.
Occasionally, researchers will either explicitly create a type system for their
formalism, or researchers will leverage the same underlying structure of their
systems. In this paper, we have not
done much \emph{per se}, other than gather these ideas, abstract over their
commonalities, and give the common language between formalisms.

\paragraph{Typed Approaches to Grammars}
The usage of a simple type system to reason about regular expressions was
introduced by Frisch and Cardelli in 2004 \cite{frischCardelli}, and later expanded
by Hengelin and Nielsen in 2011 \cite{henglein_regular_2011} to handle proofs of regular expression
containment.

In 1992, while investigating the addition of arbitrary recursion to Kleene
algebra \cite{leiss},
Lei{\ss} used a least fixed-point operator instead of the Kleene star. Although
he did not explicitly make use of a type system, or have access to the view of Frisch and
Cardelli, Lei{\ss} had added the full power of inductive types to a type system of regular expressions.

Pratt's work on the residuation of action algebras in \cite{prattActionLogicPure1991} closely mirrors
our observation that Brzozowski derivatives mirror linear function types.
Moreover, he shows that his residuals form a Galois connection with sequential
composition. This is precisely a posetal --- or \emph{thin} --- version of the
adjunction between
linear function types and tensor product in \theoryabbv.

In \cite{firsovCertifiedNormalizationContextFree2015}, Firsov and Uustalu
constructed a verified normalization procedure to turn a
context-free grammar into an equivalent grammar in Chomsky normal form.
Combining their work with a CYK parser, they have built a verified context-free
grammar parser, \emph{up to weak equivalence}. To upgrade from a weak
equivalence to a strong equivalence, they suggest updating their grammar
formalism to treat parse trees as first-class proofs of language membership.
That is, without explicitly stating so, they suggest that they next natural step
is to reason about grammars in a system where parse trees are terms, as we have
given in this paper.

\steven{Make the above paragraph shorter. Also idk if its worth including, but I
like that they practically ask for a type theory}

\steven{Mention Neel's typed algebraic approach paper here. Say it is similar at
  first glance, as its parsing with types, but that he actually is representing
  something quite different. Or is it so different that its not worth mentioning?}

\steven{TODO add lambek calculus/categorial grammars here}

\paragraph{Kleene Algebra}
Since the early works in the theory of formal languages, Kleene
algebras have played an important role in its development. They
generalize the operations known from regular languages by introducing
operations generalizing language composition, language union and the
Kleene star.  More generally, they are defined as inequational theory
where the inequality is meant to capture language containment. This
theory is extremely successful, having found applications in algebraic
path problems, theory of programming languages, compiler optimizations
and more.

A frequently fruitful research direction is exploring varying
extensions of Kleene algebras, Kleene algebra with tests (KAT) being
one of the most notable ones. Our approach is radically different from
most extensions, which usually aim at modifying or adding new
operations to Kleene algebras, but still keeping it as an inequational
theory. By adopting a category-theoretic treatment and allowing the
``order structure'' to encode more information than merely
inequalities, we were able to extend Kleene algebra to reason about
parsing as well.

\steven{Can we sketch here what an apt categorification of KATs might be? As a
  poor first pass: perhaps a Kleene category with a full involutive
  subcategory that corresponds to the Boolean algebra?.}
\steven{What can we say about internalizing Kleene algebra proofs or extensions?}
\steven{any KA citations needed above?}

\paragraph{Linearity with Dependent Types}
\steven{I don't know if it worth discussing the different influences of
  categorical model here? We take Neel's model and add to it as necessary}

Vakar might be worth citing to discuss different characterizations of the
notion of model \cite{vakarSyntaxSemanticsLinear2015}

\cite{fu2023twolevellineardependenttype} give a compiler for a two-level
linear dependent type theory. This could serve as a template for a
compiler implementation of \theoryname that is not just inference rules
embedded in Agda.

\paragraph{Implementations in Parsing}
Talk about how we have a DFA parser in Agda and want to to get out a regex
parser. Then context free, etc

future work related to building a usable correct-by-construction parser
combinator/semantic action combinator library in this language.

Contrast this
potential library with existing verified parsers. Citations for some in prelim presentation

\paragraph{Applications to Type Checking}
While parsing typically refers to the generation of semantic
objects from string input, many tasks in programming can be
viewed as parsing of objects with more structure, such as
trees with binding structure or graphs. Fundamental to the
frontend of many
programming language implementations are type systems. In
particular, \emph{type checking}
--- analogous to language recognition --- and \emph{typed
  elaboration} --- analogous to parsing --- arise when
producing a semantic object subject to some analysis. Just
as our string grammars were given as functors from $\String$
to $\Set$, we envision adapting the same philosophy
to functors from $\String$ to some \emph{category of trees} to craft a syntax
that natively captures typed elaboration. This suggests an
unusual sort of bunched type theory, where context extension
no longer resembles concatenation of strings but instead
takes on the form of tree constructors.

\steven{condense the future work section?}
\steven{Put the 2 category stuff here?}

\paragraph{Other Future Work}
Backtracking parsing: via (delimited) continuations? effect handlers?

\bibliographystyle{plain}
\bibliography{refs.bib}


\end{document}
